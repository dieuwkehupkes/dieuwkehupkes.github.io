tag	year	Title	Website	Status	motivation	type	shift locus	data shift	shift source	task	Annotator 1	Annotator 2	Notes															
he2018time	2018	Time-evolving Text Classification with Deep Neural Networks	https://doi.org/10.24963/ijcai.2018/310	Done	practical	across domain	train-test	covariate	naturally occurring shifts	text classification	Dieuwke	Dennis	Dieuwke: Locus should be double checked															
freitag2016fast	2016	Fast Domain Adaptation for Neural Machine Translation	https://arxiv.org/pdf/1612.06897.pdf	Done	practical	across domain	pretrain-train	covariate	naturally occurring shifts	machine translation	Dieuwke	Florian	Domain adaptation in NMT, model is adapted (==finetune) on OOD data															
dixon2018measuring	2018	Measuring and Mitigating Unintended Bias in Text Classification	https://doi.org/10.1145/3278721.3278729	Done	fairness	robustness	train-test	covariate	naturally occurring shifts	toxicity classification	Dieuwke	Dennis																
dixon2018measuring	2018	Measuring and Mitigating Unintended Bias in Text Classification	https://doi.org/10.1145/3278721.3278729	Done	fairness	robustness	train-test	covariate	generated shifts	toxicity classification	Dieuwke	Dennis																
borkan2019unintended	2019	Nuanced Metrics for Measuring Unintended Bias with Real Data for Text Classification	https://doi.org/10.1145/3308560.3317593	Done	fairness	robustness	train-test	covariate	generated shifts	toxicity classification	Dieuwke	Huygaa																
raffel2020t5	2020	Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer	http://jmlr.org/papers/v21/20-074.html	Done	practical	across task	pretrain-train	assumed	naturally occurring shifts	multitask	Dieuwke	Dennis																
aribandi2022ext5	2022	ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning	https://openreview.net/forum?id=Vzh1BFUCiIX	Done	practical	across task	finetune-train/test	label	naturally occurring shifts	multitask	Dieuwke	Karim	Experiment in 2.1: finetune a model on a subsec of tasks, then test on other tasks (to evaluate transfer)															
aribandi2022ext5	2022	ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning	https://openreview.net/forum?id=Vzh1BFUCiIX	Done	practical	across task	pretrain-train	label	naturally occurring shifts	multitask	Dieuwke	Karim	Experiment in 2.2: pretrain on subset of ExMIX, finetune and test on SuperGLUE. Since there is a comparison of inclusion of different tasks in pretraining, but not in finetuning, this entry is marked label shift between pretrain and finetune.															
aribandi2022ext5	2022	ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning	https://openreview.net/forum?id=Vzh1BFUCiIX	Done	practical	across task	pretrain-train	label	naturally occurring shifts	multitask	Dieuwke	Karim	Experiment in 2.3: similar setup, but they compare "pre-finetuning" vs pre-training. Because none of this is on the final stage, we consider this a comparison of different pretraining procedures.															
aribandi2022ext5	2022	ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning	https://openreview.net/forum?id=Vzh1BFUCiIX	Done	practical	across task	pretrain-train	label	naturally occurring shifts	multitask	Dieuwke	Karim	Experiment in 2.5: scaling experiments with adding more tasks in pretraining															
xie2022unifiedskg	2022	UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models	https://arxiv.org/abs/2201.05966	Done	practical	across task	pretrain-train	label	naturally occurring shifts	structured knowledge grounding	Dieuwke	Mario	Experiment in 4.1: An investigation of different pretraining procedures on generalisation to different SKG tasks															
xie2022unifiedskg	2022	UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models	https://arxiv.org/abs/2201.05966	Done	practical	across task	finetune-train/test	label	naturally occurring shifts	structured knowledge grounding	Dieuwke	Mario	Experiments in 4.2: comparison of different finetuning procedures															
xie2022unifiedskg	2022	UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models	https://arxiv.org/abs/2201.05966	Done	practical	across task	pretrain-test	assumed	naturally occurring shifts	structured knowledge grounding	Dieuwke	Mario	Exprements in 4.3: Analysis of zero and few-shot settings with T5															
wei2022finetuned	2022	Finetuned Language Models are Zero-Shot Learners	https://openreview.net/forum?id=gEZrGCozdqR	Done	practical	across task	finetune-train/test	assumed	naturally occurring shifts	multitask	Dieuwke	Florian	Generation experiments, where models are finetuned on some clask clusters and evaluated on others. Because only one pretrained model is considered, the relevant locus of shift is finetune train-test.															
sanh2022multitask	2022	Multitask Prompted Training Enables Zero-Shot Task Generalization	https://openreview.net/forum?id=9Vrb9D0WI4	Done	practical	across task	finetune-train/test	full	naturally occurring shifts	multitask	Dieuwke	Mario	Zero-shot task generalisation, where pretrained models are finetuned on some tasks and then tested on others.															
bragg2021flex	2021	FLEX: Unifying Evaluation for Few-Shot NLP	https://proceedings.neurips.cc/paper/2021/file/8493eeaccb772c0878f99d60a0bd2bb3-Paper.pdf	Done	practical	across task	pretrain-test	assumed	naturally occurring shifts	multitask	Dieuwke	Dennis	Pretraining only experiments, where they test how well pretrained generalise on the test data of the proposed benchmark															
bragg2021flex	2021	FLEX: Unifying Evaluation for Few-Shot NLP	https://proceedings.neurips.cc/paper/2021/file/8493eeaccb772c0878f99d60a0bd2bb3-Paper.pdf	Done	practical	across domain	finetune-train/test	covariate	naturally occurring shifts	multitask	Dieuwke	Dennis	"meta-train" setting, where they finetune on the benchmark and test generalisation across domain (this entry) and task (next entry)															
bragg2021flex	2021	FLEX: Unifying Evaluation for Few-Shot NLP	https://proceedings.neurips.cc/paper/2021/file/8493eeaccb772c0878f99d60a0bd2bb3-Paper.pdf	Done	practical	across task	finetune-train/test	label	naturally occurring shifts	multitask	Dieuwke	Dennis	"meta-train" setting, where they finetune on the benchmark and test generalisation across domain (previous entry) and task (this entry)															
perez2021true	2021	True Few-Shot Learning with Language Models	https://openreview.net/forum?id=ShnM-rRh4T	Done	intrinsic	across task	pretrain-test	assumed	generated shifts	multitask	Dieuwke	Tiago	This paper is about model selection, and it shows that if you don't use many examples in your validation set (which should be seen as part of training), few shot learning does not work as well anymore, thus arguing that it is not really few show learning															
collobert2008unified	2008	A unified architecture for natural language processing: deep neural networks with multitask learning	https://doi.org/10.1145/1390156.1390177	Done	practical	across task	train-test	label	naturally occurring shifts	semantic parsing / role labelling	Dieuwke	Florian	Multitask learning, several training languages, tested on semantic role labeling (no finetune stage)															
xu2022zeroprompt	2022	ZeroPrompt: Scaling Prompt-Based Pretraining to	https://arxiv.org/abs/2201.06910	Done	practical	across task	pretrain-test	assumed	naturally occurring shifts	multitask	Florian	Dieuwke																
artetxe2021moe	2021	Efficient Large Scale Language Modeling with Mixtures of Experts	https://arxiv.org/abs/2112.10684	Done	fairness	across task	pretrain-test	assumed	naturally occurring shifts	multitask	Florian	Dieuwke	Zero-shot/Few-shot experiment															
artetxe2021moe	2021	Efficient Large Scale Language Modeling with Mixtures of Experts	https://arxiv.org/abs/2112.10684	Done	fairness	across task	pretrain-train	assumed	naturally occurring shifts	multitask	Florian	Dieuwke	Supervised finetuning experiments															
artetxe2021moe	2021	Efficient Large Scale Language Modeling with Mixtures of Experts	https://arxiv.org/abs/2112.10684	Done	fairness	across domain	train-test	covariate	naturally occurring shifts	language modelling	Florian	Dieuwke	First experiment they test PPL out of domain (different dataset than training set)															
hoffmann2022chinchilla	2022	Training Compute-Optimal Large Language Models	https://arxiv.org/abs/2203.15556	Done	practical	across task	pretrain-test	assumed	naturally occurring shifts	multitask	Mikel	Dieuwke	Same experimental setup as Gopher below															
hoffmann2022chinchilla	2022	Training Compute-Optimal Large Language Models	https://arxiv.org/abs/2203.15556	Done	practical	across domain	train-test	covariate	naturally occurring shifts	language modelling	Mikel	Dieuwke	Same experimental setup as Gopher below															
rae2021gopher	2021	Scaling Language Models: Methods, Analysis & Insights from Training Gopher	https://arxiv.org/abs/2112.11446	Done	practical	across task	pretrain-test	assumed	naturally occurring shifts	multitask	Mikel	Dieuwke	In-context learning experiments															
rae2021gopher	2021	Scaling Language Models: Methods, Analysis & Insights from Training Gopher	https://arxiv.org/abs/2112.11446	Done	practical	across domain	train-test	covariate	naturally occurring shifts	language modelling	Mikel	Dieuwke	Language modeling evaluation in datasets different from training (assuming these are different domains but they are not explicitly controlling for it)															
smith2022megatronturing	2022	Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530	https://arxiv.org/abs/2201.11990	Done	practical	across task	pretrain-test	assumed	natural data splits	multitask	Mikel	Dennis	GPT-3 like in context learning															
zhang2022opt	2022	OPT: Open Pre-trained Transformer Language Models	https://arxiv.org/abs/2205.01068	Done	practical	across task	pretrain-test	assumed	naturally occurring shifts	multitask	Dieuwke	Huygaa	In-context learning experiments with different (natural) evaluation sets															
zhang2022opt	2022	OPT: Open Pre-trained Transformer Language Models	https://arxiv.org/abs/2205.01068	Done	practical	across task	pretrain-test	assumed	generated shifts	multitask	Dieuwke	Huygaa	In-context learning experiments with different generated evaluation sets															
li2021xglm	2021	Few-shot Learning with Multilingual Language Models	https://arxiv.org/abs/2112.10668	Flag for further review	practical	across task	pretrain-test	assumed	natural data splits	multitask	Mikel		The paper evaluates a multilingual LM on in-context-learning, which I annotate as cross-task generalization instead of cross-lingual because the pretrain vs test languages are the same. There are also experiments where the in-context examples are in a different language though, which does not seem to fit our taxonomy very well. I annotated the shift_source as natural data splits because it's fragmented according to the language.															
talat2018bridging	2018	Bridging the gaps: {M}ulti task learning for domain transfer of hate speech detection	https://link.springer.com/content/pdf/10.1007/978-3-319-78583-7.pdf	Flag for further review	fairness	across domain	train-test	covariate	naturally occurring shifts	hate speech detection	Dieuwke		The "multitask learning" setup they have is super confusing to me. Do they actually evaluate out of domain there?															
fitzgerald2022massive	2022	MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages	https://arxiv.org/abs/2204.08582	Done	practical	across task	pretrain-train	label	naturally occurring shifts	intent classification	Christos	Dieuwke	In the first experiment, they investigate how different pretrained encoders generalise when trained on the massive dataset, there is no shift in the finetuning stage, but (presumably) a shift from pretraining to finetuning, so it is marked "assumed shift" from pretrain to training.															
fitzgerald2022massive	2022	MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages	https://arxiv.org/abs/2204.08582	Done	practical	across language	all	double	naturally occurring shifts	intent classification	Christos	Dieuwke	In the second experiment, they finetune (multilingual) models on English, and assess them on all non-English languages. Because they compare multiple multilingual pretraining procedures, this is marked a double shift.															
hu2020xtreme	2020	{XTREME}: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalisation	https://proceedings.mlr.press/v119/hu20b.html	Flag for further review	practical	across language	all	double	naturally occurring shifts	multitask	Koustuv	Dieuwke	Finetune on English, then test on other languages, for a large number of different tasks. Since this paper compares different pretrained multi-lingual models and considers a covariate shift in the finetuning stage, it is marked a double shift: it evaluates two stages of the modelling pipeline simultaneously.															
muller2021genqa	2021	Cross-Lingual GenQA: Open-Domain Question Answering with Answer Sentence Generation	https://arxiv.org/abs/2110.07150	Flag for further review	fairness	across language			naturally occurring shifts	question answering	Dieuwke																	
wang2022benchmarking	2022	Benchmarking Generalization via In-Context Instructions on 1,600+ Language Tasks	https://arxiv.org/pdf/2204.07705	Flag for further review	practical	across task	pretrain-test	assumed	naturally occurring shifts	multitask	Dieuwke	Zhijing	Experiments with off-the-shelve pretrained LLMs without further finetuning															
wang2022benchmarking	2022	Benchmarking Generalization via In-Context Instructions on 1,600+ Language Tasks	https://arxiv.org/pdf/2204.07705	Done	practical	across task	all	full	naturally occurring shifts	multitask	Dieuwke	Zhijing	Finetune on some tasks, test on others															
wang2022benchmarking	2022	Benchmarking Generalization via In-Context Instructions on 1,600+ Language Tasks	https://arxiv.org/pdf/2204.07705	Done	practical	across language	all	full	naturally occurring shifts	multitask	Dieuwke	Zhijing	Multilingual experiments, finetune on tasks in some languages, test on (other?) tasks in other languages															
liu2021challenges	2021	Challenges in Generalization in Open Domain Question Answering	https://arxiv.org/abs/2109.01156	Done	intrinsic	compositional	finetune-train/test	covariate	natural data splits	question answering	Dieuwke	Zhijing																
Phang2018SentenceEO	2018	Sentence Encoders on {STILT}s: {S}upplementary Training on Intermediate Labeled-data Tasks	https://arxiv.org/abs/1811.01088	Flag for further review	practical	across task	pretrain-train	label	naturally occurring shifts	multitask	Dieuwke		This work considers the effect of a "pre-finetuning stage", in which they add some of the GLUE tasks, and then evaluate on the others. If we consider this pre-finetuning stage part of the pretraining procedure, I'd   say this this a label shift between pretrain and finetune. They also compare different models during pretraining. (Whoever is checking this: I found it a bit difficult to annotate)															
bowman2015tree	2015	Tree-structured composition in neural networks without tree-structured architectures	https://arxiv.org/pdf/1506.04834.pdf	Done	intrinsic	compositional	train-test	covariate	fully generated/selected	seq2seq synthetic	Dieuwke	Florian																
brown2020language	2020	Language models are few-shot learners	https://arxiv.org/pdf/2005.14165.pdf	Done	practical	across task	pretrain-test	assumed	naturally occurring shifts	multitask	Dieuwke	Tiago	Zero-shot experiments with no further updating, on several different tasks. This entry represents the natural datasets involved in the test phase.															
brown2020language	2020	Language models are few-shot learners	https://arxiv.org/pdf/2005.14165.pdf	Done	practical	across task	pretrain-test	assumed	generated shifts	multitask	Dieuwke	Tiago	Zero-shot experiments with no further updating, on several different tasks. This entry represents the generated datasets involved in the test phase.															
gontier2020measuring	2020	Measuring systematic generalization in neural proof generation with transformers	https://arxiv.org/abs/2009.14786	Done	cognitive	compositional	finetune-train/test	covariate	fully generated/selected	seq2seq synthetic	Koustuv	Dieuwke	Finetuning of pretrained TLMs to soft-theorem proving															
hupkes2018diagnostic	2018	Visualisation and `diagnostic classifiers' reveal how recurrent and recursive neural networks process hierarchical structure		Done	cognitive	structural	train-test	covariate	fully generated/selected	seq2seq synthetic	Verna	Dieuwke	For motivation both intrinsic and cognitive come to mind. Opted for intrinsic due to the stark focus on *how* the models work. Also... task is not seq2seq, right?															
hupkes2020compositionality	2020	Compositionality Decomposed: {H}ow do Neural Networks Generalise?		Done	cognitive	compositional	train-test	covariate	fully generated/selected	seq2seq synthetic	Dieuwke	Verna																
lake2018generalization	2018	Generalization without systematicity: {O}n the compositional skills of sequence-to-sequence recurrent networks		Done	cognitive	compositional	train-test	covariate	fully generated/selected	seq2seq synthetic	Dieuwke	Verna																
lakretz2021causal	2021	Causal Transformers Perform Below Chance on Recursive Nested Construction	https://arxiv.org/abs/2110.07240	Done	cognitive	structural	train-test	assumed	generated shifts	syntactic tests	Dieuwke	Verna																
lakretz2021mechanisms	2021	Mechanisms for handling nested dependencies in neural-network language models and humans	https://www.sciencedirect.com/science/article/pii/S0010027721001189	Done	cognitive	structural	train-test	assumed	generated shifts	syntactic tests	Dieuwke	Verna																
keysers2019measuring	2019	Measuring Compositional Generalization: {A} Comprehensive Method on Realistic Data		Done	cognitive	compositional	train-test	covariate	fully generated/selected	semantic parsing / role labelling	Dieuwke	Verna																
livska2018memorize	2018	Memorize or generalize? {S}earching for a compositional {RNN} in a haystack		Done	cognitive	compositional	train-test	covariate	fully generated/selected	seq2seq synthetic	Dieuwke	Verna																
mul2019siamese	2019	Siamese recurrent networks learn first-order logic reasoning and exhibit zero-shot compositional generalization	https://arxiv.org/pdf/1906.00180.pdf	Done	cognitive	compositional	train-test	covariate	fully generated/selected	NLI	Dieuwke	Verna	Substitutivity experiments in an NLI task, among other things															
raunak2019compositionality	2019	On Compositionality in Neural Machine Translation	https://arxiv.org/abs/1911.01497	Done	cognitive	compositional	train-test	covariate	fully generated/selected	machine translation	Verna	Dieuwke	systematicity experiments															
raunak2019compositionality	2019	On Compositionality in Neural Machine Translation	https://arxiv.org/abs/1911.01497	Done	cognitive	compositional	train-test	covariate	generated shifts	machine translation	Verna	Dieuwke	productivity experiments															
saxton2019analysing	2019	Analysing Mathematical Reasoning Abilities of Neural Models	https://arxiv.org/pdf/1904.01557.pdf	Done	cognitive	compositional	train-test	covariate	generated shifts	seq2seq synthetic	Yanai	Dieuwke	Comparison of how well transformers and LSTMs generalise to mathematical reasoning problems															
power2021grokking	2021	Grokking: {G}eneralization beyond overfitting on small algorithmic datasets	https://arxiv.org/pdf/2201.02177.pdf	Done	practical	compositional	train-test	assumed	fully generated/selected	seq2seq synthetic	Dennis	Dieuwke	Task here is a bit unclear, since this is not NLP but predicting the result sof equations															
mccann2018natural	2018	The natural language decathlon: {M}ultitask learning as question answering	https://arxiv.org/pdf/1806.08730.pdf	Done	practical	across task	train-test	covariate	naturally occurring shifts	multitask	Dennis	Dieuwke	The experiment that compares multitask with single task training for classification tasks formulated as questions, given the differences in training sets between those two experiments, this implicitly investigates a covariate shift between the two experiments.															
mccann2018natural	2018	The natural language decathlon: {M}ultitask learning as question answering	https://arxiv.org/pdf/1806.08730.pdf	Done	practical	across domain	pretrain-train	covariate	naturally occurring shifts	multitask	Dieuwke	Dennis	There is one pretrain finetune experiment where they check whether pretraining on DecaNLP helps when finetuning for SST and SNLI (zero-shot performance is also tested)															
mccann2018natural	2018	The natural language decathlon: {M}ultitask learning as question answering	https://arxiv.org/pdf/1806.08730.pdf	Done	practical	across task	pretrain-train	covariate	naturally occurring shifts	multitask	Dieuwke	Dennis	There is one pretrain finetune experiment where they check whether pretraining on DecaNLP helps when finetuning for two new tasks (NER and English -Czech translation). 															
hu2020xtreme	2020	{XTREME}: {A} Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalisation	http://proceedings.mlr.press/v119/hu20b.html	Done	practical	across language	finetune-train/test	covariate	naturally occurring shifts	multitask	Dennis	Dieuwke	Benchmark for zero-shot cross-lingual transfer															
elangovan-etal-2021-memorization	2021	Memorization vs. generalization: quantifying data leakage in NLP performance evaluation	https://arxiv.org/pdf/2102.01818.pdf	Done	intrinsic	robustness	train-test	covariate	natural data splits	named entity recognition (NER)	Dennis	Dieuwke	They test to what extend train / test overlap affects model performance on NER / RE? Might still interesting due to the memorization vs. generalization angle															
yogatama2019learning	2019	Learning and evaluating general linguistic intelligence	https://arxiv.org/pdf/1901.11373.pdf	Done	cognitive	robustness	pretrain-train	covariate	naturally occurring shifts	NLI	Dennis	Dieuwke	Test generalization by training from scratch / fine-tuning pre-trained models on show how pre-training impacts performance on other tasks / how much data is needed to perform well															
yogatama2019learning	2019	Learning and evaluating general linguistic intelligence	https://arxiv.org/pdf/1901.11373.pdf	Done	cognitive	robustness	finetune-train/test	covariate	naturally occurring shifts	question answering	Dennis	Dieuwke	Also check generalization to other datasets of the same task															
wong2007generalisation	2007	Generalisation towards combinatorial productivity in language acquisition by simple recurrent networks	https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4227538&casa_token=Tq0pu6906bsAAAAA:FkRWwTXUPiym5ecgaNFYQfC8T4uzQnLQmvxTRR6cYKKHrks6KwDeWI1qkEPrl2dHKwxEZDH4pXWU&tag=1	Done	cognitive	structural	train-test	covariate	fully generated/selected	syntactic tests	Dennis	Dieuwke	Oldschool RNN experiment testing their generalization w.r.t. combinatorial productivity by feeding "sentence templates" into the network and predicting the next word (category)															
geiger2019posing	2019	Posing fair generalization tasks for natural language inference	https://arxiv.org/pdf/1911.00811.pdf	Done	practical	structural	train-test	covariate	fully generated/selected	NLI	Dennis	Dieuwke	Generate synthetic NLI task where generalizable rules are ensured to be  learnable from train (but difficulty on test is ensured nevertheless)															
wang2019super	2019	Super{GLUE}: {A} Stickier Benchmark for General-Purpose Language Understanding Systems	https://proceedings.neurips.cc/paper/2019/file/4496bf24afe7fab6f046bf4923da8de6-Paper.pdf	Done	intrinsic	across task	pretrain-train	label	naturally occurring shifts	multitask	Christos	Dieuwke	Assumed shift or label shift															
koh2021wilds	2021	Wilds: {A} benchmark of in-the-wild distribution shifts	https://arxiv.org/abs/2012.07421	Done	practical	across domain	train-test	covariate	naturally occurring shifts	toxicity classification	Dieuwke	Christos																
koh2021wilds	2021	Wilds: {A} benchmark of in-the-wild distribution shifts	https://arxiv.org/abs/2012.07421	Done	practical	across domain	train-test	covariate	naturally occurring shifts	sentiment analysis	Christos	Dennis																
koh2021wilds	2021	Wilds: {A} benchmark of in-the-wild distribution shifts	https://arxiv.org/abs/2012.07421	Done	practical	across domain	train-test	covariate	naturally occurring shifts	toxicity classification	Christos	Dennis			https://arxiv.org/abs/2012.07421	First check done	practical	across domain	train-test	covariate	naturally occurring shifts	sentiment analysis	Christos					
nangia-bowman-2019-human	2019	They add a new training stage called pre-finetuning with huge amount of data on a number of enormous tasks. This stage is after pretraining, before finetuning (for task-specific knowledge). So basically, I think none of the locus options work for this and it is more. like prefinetune-finetune!	https://aclanthology.org/P19-1449	Done	cognitive	across task	pretrain-train	label	naturally occurring shifts	multitask	Dennis	Dieuwke	Paper about finding a good estimate of human performance on GLUE; however BERTs on differently-sized finetune sets are also tested															
nye2020learning	2020	Learning compositional rules via neural program synthesis	https://proceedings.neurips.cc/paper/2020/file/7a685d9edd95508471a9d3d6fcace432-Paper.pdf	Done	cognitive	compositional	train-test	covariate	fully generated/selected	seq2seq synthetic	Dennis	Florian	Neuro-symbolic program synthesis model tested on SCAN and number words															
plank2016non	2016	What to do about non-standard (or non-canonical) language in {NLP}		Done	fairness	robustness	train-test	covariate	naturally occurring shifts	POStagging	Dieuwke	Florian																
kaushik2019learning	2019	Learning The Difference That Makes A Difference With Counterfactually-Augmented Data	https://arxiv.org/pdf/1909.12434.pdf	Done	intrinsic	robustness	train-test	covariate	generated shifts	NLI	Yanai	Dieuwke	The shift is mostly natural, since the examples are minimally modified from existing, natural examples (but thus these are not completely neutral															
kaushik2019learning	2019	Learning The Difference That Makes A Difference With Counterfactually-Augmented Data	https://arxiv.org/pdf/1909.12434.pdf	Done	intrinsic	robustness	finetune-train/test	covariate	generated shifts	NLI	Yanai	Dieuwke	The shift is mostly natural, since the examples are minimally modified from existing, natural examples (but thus these are not completely neutral)															
razeghi2022impact	2022	Impact of pretraining term frequencies on few-shot reasoning		Done	intrinsic	structural	pretrain-test	covariate	generated shifts	reasoning	Yanai	Dieuwke																
yang2020strongly	2020	Strongly incremental constituency parsing with graph neural networks	https://proceedings.neurips.cc/paper/2020/file/f7177163c833dff4b38fc8d2872f1ec6-Paper.pdf	Done	cognitive	structural	train-test	assumed	naturally occurring shifts	constituency parsing	Dennis	Zhijing																
bhargava-etal-2021-generalization	2021	Generalization in {NLI}: {W}ays (Not) To Go Beyond Simple Heuristics	https://aclanthology.org/2021.insights-1.18	Done	intrinsic	robustness	all	double	generated shifts	NLI	Dieuwke	Florian	Comparison of multiple different pretraining architectures on the MNLI challenge set HANS															
liu2019roberta	2019	{RoBERTa}: {A} Robustly Optimized {BERT} Pretraining Approach	http://arxiv.org/abs/1907.11692	Done	practical	across task	pretrain-train	label	naturally occurring shifts	multitask	Yanai	Dieuwke	Paper introduces a new pretraining method, and checks how well it generalises when finetuned on different tasks. Ergo: label shift between pretrain and train															
merity2017pointer	2017	Pointer Sentinel Mixture Models	https://openreview.net/forum?id=Byj72udxe	Done	practical	structural	train-test	covariate	natural data splits	language modelling	Yanai	Dieuwke	Investigation of how two different types of networks behave for rare words.															
chowdhery2022palm	2022	Palm: Scaling language modeling with pathways	https://arxiv.org/pdf/2204.02311.pdf	Flag for further review	practical	across task	pretrain-test	assumed	naturally occurring shifts	multitask	Mikel		GPT-3 style in context learning. They also have multilingual experiments but same consideration as for li2021xglm															
chowdhery2022palm	2022	Palm: Scaling language modeling with pathways	https://arxiv.org/pdf/2204.02311.pdf	Flag for further review	practical	across task	finetune-train/test	label	naturally occurring shifts	multitask	Mikel	Dennis	Finetuning on SuperGLUE															
khishigsuren2022using	2022	Using Linguistic Typology to Enrich Multilingual Lexicons: the Case of Lexical Gaps in Kinship	https://doi.org/10.48550/arXiv.2204.05049	Done	practical	across domain	pretrain-test	covariate	generated shifts	machine translation	Huygaa	Dennis	Paper tested Google Translate in the kinship domain. The test data was manually translated from British National Corpus 															
srivastava2022beyond	2022	Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models	https://arxiv.org/pdf/2206.04615.pdf	Done	practical	across task	pretrain-test	assumed	generated shifts	multitask	Dennis	Dieuwke	There are so many tasks in BigBench it's insane (https://github.com/google/BIG-bench/blob/main/bigbench/benchmark_tasks/README.md). How should we treat this? 															
srivastava2022beyond	2022	Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models	https://arxiv.org/pdf/2206.04615.pdf	Done	fairness	robustness	pretrain-test	assumed	generated shifts	multitask	Dennis	Dieuwke	Social bias experiments															
srivastava2022beyond	2022	Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models	https://arxiv.org/pdf/2206.04615.pdf	Done	practical	across language	pretrain-test	assumed	generated shifts	multitask	Dennis	Dieuwke	Experiments to non-english languages, explicit discussion of this data not being present in the training data is included in the test															
lazaridou2021mind	2021	Mind the gap: {A}ssessing temporal generalization in neural language models	https://proceedings.neurips.cc/paper/2021/file/f5bf0ba0a17ef18f9607774722f5698c-Paper.pdf	Done	practical	across domain	train-test	full	naturally occurring shifts	language modelling	Mario	Dieuwke																
lazaridou2021mind	2021	Mind the gap: {A}ssessing temporal generalization in neural language models	https://proceedings.neurips.cc/paper/2021/file/f5bf0ba0a17ef18f9607774722f5698c-Paper.pdf	Done	practical	across domain	finetune-train/test	full	generated shifts	question answering	Mario	Dieuwke																
malinin2021shifts	2021	{A} Dataset of Real Distributional Shift Across Multiple Large-Scale Tasks	https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/ad61ab143223efbc24c7d2583be69251-Abstract-round2.html	Done	practical	across domain	train-test	covariate	natural data splits	machine translation	Dieuwke	Dennis	Experiments with within and out-of-domain machine translation in Russian and English															
rumelhart1986learning	1986	On learning the past tenses of {E}nglish verbs		Done	cognitive	structural	train-test	covariate	naturally occurring shifts	morphological inflection	Verna	Dieuwke	I think only the "Transfer to Novel Verbs" experiments qualify for being included here. I think they are a form of OOD generalisation because they are specifically low-frequency verbs, so the train-test distribution is not IID.															
li-etal-2021-leveraging	2021	Leveraging Paradigmatic Information in Inflection Acceptability Prediction: The {JHU-SFU} Submission to {SIGMORPHON} Shared Task 0.2	https://drive.google.com/file/d/13G0gweT_7b4-uQoEs3bEMc9ftmxcV_1i/view	Done	cognitive	across language	train-test	covariate	generated shifts	morphological inflection	Huygaa	Dieuwke	This system was submitted to part 2 of the shared task															
nllb2022no	2022	No Language Left Behind: Scaling Human-Centered Machine Translation	https://doi.org/10.48550/arXiv.2207.04672	Done	fairness	across domain	all	covariate	naturally occurring shifts	machine translation	Verna	Dieuwke	Out of domain experiments. Not sure about the locus: the monolingual data vs the mined bitext vs the test data might be a "full shift". 															
nllb2022no	2022	No Language Left Behind: Scaling Human-Centered Machine Translation	https://doi.org/10.48550/arXiv.2207.04672	Done	fairness	across language	all	covariate	naturally occurring shifts	machine translation	Verna	Dieuwke	Zero-shot performance for language pairs unseen together during training.															
ludwig-etal-2022-improving	2022	Improving Generalization of Hate Speech Detection Systems to Novel Target Groups via Domain Adaptation	https://aclanthology.org/2022.woah-1.4	Done	practical	across domain	train-test	covariate	naturally occurring shifts	hate speech detection	Dieuwke	Florian	They consider two different scenarios, where the shift has a different locus (e.g. comparing models trained from scratch with pretrained models)															
ludwig-etal-2022-improving	2022	Improving Generalization of Hate Speech Detection Systems to Novel Target Groups via Domain Adaptation	https://aclanthology.org/2022.woah-1.4	Done	practical	across domain	finetune-train/test	covariate	naturally occurring shifts	hate speech detection	Dieuwke	Florian																
sachdeva-etal-2022-targeted	2022	Targeted Identity Group Prediction in Hate Speech Corpora	https://aclanthology.org/2022.woah-1.22	Done	practical	across domain	all	double	naturally occurring shifts	hate speech detection	Dieuwke	Florian																
wei-etal-2022-compositional	2022	Compositional Generalization for Kinship Prediction through Data Augmentation	https://aclanthology.org/2022.wnu-1.2.pdf	Flag for further review		compositional	train-test	assumed		kinship prediction	Dieuwke		Not really sure what to do with this data-augmentation study. Since compositional generalisation tests typically rely strongly on leaving out specific things from the training dataset, can we still consider these data aumentation setups to evaluate compositional generalisation?															
ganti-etal-2022-narrative	2022	Narrative Detection and Feature Analysis in Online Health Communities	https://aclanthology.org/2022.wnu-1.7	Flag for further review		across domain	finetune-train/test	covariate	naturally occurring shifts	narrative detection	Dieuwke		Use BERT-based classifiers to predict narrative from annotated corpora with data from different health organisations (?). Not sure if this actually constitutes a covariate shift.															
liu-prudhommeaux-2022-data	2022	Data-driven Model Generalizability in Crosslinguistic Low-resource Morphological Segmentation	https://aclanthology.org/2022.tacl-1.23	Done	cognitive	structural	train-test	covariate	generated shifts	morphological segmentation	Huygaa	Dieuwke																
guan-etal-2022-lot	2022	{LOT}: A Story-Centric Benchmark for Evaluating {C}hinese Long Text Understanding and Generation	https://aclanthology.org/2022.tacl-1.25	Done	practical	robustness	pretrain-train	assumed	naturally occurring shifts	NLU	Dieuwke	Karim	Comparison of different pretraining procedures to generalise to long texts, multiple tasks. The finetune train/test data is definitely longer than normal benchmarks, but I am unsure if it is also longer than the training data. I think it probably is not, and it isn't checked. As far as I can see, everything is furthermore done in auto-completion mode, so I mark this "assumed shift".															
tamari-etal-2022-dyna	2022	{D}yna-b{A}b{I}: unlocking b{A}b{I}{'}s potential with dynamic synthetic benchmarking	https://aclanthology.org/2022.starsem-1.9	Done	intrinsic	compositional	train-test	covariate	fully generated/selected	story understanding	Dieuwke	Florian																
tamari-etal-2022-dyna	2022	{D}yna-b{A}b{I}: unlocking b{A}b{I}{'}s potential with dynamic synthetic benchmarking	https://aclanthology.org/2022.starsem-1.9	Done	intrinsic	compositional	finetune-train/test	covariate	fully generated/selected	story understanding	Dieuwke	Florian																
de-varda-zamparelli-2022-multilingualism	2022	Multilingualism Encourages Recursion: a Transfer Study with m{BERT}	https://aclanthology.org/2022.sigtyp-1.1	Done	intrinsic	across language	pretrain-test	full	generated shifts	language modelling	Dieuwke	Leila	BERT and MBERT zero-shot generalisation to different (synthetic) languages, without any finetuning															
kodner-khalifa-2022-sigmorphon	2022	{SIGMORPHON}{--}{U}ni{M}orph 2022 Shared Task 0: Modeling Inflection in Language Acquisition	https://aclanthology.org/2022.sigmorphon-1.18	Done	cognitive	structural	train-test	covariate	generated shifts	morphological inflection	Tiago	Huygaa	Czeh and Polish, and lemma overlap experiment															
kodner-khalifa-2022-sigmorphon	2022	{SIGMORPHON}{--}{U}ni{M}orph 2022 Shared Task 0: Modeling Inflection in Language Acquisition	https://aclanthology.org/2022.sigmorphon-1.18	Done	cognitive	structural	train-test	covariate	natural data splits	morphological inflection	Dieuwke	Huygaa	Also here the dimension of sample efficiency is included. I don't think there is an actual shift here though, things are uniformly sampled from the corpus..															
kodner-etal-2022-sigmorphon	2022	{SIGMORPHON}{--}{U}ni{M}orph 2022 Shared Task 0: Generalization and Typologically Diverse Morphological Inflection	https://aclanthology.org/2022.sigmorphon-1.19	Done	cognitive	structural	train-test	label	natural data splits	morphological inflection	Dieuwke	Huygaa	Label overlap experiments															
kodner-etal-2022-sigmorphon	2022	{SIGMORPHON}{--}{U}ni{M}orph 2022 Shared Task 0: Generalization and Typologically Diverse Morphological Inflection	https://aclanthology.org/2022.sigmorphon-1.19	Done	cognitive	structural	train-test	label	generated shifts	morphological inflection	Tiago	Huygaa	Czech and Polish, Label overlap experiments															
kodner-etal-2022-sigmorphon	2022	{SIGMORPHON}{--}{U}ni{M}orph 2022 Shared Task 0: Generalization and Typologically Diverse Morphological Inflection	https://aclanthology.org/2022.sigmorphon-1.19	Done	cognitive	structural	train-test	full	natural data splits	morphological inflection	Tiago	Huygaa	For other languages, Neither overlap experiments															
kodner-etal-2022-sigmorphon	2022	{SIGMORPHON}{--}{U}ni{M}orph 2022 Shared Task 0: Generalization and Typologically Diverse Morphological Inflection	https://aclanthology.org/2022.sigmorphon-1.19	Done	cognitive	structural	train-test	full	generated shifts	morphological inflection	Dieuwke	Huygaa	Czech and Polish data were automatically generated from morphological transducer															
sauer-etal-2022-knowledge	2022	Knowledge Distillation Meets Few-Shot Learning: An Approach for Few-Shot Intent Classification Within and Across Domains	https://aclanthology.org/2022.nlp4convai-1.10	First check done	practical	across domain	pretrain-test	assumed	naturally occurring shifts	intent classification	Dennis		Testing cross-domain generalization by modifying test splits of datasets and using several datasets' test splits. Furthermore, they test the generalization abilities of both a pre-trained teacher network and a distilled (trained from scratch) student network															
sauer-etal-2022-knowledge	2022	Knowledge Distillation Meets Few-Shot Learning: An Approach for Few-Shot Intent Classification Within and Across Domains	https://aclanthology.org/2022.nlp4convai-1.10	Done	practical	across domain	train-test	covariate	natural data splits	NLU	Dennis	Zhijing																
chakravarthy-etal-2022-systematicity	2022	Systematicity Emerges in Transformers when Abstract Grammatical Roles Guide Attention	https://aclanthology.org/2022.naacl-srw.1	Done	cognitive	compositional	train-test	covariate	fully generated/selected	seq2seq synthetic	Dennis	Dieuwke	Test linguistically motivated modification for transformers on SCAN															
yan-etal-2022-robustness	2022	On the Robustness of Reading Comprehension Models to Entity Renaming	https://aclanthology.org/2022.naacl-main.37	Done	practical	robustness	all	double	generated shifts	reading comprehension	Florian	Dieuwke	Robustness experiments (Locus shift "all" because they take a pretrained language model, finetune it on MRC task and then evaluate on a test set with unseen entities; Shift type "double" because from pretrain to train, the task changes (i.e. label shift) and from train to test the covariate distribution changes)															
maharana-bansal-2022-curriculum	2022	On Curriculum Learning for Commonsense Reasoning	https://aclanthology.org/2022.naacl-main.72	Flag for further review	intrinsic	robustness	train-test	assumed	naturally occurring shifts	question answering	Janie	Dennis	main description of generalisation test at the end of section 5.1															
chronopoulou-etal-2022-efficient	2022	Efficient Hierarchical Domain Adaptation for Pretrained Language Models	https://aclanthology.org/2022.naacl-main.96	Done	practical	across domain	finetune-train/test	covariate	naturally occurring shifts	language modelling	Dennis	Leila	Propose a hierarchical set of model adapters that are added to frozen pre-trained models to improve language modelling on different domains															
sawhney-etal-2022-ciaug	2022	{CIA}ug: Equipping Interpolative Augmentation with Curriculum Learning	https://aclanthology.org/2022.naacl-main.127	Done	practical	across task	finetune-train/test	full	naturally occurring shifts	named entity recognition (NER)	Dennis	Zhijing	Propose a modification of mixup training for faster converge and better generalization. Tested on NER and GLUE in different lamguages, fine-tuning (m)BERTs.															
sawhney-etal-2022-ciaug	2022	{CIA}ug: Equipping Interpolative Augmentation with Curriculum Learning	https://aclanthology.org/2022.naacl-main.127	Done	practical	across task	finetune-train/test	full	naturally occurring shifts	text classification	Dennis	Zhijing																
ye-etal-2022-sparse	2022	Sparse Distillation: Speeding Up Text Classification by Using Bigger Student Models	https://aclanthology.org/2022.naacl-main.169	Done	practical	across task	pretrain-train	label	naturally occurring shifts	text classification	Dieuwke	Dennis	Knowledge distillation paper, comparison between their distilled models and several pretrained models on out-of-domain adaptation in the finetuning stage															
ye-etal-2022-sparse	2022	Sparse Distillation: Speeding Up Text Classification by Using Bigger Student Models	https://aclanthology.org/2022.naacl-main.169	Done	practical	across domain	all	double	naturally occurring shifts	text classification	Dieuwke	Dennis	Knowledge distillation paper, comparison between their distilled models and several pretrained models on a bunch of downstream text classification tasks															
li-etal-2022-low	2022	Low Resource Style Transfer via Domain Adaptive Meta Learning	https://aclanthology.org/2022.naacl-main.220	Flag for further review	practical	across domain	finetune-train/test	covariate	naturally occurring shifts	other	Dieuwke		Domain adaptaption method for style transfer to different domains. (I had a lot of trouble annotating this, second check from a meta-learning expert appreciated).															
lee-etal-2022-really	2022	Does it Really Generalize Well on Unseen Data? Systematic Evaluation of Relational Triple Extraction Methods	https://aclanthology.org/2022.naacl-main.282	Done	practical	robustness	finetune-train/test	covariate	natural data splits	relation extraction	Florian	Dieuwke	Relation-extraction experiments on seen and unseen triplets, to compare memorisation vs generalisation.															
zhong-etal-2022-proqa	2022	{P}ro{QA}: Structural Prompt-based Pre-training for Unified Question Answering	https://aclanthology.org/2022.naacl-main.313	Done	practical	across domain	pretrain-train	covariate	generated shifts	question answering	Karim	Dieuwke	Prompt-based QA - Fine-tune a pre-trained model on structured prompts															
zhong-etal-2022-proqa	2022	{P}ro{QA}: Structural Prompt-based Pre-training for Unified Question Answering	https://aclanthology.org/2022.naacl-main.313	First check done	practical	across domain	pretrain-test	covariate	generated shifts	question answering	Karim	Dennis	Prompt-based QA - Test a pre-trained model on structured prompts without finetuning															
zhong-etal-2022-proqa	2022	{P}ro{QA}: Structural Prompt-based Pre-training for Unified Question Answering	https://aclanthology.org/2022.naacl-main.313	First check done	practical	across domain	finetune-train/test	covariate	generated shifts	question answering	Karim	Dennis	Prompt-based QA - Test a pre-trained model on structured prompts after seeing few examples															
qiu-etal-2022-improving	2022	Improving Compositional Generalization with Latent Structure and Data Augmentation	https://aclanthology.org/2022.naacl-main.323	Flag for further review	practical	compositional	train-test	assumed	fully generated/selected	seq2seq synthetic	Dieuwke	Verna	Grammar-based augmentation technique, where data is added to the training data by sampling from the learned grammar. This row represents experiments on SCAN and COGs															
qiu-etal-2022-improving	2022	Improving Compositional Generalization with Latent Structure and Data Augmentation	https://aclanthology.org/2022.naacl-main.323	Flag for further review	practical	compositional	train-test	assumed	natural data splits	seq2seq synthetic	Dieuwke	Verna	Grammar-based augmentation technique, where data is added to the training data by sampling from the learned grammar. This row represents experiments on GeoQuery and SMCalFlow															
gupta-etal-2022-show	2022	Show, Donâ€™t Tell: Demonstrations Outperform Descriptions for Schema-Guided Task-Oriented Dialogue	https://aclanthology.org/2022.naacl-main.336	Done	practical	across domain	finetune-train/test	covariate	naturally occurring shifts	dialogue	Dieuwke	Mario	Finetune T5 on dialogue in a "leave-one-dataset-out" setting, thus evaluating domain generalisation in the finetuning stage. Only one pretrained model is considered.															
li-etal-2022-quantifying	2022	Quantifying Adaptability in Pre-trained Language Models with 500 Tasks	https://aclanthology.org/2022.naacl-main.346	Done	intrinsic	robustness	all	double	natural data splits	NLU	Dieuwke	Dennis	Investigation of different test classes containing different types of generalised quantifiers, in different languages. Because the focus here is on generalisation on a finetuning task, but there is also a comparison between different pretraining architecture, this entry is marked a double shift.															
li-etal-2022-quantifying	2022	Quantifying Adaptability in Pre-trained Language Models with 500 Tasks	https://aclanthology.org/2022.naacl-main.346	Done	intrinsic	robustness	all	double	generated shifts	NLU	Dieuwke	Dennis	Authors also propose a new adversarial generalised quantifiers NLI set, and evaluate 7 different types of pretraining architectures															
jin-etal-2022-lifelong-pretraining	2022	Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora	https://aclanthology.org/2022.naacl-main.351.pdf	Flag for further review	practical	across domain	finetune-train/test	covariate	naturally occurring shifts	multitask	Dennis	Zhijing																
zhao-etal-2022-domain	2022	Domain-Oriented Prefix-Tuning: Towards Efficient and Generalizable Fine-tuning for Zero-Shot Dialogue Summarization	https://aclanthology.org/2022.naacl-main.357	Done	practical	across domain	finetune-train/test	full	naturally occurring shifts	summarisation	Mario	Janie	To be precise: assumed covariate + label -- but the assumption is quite solid given the addition of artificial prompts. There is also an artificial/generated component in the prompting experiments, but I'd say the main source of shift is the use of different existing dialogue summarisation datasets															
cui-etal-2022-generalized-quantifiers	2022	Generalized Quantifiers as a Source of Error in Multilingual {NLU} Benchmarks	https://aclanthology.org/2022.naacl-main.359	Done	cognitive	robustness	finetune-train/test	assumed	generated shifts	NLI	Dennis	Dieuwke	Authors create CQNLI for testing generalised quantifiers in NLI tasks (which I now annotated as robustness?)															
gururangan-etal-2022-demix	2022	{DEM}ix Layers: Disentangling Domains for Modular Language Modeling	https://aclanthology.org/2022.naacl-main.407	Done	practical	across domain	train-test	covariate	naturally occurring shifts	language modelling	Dennis	Zhijing																
gaspers-etal-2022-temporal	2022	Temporal Generalization for Spoken Language Understanding	https://aclanthology.org/2022.naacl-industry.5	Done	practical	across domain	all	double	naturally occurring shifts	spoken language understanding	Florian	Dieuwke	The locus is all and the shift type is "double", since pretrained language models were finetuned on data from many periods (and on a different set of tasks, constituting a label shift) and then evaluated on data from an unseen period (covariate shift)															
pratapa-etal-2022-multilingual	2022	Multilingual Event Linking to {W}ikidata	https://aclanthology.org/2022.mia-1.5	Done	practical	across language	finetune-train/test	covariate	naturally occurring shifts	other	Dennis	Zhijing																
kumar-etal-2022-empirical	2022	An Empirical study to understand the Compositional Prowess of Neural Dialog Models	https://aclanthology.org/2022.insights-1.21	Done	intrinsic	compositional	train-test	covariate	natural data splits	dialogue	Dieuwke	Mario	Systematicity, productivity and substitutivity experiments with a neural dialogue model															
yang-etal-2022-seqzero	2022	{SEQZERO}: Few-shot Compositional Semantic Parsing with Sequential Prompts and Zero-shot Models	https://aclanthology.org/2022.findings-naacl.5	Done	practical	compositional	pretrain-test	full	natural data splits	question answering	Dennis	Zhijing	Dataset is a database of a US geography database, which is not natural language, but still natural?															
yang-etal-2022-seqzero	2022	{SEQZERO}: Few-shot Compositional Semantic Parsing with Sequential Prompts and Zero-shot Models	https://aclanthology.org/2022.findings-naacl.5	Done	practical	compositional	finetune-train/test	full	natural data splits	question answering	Dennis	Zhijing																
parmar-etal-2022-boxbart	2022	In-{B}o{XBART}: Get Instructions into Biomedical Multi-Task Learning	https://aclanthology.org/2022.findings-naacl.10	Done	practical	across task	finetune-train/test	full	naturally occurring shifts	multitask	Dennis	Huygaa	Tested on a bunch of biomedical tasks, unified model															
gan-etal-2022-measuring	2022	Measuring and Improving Compositional Generalization in Text-to-{SQL} via Component Alignment	https://aclanthology.org/2022.findings-naacl.62	Done	intrinsic	compositional	all	double	natural data splits	semantic parsing / role labelling	Florian	Verna	Since multiple pre-trained embeddings are used, I opted for double shift and locus "all"															
sharma-buduru-2022-fatnet	2022	{FA}t{N}et: Cost-Effective Approach Towards Mitigating the Linguistic Bias in Speaker Verification Systems	https://aclanthology.org/2022.findings-naacl.93	Flag for further review	fairness	robustness	train-test	covariate	naturally occurring shifts	other	Dennis	Zhijing																
sharma-buduru-2022-fatnet	2022	{FA}t{N}et: Cost-Effective Approach Towards Mitigating the Linguistic Bias in Speaker Verification Systems	https://aclanthology.org/2022.findings-naacl.93	Done	cognitive	robustness	train-test	covariate	naturally occurring shifts	other	Dennis	Zhijing																
shao-etal-2022-low	2022	Low-resource Entity Set Expansion: A Comprehensive Study on User-generated Text	https://aclanthology.org/2022.findings-naacl.100	Done	practical	across domain	train-test	covariate	naturally occurring shifts	other	Dennis	Zhijing	Task is entity set expansion, tested cross-domain for English															
shao-etal-2022-low	2022	Low-resource Entity Set Expansion: A Comprehensive Study on User-generated Text	https://aclanthology.org/2022.findings-naacl.100	Done	practical	across domain	finetune-train/test	covariate	naturally occurring shifts	other	Dennis	Zhijing																
rosin-radinsky-2022-temporal	2022	Temporal Attention for Language Models	https://aclanthology.org/2022.findings-naacl.112	Done	practical	across task	pretrain-train	label	naturally occurring shifts	semantic change detection	Florian	Dieuwke	Authors use a pretrained model and test if their finetuning method for semantic change detection improves over SOTA. The focus is really on the finetuning stage, not considering multiple pretraining procedures, but there is no shift in the finetuning stage, so this entry is marked as a label shift from pretrain to train.															
liu-etal-2022-challenges	2022	Challenges in Generalization in Open Domain Question Answering	https://aclanthology.org/2022.findings-naacl.155	Done	practical	compositional	train-test	covariate	natural data splits	question answering	Dieuwke	Florian																
liscio-etal-2022-cross	2022	Cross-Domain Classification of Moral Values	https://aclanthology.org/2022.findings-naacl.209	Done	cognitive	across domain	train-test	assumed	naturally occurring shifts	text classification	Florian	Dieuwke	Topline experiment, Generalizability experiment															
liscio-etal-2022-cross	2022	Cross-Domain Classification of Moral Values	https://aclanthology.org/2022.findings-naacl.209	Done	cognitive	across domain	all	double	naturally occurring shifts	text classification	Florian	Dieuwke	Transferability experiment															
yang-etal-2022-challenges	2022	Challenges to Open-Domain Constituency Parsing	https://aclanthology.org/2022.findings-acl.11	Done	intrinsic	across domain	train-test	covariate	naturally occurring shifts	constituency parsing	Dieuwke	Dennis	Investigation of how well constituency parsers trained on PTB generalise to other domains															
yang-etal-2022-challenges	2022	Challenges to Open-Domain Constituency Parsing	https://aclanthology.org/2022.findings-acl.11	Done	intrinsic	across domain	all	double	naturally occurring shifts	constituency parsing	Dieuwke	Dennis	The same authors also present a comparison between with and without pretraining with BERT, hereby evaluating how useful BERT representations are to the task.															
hua-wang-2022-efficient	2022	Efficient Argument Structure Extraction with Transfer Learning and Active Learning	https://aclanthology.org/2022.findings-acl.36	Done	practical	across domain	finetune-train/test	covariate	naturally occurring shifts	other	Dennis	Huygaa	Argument structure extraction using pre-trained models which are fine-tuned / trained using active learning															
alkiek-etal-2022-classification	2022	Classification without (Proper) Representation: Political Heterogeneity in Social Media and Its Implications for Classification and Behavioral Analysis	https://aclanthology.org/2022.findings-acl.43	Done	fairness	across domain	train-test	full	naturally occurring shifts	text classification	Dennis	Zhijing	Modelling of political affiliation, behavior, toxicity and more across datasets. I put full as shift type since e.g. the political affiliation for the same user might be different depending on the dataset the classifier was trained on.															
alkiek-etal-2022-classification	2022	Classification without (Proper) Representation: Political Heterogeneity in Social Media and Its Implications for Classification and Behavioral Analysis	https://aclanthology.org/2022.findings-acl.43	Done	fairness	across domain	finetune-train/test	full	naturally occurring shifts	text classification	Dennis	Zhijing																
vu-etal-2022-domain	2022	Domain Generalisation of {NMT}: Fusing Adapters with Leave-One-Domain-Out Training	https://aclanthology.org/2022.findings-acl.49	Done	practical	across domain	pretrain-test	covariate	naturally occurring shifts	machine translation	Florian	Dieuwke	zero-shot experiments															
vu-etal-2022-domain	2022	Domain Generalisation of {NMT}: Fusing Adapters with Leave-One-Domain-Out Training	https://aclanthology.org/2022.findings-acl.49	Done	practical	across domain	finetune-train/test	covariate	naturally occurring shifts	machine translation	Florian	Dieuwke	finetuning experiments															
atwell-etal-2022-change	2022	The Change that Matters in Discourse Parsing: Estimating the Impact of Domain Shift on Parser Error	https://aclanthology.org/2022.findings-acl.68	First check done	practical	across domain	finetune-train/test	covariate	naturally occurring shifts	relation extraction	Dennis		Task is discourse classification, so I put relation extraction here (since it is about identifiying discourse relations?).  Model seems to be finetuned BERTs based on appendix B, but I am not entirely sure.															
reuel-etal-2022-measuring	2022	Measuring the Language of Self-Disclosure across Corpora	https://aclanthology.org/2022.findings-acl.83	Done	practical	across domain	train-test	covariate	naturally occurring shifts	text classification	Janie	Dieuwke	Cross-domain generalisation experiments for self-disclosure tasks															
liu-etal-2022-multi	2022	Multi-Stage Prompting for Knowledgeable Dialogue Generation	https://aclanthology.org/2022.findings-acl.104	Done	practical	across domain	finetune-train/test	covariate	naturally occurring shifts	dialogue	Dennis	Kaiser	Evaluate dialogue generation on two test sets: One with topics seen in training set, one without															
liu-etal-2022-multi	2022	Multi-Stage Prompting for Knowledgeable Dialogue Generation	https://aclanthology.org/2022.findings-acl.104	Done	practical	across domain	pretrain-test	covariate	naturally occurring shifts	dialogue	Dennis	Kaiser																
mueller-etal-2022-coloring	2022	Coloring the Blank Slate: Pre-training Imparts a Hierarchical Inductive Bias to Sequence-to-sequence Models	https://aclanthology.org/2022.findings-acl.106	Done	cognitive	structural	all	double	generated shifts	diagnostic task	Verna	Dieuwke	Task is passivisation, so doesn't really fit existing tags.															
khot-etal-2022-hey	2022	Hey AI, Can You Solve Complex Tasks by Talking to Agents?	https://aclanthology.org/2022.findings-acl.142	Done	cognitive	compositional	train-test	covariate	generated shifts	question answering	Janie	Dieuwke	section 5.3 is the only small generalization task: test set with questions consisting of unseen combinations of question units seen in training 															
khot-etal-2022-hey	2022	Hey AI, Can You Solve Complex Tasks by Talking to Agents?	https://aclanthology.org/2022.findings-acl.142	Done	cognitive	compositional	finetune-train/test	covariate	generated shifts	question answering	Janie	Mario	for the T5 fine tuned model															
lasri-etal-2022-bert	2022	Does {BERT} really agree ? Fine-grained Analysis of Lexical Dependence on a Syntactic Task	https://aclanthology.org/2022.findings-acl.181	Done	intrinsic	structural	pretrain-test	assumed	generated shifts	syntactic tests	Karim	Dieuwke	Test whether BERT captures morphological agreement in a MLM setting, on selected sentences															
libovicky-etal-2022-dont	2022	Why don{'}t people use character-level machine translation?	https://aclanthology.org/2022.findings-acl.194	Done	practical	across domain	train-test	covariate	naturally occurring shifts	machine translation	Florian	Dieuwke	Comparison of character-level vs subword MT models on out-of-domain testset from WMT17 Biometical task and the WMT16IT Domain test															
libovicky-etal-2022-dont	2022	Why don{'}t people use character-level machine translation?	https://aclanthology.org/2022.findings-acl.194	Done	fairness	robustness	train-test	covariate	generated shifts	machine translation	Florian	Dieuwke	Comparison of character-level vs subword MT models on gender bias, using a dataset with (generated) stereotypical and non-stereotypical English sentences.															
libovicky-etal-2022-dont	2022	Why don{'}t people use character-level machine translation?	https://aclanthology.org/2022.findings-acl.194	Done	practical	structural	train-test	covariate	generated shifts	machine translation	Florian	Dieuwke	Comparison of character-level vs subword MT models on morphological generalisation															
jin-etal-2022-cross	2022	How Can Cross-lingual Knowledge Contribute Better to Fine-Grained Entity Typing?	https://aclanthology.org/2022.findings-acl.243	Done	practical	across domain	finetune-train/test	assumed	naturally occurring shifts	information retrieval	Dennis	Leila	Retrieval task, splitting test set in subbuckets with products / queries that do or do not exist in the train set by themselves. I chose naturally occuring shift because although the test set is split, it seems to be more of a fine-grained analysis than trying to split the whole dataset in a challenging way.															
jin-etal-2022-cross	2022	How Can Cross-lingual Knowledge Contribute Better to Fine-Grained Entity Typing?	https://aclanthology.org/2022.findings-acl.243	Done	practical	across domain	finetune-train/test	covariate	natural data splits	information retrieval	Dennis	Leila	Retrieval task, splitting test set in subbuckets with products / queries that do or do not exist in the train set by themselves. I chose naturally occuring shift because although the test set is split, it seems to be more of a fine-grained analysis than trying to split the whole dataset in a challenging way.															
jin-etal-2022-cross	2022	How Can Cross-lingual Knowledge Contribute Better to Fine-Grained Entity Typing?	https://aclanthology.org/2022.findings-acl.243	Done	practical	across language	finetune-train/test	covariate	natural data splits	information retrieval	Dennis	Leila	Retrieval task, splitting test set in subbuckets with products / queries that do or do not exist in the train set by themselves. I chose naturally occuring shift because although the test set is split, it seems to be more of a fine-grained analysis than trying to split the whole dataset in a challenging way.															
xin-etal-2022-zero	2022	Zero-Shot Dense Retrieval with Momentum Adversarial Domain Invariant Representations	https://aclanthology.org/2022.findings-acl.316	Done	practical	across domain	finetune-train/test	covariate	naturally occurring shifts	information retrieval	Dennis	Leila	They finetune BERT in adverserial manner to learn domain invarient representations for across domain generalisation of text retrieval. 															
ghazarian-etal-2022-wrong	2022	What is wrong with you?: Leveraging User Sentiment for Automatic Dialog Evaluation	https://aclanthology.org/2022.findings-acl.331	Done	practical	across domain	finetune-train/test	covariate	naturally occurring shifts	dialogue	Dennis	Janie	changed task to dialogue: think you mixed up labels between 168 an 167. 															
liu-etal-2022-towards	2022	Towards Generalizeable Semantic Product Search by Text Similarity Pre-training on Search Click Logs	https://aclanthology.org/2022.ecnlp-1.26	Done	practical	across domain	finetune-train/test	covariate	naturally occurring shifts	other	Dennis	Leila	Entity identification task, using cross-lingual transfer															
koto-etal-2022-cloze	2022	Cloze Evaluation for Deeper Understanding of Commonsense Stories in {I}ndonesian	https://aclanthology.org/2022.csrr-1.2	Done	practical	across task	pretrain-train	covariate	generated shifts	multiple tasks	Karim	Dieuwke																
koto-etal-2022-cloze	2022	Cloze Evaluation for Deeper Understanding of Commonsense Stories in {I}ndonesian	https://aclanthology.org/2022.csrr-1.2	Done	practical	across language	finetune-train/test	covariate	generated shifts	multiple tasks	Karim	Dieuwke																
amin-etal-2022-shot	2022	Few-Shot Cross-lingual Transfer for Coarse-grained De-identification of Code-Mixed Clinical Texts	https://aclanthology.org/2022.bionlp-1.20	Done	practical	across domain	pretrain-train	covariate	fully generated/selected	named entity recognition (NER)	Karim	Dieuwke																
amin-etal-2022-shot	2022	Few-Shot Cross-lingual Transfer for Coarse-grained De-identification of Code-Mixed Clinical Texts	https://aclanthology.org/2022.bionlp-1.20	Done	practical	across language	finetune-train/test	covariate	generated shifts	named entity recognition (NER)	Karim	Dieuwke																
guriel-etal-2022-morphological	2022	Morphological Reinflection with Multiple Arguments: An Extended Annotation schema and a {G}eorgian Case Study	https://aclanthology.org/2022.acl-short.21	Done	intrinsic	structural	train-test	covariate	generated shifts	morphological inflection	Huygaa	Dieuwke	The authors proposed a new morphological schema with its new data, and blended it with UniMorph in several different way to test the generalisation															
sun-etal-2022-leveraging	2022	Leveraging Explicit Lexico-logical Alignments in Text-to-{SQL} Parsing	https://aclanthology.org/2022.acl-short.31	Done	practical	compositional	finetune-train/test	full	natural data splits	semantic parsing / role labelling	Dennis	Huygaa	I added train-test here for the LSTM decoder that is trained from scratch															
sun-etal-2022-leveraging	2022	Leveraging Explicit Lexico-logical Alignments in Text-to-{SQL} Parsing	https://aclanthology.org/2022.acl-short.31	Done	practical	across domain	train-test	full	natural data splits	semantic parsing / role labelling	Dennis	Huygaa																
patel-etal-2022-revisiting	2022	Revisiting the Compositional Generalization Abilities of Neural Sequence Models	https://aclanthology.org/2022.acl-short.46	Done	practical	compositional	train-test	covariate	fully generated/selected	seq2seq synthetic	Koustuv	Dieuwke	Investigation of how simple changes in SCAN training data impact generalisation behaviour of various seq2seq models															
goldman-etal-2022-un	2022	(Un)solving Morphological Inflection: Lemma Overlap Artificially Inflates Models{'} Performance	https://aclanthology.org/2022.acl-short.96	Done	practical	structural	train-test	covariate	generated shifts	morphological inflection	Huygaa	Dieuwke	The traditional split in morphological inflection was random split, and the authors introduced a lemma-overalap split to test a generalisation ability															
zaharia-etal-2022-domain	2022	Domain Adaptation in Multilingual and Multi-Domain Monolingual Settings for Complex Word Identification	https://aclanthology.org/2022.acl-long.6	Flag for further review	practical	across domain	finetune-train/test	full	naturally occurring shifts	other	Dennis		As far as I understand they try cross-lingual and cross-domain setups, with parts of the final model that are fine-tuned and some that are trained from scratch. Datasets seems natural, but I am very confused about the type of shift to put here															
zaharia-etal-2022-domain	2022	Domain Adaptation in Multilingual and Multi-Domain Monolingual Settings for Complex Word Identification	https://aclanthology.org/2022.acl-long.6	Flag for further review	practical	across language	train-test	full	naturally occurring shifts	other	Dennis																	
zhang-etal-2022-learn	2022	Learn to Adapt for Generalized Zero-Shot Text Classification	https://aclanthology.org/2022.acl-long.39	Flag for further review	practical	robustness	finetune-train/test	full	naturally occurring shifts	text classification	Dennis																	
zhang-etal-2022-learn	2022	Learn to Adapt for Generalized Zero-Shot Text Classification	https://aclanthology.org/2022.acl-long.39	Done	practical	across domain	all	double	natural data splits	text classification	Dennis	Leila																
bai-etal-2022-better	2022	Better Language Model with Hypernym Class Prediction	https://aclanthology.org/2022.acl-long.96	Done	intrinsic	robustness	train-test	covariate	natural data splits	language modelling	Dieuwke	Zhijing	Investigation of class-based curricula to improve generalisation to low-frequency words; Problem: type=robust/domain/this doesn't make sense for papers aiming to improve LM on general domain															
behnke-etal-2022-bias	2022	Bias Mitigation in Machine Translation Quality Estimation	https://aclanthology.org/2022.acl-long.104	Done	practical	robustness	finetune-train/test	covariate	naturally occurring shifts	NLU	Huygaa	Zhijing	Zhijing: I fixed several values; see comment in text															
kavumba-etal-2022-prompt	2022	Are Prompt-based Models Clueless?	https://aclanthology.org/2022.acl-long.166	Done	intrinsic	robustness	finetune-train/test	covariate	generated shifts	NLU	Dieuwke	Dennis	Investigate whether models that are prompted rely on superficial cues (as has been shows also in finetuned models with additional classifier heads)															
zhou-etal-2022-claret	2022	{C}lar{ET}: Pre-training a Correlation-Aware Context-To-Event Transformer for Event-Centric Generation and Classification	https://aclanthology.org/2022.acl-long.183	Flag for further review	practical	across task	all	double	naturally occurring shifts	reasoning	Dennis	Janie	Im unsure about the all and full for locus and data since the pretrain data is a subset of the foundation models training data? section 4 pretraining. However I am inclined to agree since it is a multi-step pipeline and a curated version of the data?															
qin-joty-2022-continual	2022	Continual Few-shot Relation Learning via Embedding Space Regularization and Data Augmentation	https://aclanthology.org/2022.acl-long.198	First check done	cognitive	across task	finetune-train/test	full	natural data splits	relation extraction	Dennis		"Across task" was chosen here since the authors frame an expanding relation set as continual learning on slightly different RE tasks															
jambor-bahdanau-2022-lagr	2022	{LAG}r: Label Aligned Graphs for Better Systematic Generalization in Semantic Parsing	https://aclanthology.org/2022.acl-long.233	Done	practical	compositional	train-test	assumed	generated shifts	semantic parsing / role labelling	Dennis	Huygaa																
mishra-etal-2022-cross	2022	Cross-Task Generalization via Natural Language Crowdsourcing Instructions	https://aclanthology.org/2022.acl-long.244	Done	cognitive	across task	pretrain-test	assumed	naturally occurring shifts	multitask	Verna	Florian	GPT-3 experiments															
mishra-etal-2022-cross	2022	Cross-Task Generalization via Natural Language Crowdsourcing Instructions	https://aclanthology.org/2022.acl-long.244	Done	cognitive	across task	finetune-train/test	assumed	naturally occurring shifts	multitask	Verna	Florian	BART experiments															
ontanon-etal-2022-making	2022	Making Transformers Solve Compositional Tasks	https://aclanthology.org/2022.acl-long.251	Done	practical	compositional	train-test	covariate	fully generated/selected	seq2seq synthetic	Koustuv	Verna	Flagged for review because of questions posed to Koustuv in comments.															
ontanon-etal-2022-making	2022	Making Transformers Solve Compositional Tasks	https://aclanthology.org/2022.acl-long.251	Done	practical	compositional	train-test	covariate	fully generated/selected	semantic parsing / role labelling	Koustuv	Verna	Flagged for review because of questions posed to Koustuv in comments.															
joshi-he-2022-investigation	2022	An Investigation of the (In)effectiveness of Counterfactually Augmented Data	https://aclanthology.org/2022.acl-long.256	Done	practical	robustness	finetune-train/test	assumed	generated shifts	NLI	Koustuv	Dieuwke																
cheng-etal-2022-multilingual	2022	Multilingual Mix: Example Interpolation Improves Multilingual Neural Machine Translation	https://aclanthology.org/2022.acl-long.282	Done	intrinsic	across language	train-test	label	naturally occurring shifts	machine translation	Florian	Dieuwke	Zero shot experiments (Rare example of train-test label shift?)															
cheng-etal-2022-multilingual	2022	Multilingual Mix: Example Interpolation Improves Multilingual Neural Machine Translation	https://aclanthology.org/2022.acl-long.282	Done	intrinsic	robustness	train-test	covariate	generated shifts	machine translation	Florian	Dieuwke	Robustness experiments where they test on noisy data															
dankers-etal-2022-paradox	2022	The Paradox of the Compositionality of Natural Language: A Neural Machine Translation Case Study	https://aclanthology.org/2022.acl-long.286	Done	cognitive	compositional	train-test	covariate	generated shifts	machine translation	Dieuwke	Verna	Compositionality tests in NMT, using natural data with generated shifts															
mehta-etal-2022-improving	2022	Improving Compositional Generalization with Self-Training for Data-to-Text Generation	https://aclanthology.org/2022.acl-long.289	Done	practical	compositional	finetune-train/test	covariate	generated shifts	natural language generation	Huygaa	Dieuwke	The task is data-to-text generation. The self training and evaluation was conducted by partitioning the original dataset into smaller sets and assuming the subsets as unlabeled during self-training and used its actual labels in evaluation.															
zheng-lapata-2022-disentangled	2022	Disentangled Sequence to Sequence Learning for Compositional Generalization	https://aclanthology.org/2022.acl-long.293	Done	cognitive	compositional	train-test	covariate	fully generated/selected	seq2seq synthetic	Dieuwke	Dennis	Experiments with a toy task															
zheng-lapata-2022-disentangled	2022	Disentangled Sequence to Sequence Learning for Compositional Generalization	https://aclanthology.org/2022.acl-long.293	Done	cognitive	compositional	train-test	covariate	fully generated/selected	machine translation	Dieuwke	Dennis	Experiments with COGs															
zheng-lapata-2022-disentangled	2022	Disentangled Sequence to Sequence Learning for Compositional Generalization	https://aclanthology.org/2022.acl-long.293	Done	cognitive	compositional	train-test	covariate	fully generated/selected	semantic parsing / role labelling	Dieuwke	Dennis	Experiments with CFQ															
chalkidis-etal-2022-lexglue	2022	{L}ex{GLUE}: A Benchmark Dataset for Legal Language Understanding in {E}nglish	https://aclanthology.org/2022.acl-long.297	Done	practical	across task	pretrain-train	full	naturally occurring shifts	multiple tasks	Dieuwke	Zhijing	Consider how different models can generalise to a different set of legal tasks. Because of the comparison between models pretrained on legal and non-legal text, I added an extra row.															
chalkidis-etal-2022-lexglue	2022	{L}ex{GLUE}: A Benchmark Dataset for Legal Language Understanding in {E}nglish	https://aclanthology.org/2022.acl-long.297	Done	practical	across domain	pretrain-train	full	naturally occurring shifts	multitask	Dieuwke	Zhijing	Consider how different models can generalise to a set of legal tasks (during finetuning), because of the because some of the experiments are focussed on comparing models pretrained on legal data and models pretrained on non-legal data, I added an exra row for domain generalisation experiments.															
su-etal-2022-rare	2022	Rare and Zero-shot Word Sense Disambiguation using {Z}-Reweighting	https://aclanthology.org/2022.acl-long.323	Done	intrinsic	robustness	finetune-train/test	covariate	natural data splits	word sense disambiguation (WSD)	Huygaa	Dieuwke	This aims to adjust the imbalance of most frequent senses from the training dataset															
min-etal-2022-noisy	2022	Noisy Channel Language Model Prompting for Few-Shot Text Classification	https://aclanthology.org/2022.acl-long.365	Done	practical	robustness	finetune-train/test	label	natural data splits	text classification	Mario	Dieuwke	This is for the first set of experiments in section 6.4, generalisation to unseen labels															
min-etal-2022-noisy	2022	Noisy Channel Language Model Prompting for Few-Shot Text Classification	https://aclanthology.org/2022.acl-long.365	Done	practical	across task	finetune-train/test	full	naturally occurring shifts	text classification	Mario	Dieuwke	This is for the zero-shot experiments in section 6.4. Type could be both task and domain; I labelled as task as it is more general															
min-etal-2022-noisy	2022	Noisy Channel Language Model Prompting for Few-Shot Text Classification	https://aclanthology.org/2022.acl-long.365	Done	practical	across task	pretrain-test	full	naturally occurring shifts	text classification	Mario	Zhijing	Zero-shot experiments															
min-etal-2022-noisy	2022	Noisy Channel Language Model Prompting for Few-Shot Text Classification	https://aclanthology.org/2022.acl-long.365	Done	practical	across task	pretrain-train	full	naturally occurring shifts	text classification	Mario	Zhijing	Tuning experiments															
zhao-etal-2022-bridging	2022	Bridging the Generalization Gap in Text-to-{SQL} Parsing with Schema Expansion	https://aclanthology.org/2022.acl-long.381	Done	practical	across domain	all	double	natural data splits	semantic parsing / role labelling	Dennis	Huygaa																
zhao-etal-2022-bridging	2022	Bridging the Generalization Gap in Text-to-{SQL} Parsing with Schema Expansion	https://aclanthology.org/2022.acl-long.381	Done	practical	across domain	all	double	fully generated/selected	semantic parsing / role labelling	Dennis	Huygaa																
liu-etal-2022-flooding	2022	Flooding-{X}: Improving {BERT}{'}s Resistance to Adversarial Attacks via Loss-Restricted Fine-Tuning	https://aclanthology.org/2022.acl-long.386	Flag for further review	practical	robustness	finetune-train/test	covariate	generated shifts	text classification	Dieuwke		A method to improve models against adversarial attacks, on a number of different (GLUE) classification tasks. It is not super clear to me where they apply this technique, but it seems that at least the evaluation is in the finetuning stage.															
ye-etal-2022-rng	2022	{RNG}-{KBQA}: Generation Augmented Iterative Ranking for Knowledge Base Question Answering	https://aclanthology.org/2022.acl-long.417	Done	practical	compositional	finetune-train/test	full	natural data splits	question answering	Dennis	Leila																
ye-etal-2022-rng	2022	{RNG}-{KBQA}: Generation Augmented Iterative Ranking for Knowledge Base Question Answering	https://aclanthology.org/2022.acl-long.417	Done	practical	across domain	finetune-train/test	full	natural data splits	question answering	Dennis	Leila	zero-shot generalization column in table 1															
das-etal-2022-container	2022	{CONT}ai{NER}: Few-Shot Named Entity Recognition via Contrastive Learning	https://aclanthology.org/2022.acl-long.439	Done	practical	across domain	all	double	naturally occurring shifts	named entity recognition (NER)	Dennis	Leila																
goodwin-etal-2022-compositional	2022	Compositional Generalization in Dependency Parsing	https://aclanthology.org/2022.acl-long.448	Done	practical	compositional	train-test	covariate	fully generated/selected	dependency parsing	Koustuv	Verna	Verna has outstanding question to Koustuv due to "flag for review" regarding the shift_source															
ahuja-etal-2022-aspectnews	2022	{ASPECTNEWS}: Aspect-Oriented Summarization of News Documents	https://aclanthology.org/2022.acl-long.449	First check done	practical	across domain	finetune-train/test	covariate	naturally occurring shifts	summarisation	Dennis																	
menon-etal-2022-clues	2022	{CLUES}: A Benchmark for Learning Classifiers using Natural Language Explanations	https://aclanthology.org/2022.acl-long.451	Done	cognitive	across task	pretrain-test	full	naturally occurring shifts	text classification	Dennis	Huygaa	Text classification with explanations, with generalization to unseen tasks															
menon-etal-2022-clues	2022	{CLUES}: A Benchmark for Learning Classifiers using Natural Language Explanations	https://aclanthology.org/2022.acl-long.451	Done	cognitive	across task	finetune-train/test	full	fully generated/selected	text classification	Dennis	Huygaa																
ma-etal-2022-prompt	2022	{P}rompt for Extraction? {PAIE}: {P}rompting Argument Interaction for Event Argument Extraction	https://aclanthology.org/2022.acl-long.466	Done	practical	across domain	finetune-train/test	full	naturally occurring shifts	relation extraction	Dennis	Koustuv																
bahri-etal-2022-sharpness	2022	Sharpness-Aware Minimization Improves Language Model Generalization	https://aclanthology.org/2022.acl-long.508	Flag for further review	practical	across task	finetune-train/test	full	naturally occurring shifts	NLU	Dennis	Koustuv																
bahri-etal-2022-sharpness	2022	Sharpness-Aware Minimization Improves Language Model Generalization	https://aclanthology.org/2022.acl-long.508	Flag for further review	practical	across task	finetune-train/test	full	naturally occurring shifts	question answering	Dennis	Koustuv																
tanzer-etal-2022-memorisation	2022	Memorisation versus Generalisation in Pre-trained Language Models	https://aclanthology.org/2022.acl-long.521	Done	intrinsic	robustness	finetune-train/test	covariate	natural data splits	named entity recognition (NER)	Verna	Dieuwke	Most sections of the paper are analysis, here I am annotating section 6, specifically, that measures generalisation in a few-shot setup when the training data contains noise.															
chiang-cholak-2022-overcoming	2022	Overcoming a Theoretical Limitation of Self-Attention	https://aclanthology.org/2022.acl-long.527	Flag for further review	intrinsic	compositional	train-test	assumed	fully generated/selected	seq2seq synthetic	Dennis	Dieuwke	This is a bit tricky: The motivation seems theoretical, so I put intrinsic (are transformers modelling sequences like we would expect). Since the paper is about constraint in modelling sequence lengths, I put compositional as type, and it seems that all shifts in the datasets used are assumed?															
chiang-cholak-2022-overcoming	2022	Overcoming a Theoretical Limitation of Self-Attention	https://aclanthology.org/2022.acl-long.527	Flag for further review	intrinsic	compositional	train-test	assumed	naturally occurring shifts	machine translation	Dennis	Dieuwke																
chiang-cholak-2022-overcoming	2022	Overcoming a Theoretical Limitation of Self-Attention	https://aclanthology.org/2022.acl-long.527	Flag for further review	intrinsic	compositional	train-test	covariate	natural data splits	machine translation	Dennis	Dieuwke																
dutta-etal-2022-unsupervised	2022	Can Unsupervised Knowledge Transfer from Social Discussions Help Argument Mining?	https://aclanthology.org/2022.acl-long.536	Done	practical	robustness	finetune-train/test	covariate	naturally occurring shifts	argument mining	Janie	Mario																
dutta-etal-2022-unsupervised	2022	Can Unsupervised Knowledge Transfer from Social Discussions Help Argument Mining?	https://aclanthology.org/2022.acl-long.536	Done	practical	across task	pretrain-train	label	naturally occurring shifts	argument mining	Janie	Mario																
mosca-etal-2022-suspicious	2022	{``}That Is a Suspicious Reaction!{''}: Interpreting Logits Variation to Detect {NLP} Adversarial Attacks	https://aclanthology.org/2022.acl-long.538								Rita																	
ammanabrolu-etal-2022-situated	2022	Situated Dialogue Learning through Procedural Environment Generation	https://aclanthology.org/2022.acl-long.557								Rita																	
nguyen-etal-2022-improving	2022	Improving the Generalizability of Depression Detection by Leveraging Clinical Questionnaires	https://aclanthology.org/2022.acl-long.578	Done	practical	across task	pretrain-test	label	naturally occurring shifts	text classification	Janie	Mario																
nguyen-etal-2022-improving	2022	Improving the Generalizability of Depression Detection by Leveraging Clinical Questionnaires	https://aclanthology.org/2022.acl-long.578	Done	intrinsic	robustness	train-test	covariate	naturally occurring shifts	text classification	Janie	Mario																
ding-etal-2022-openprompt	2022	{O}pen{P}rompt: An Open-source Framework for Prompt-learning	https://aclanthology.org/2022.acl-demo.10	Flag for further review	practical	across task	pretrain-test	covariate	naturally occurring shifts	multitask	Dennis		Authors provide a framework for promptlearning (and argue that it might help generalization, e.g. for low-resource data), but don't really run experiments. Is this relevant?															
hahn-etal-2021-modeling	2021	Modeling Profanity and Hate Speech in Social Media with Semantic Subspaces	https://aclanthology.org/2021.woah-1.2	Done	practical	across language	finetune-train/test	covariate	naturally occurring shifts	hate speech detection	Dennis	Koustuv																
kumar-etal-2021-learning-feature	2021	Learning Feature Weights using Reward Modeling for Denoising Parallel Corpora	https://aclanthology.org/2021.wmt-1.118	Done	practical	across language	train-test	covariate	naturally occurring shifts	machine translation	Dennis	Koustuv																
kumar-etal-2021-learning-feature	2021	Learning Feature Weights using Reward Modeling for Denoising Parallel Corpora	https://aclanthology.org/2021.wmt-1.118	First check done	practical	robustness	train-test	covariate	naturally occurring shifts	machine translation	Dennis																	
dayanik-pado-2021-disentangling	2021	Disentangling Document Topic and Author Gender in Multiple Languages: Lessons for Adversarial Debiasing	https://aclanthology.org/2021.wassa-1.6	Done	fairness	compositional	finetune-train/test	covariate	generated shifts	text classification	Huygaa	Zhijing																
conforti-etal-2021-synthetic	2021	Synthetic Examples Improve Cross-Target Generalization: A Study on Stance Detection on a {T}witter corpus.	https://aclanthology.org/2021.wassa-1.19	Done	practical	across domain	train-test	covariate	naturally occurring shifts	text classification	Dennis	Yanai																
prange-etal-2021-supertagging	2021	Supertagging the Long Tail with Tree-Structured Decoding of Complex Categories	https://aclanthology.org/2021.tacl-1.15	First check done	practical	across domain	finetune-train/test	covariate	natural data splits	constituency parsing	Dennis	Yanai																
prange-etal-2021-supertagging	2021	Supertagging the Long Tail with Tree-Structured Decoding of Complex Categories	https://aclanthology.org/2021.tacl-1.15	First check done	practical	across domain	train-test	label	naturally occurring shifts	constituency parsing	Dennis	Yanai																
ponti-etal-2021-parameter	2021	Parameter Space Factorization for Zero-Shot Learning across Tasks and Languages	https://aclanthology.org/2021.tacl-1.25	Done	practical	across language	finetune-train/test	covariate	naturally occurring shifts	POStagging	Dennis	Huygaa																
ponti-etal-2021-parameter	2021	Parameter Space Factorization for Zero-Shot Learning across Tasks and Languages	https://aclanthology.org/2021.tacl-1.25	Done	practical	across language	finetune-train/test	covariate	naturally occurring shifts	named entity recognition (NER)	Dennis	Huygaa																
ponti-etal-2021-parameter	2021	Parameter Space Factorization for Zero-Shot Learning across Tasks and Languages	https://aclanthology.org/2021.tacl-1.25	Done	practical	across task	all	full	naturally occurring shifts	multiple tasks	Dennis	Huygaa																
shen-etal-2021-evaluating	2021	Evaluating Document Coherence Modeling	https://aclanthology.org/2021.tacl-1.38	First check done	practical	across domain	finetune-train/test	covariate	naturally occurring shifts	text classification	Dennis																	
shen-etal-2021-evaluating	2021	Evaluating Document Coherence Modeling	https://aclanthology.org/2021.tacl-1.38	First check done	practical	robustness	finetune-train/test	covariate	generated shifts	text classification	Dennis																	
rotman-etal-2021-model	2021	Model Compression for Domain Adaptation through Causal Effect Estimation	https://aclanthology.org/2021.tacl-1.80	First check done	practical	across domain	pretrain-test	covariate	naturally occurring shifts	text classification	Dennis																	
rotman-etal-2021-model	2021	Model Compression for Domain Adaptation through Causal Effect Estimation	https://aclanthology.org/2021.tacl-1.80	First check done	practical	across domain	pretrain-test	covariate	naturally occurring shifts	named entity recognition (NER)	Dennis																	
rotman-etal-2021-model	2021	Model Compression for Domain Adaptation through Causal Effect Estimation	https://aclanthology.org/2021.tacl-1.80	First check done	practical	across domain	pretrain-test	covariate	naturally occurring shifts	NLI	Dennis																	
pedinotti-etal-2021-cat	2021	Did the Cat Drink the Coffee? Challenging Transformers with Generalized Event Knowledge	https://aclanthology.org/2021.starsem-1.1	Flag for further review	intrinsic	structural	pretrain-test	full	generated shifts	diagnostic task	Karim	Zhijing	To Karim: See question for "task"															
damonte-monti-2021-one	2021	One Semantic Parser to Parse Them All: Sequence to Sequence Multi-Task Learning on Semantic Parsing Datasets	https://aclanthology.org/2021.starsem-1.16	Done	practical	across task	finetune-train/test	label	naturally occurring shifts	semantic parsing / role labelling	Christos	Dieuwke	Experiments in section 4.2, which evaluates across-task generalisation by comparing MTL with single-task baselines. Tasks covered are Q&A and semantic parsing), different domains, and compositional generalisation. Also there are both naturally occuring shifts and generated splits															
damonte-monti-2021-one	2021	One Semantic Parser to Parse Them All: Sequence to Sequence Multi-Task Learning on Semantic Parsing Datasets	https://aclanthology.org/2021.starsem-1.16	Done	practical	across task	finetune-train/test	covariate	generated shifts	semantic parsing / role labelling	Christos	Dieuwke	Experiments in section 4.3, where they use the generated CFQ training split to the training data.															
damonte-monti-2021-one	2021	One Semantic Parser to Parse Them All: Sequence to Sequence Multi-Task Learning on Semantic Parsing Datasets	https://aclanthology.org/2021.starsem-1.16	Done	practical	across domain	finetune-train/test	assumed	naturally occurring shifts	semantic parsing / role labelling	Christos	Dieuwke	The type of generalisation is across datasets, which, in this case covers different tasks (Q&A, semantic parsing), different domains, and compositional generalisation. Also there are both naturally occuring shifts and generated splits															
damonte-monti-2021-one	2021	One Semantic Parser to Parse Them All: Sequence to Sequence Multi-Task Learning on Semantic Parsing Datasets	https://aclanthology.org/2021.starsem-1.16	Done	practical	compositional	finetune-train/test	covariate	fully generated/selected	question answering	Christos	Dieuwke	The type of generalisation is across datasets, which, in this case covers different tasks (Q&A, semantic parsing), different domains, and compositional generalisation. Also there are both naturally occuring shifts and generated splits															
thorn-jakobsen-etal-2021-spurious	2021	Spurious Correlations in Cross-Topic Argument Mining	https://aclanthology.org/2021.starsem-1.25																									
di-giovanni-brambilla-2021-content	2021	Content-based Stance Classification of Tweets about the 2020 {I}talian Constitutional Referendum	https://aclanthology.org/2021.socialnlp-1.2																									
choudhary-2021-improving	2021	Improving the Performance of {UD}ify with Linguistic Typology Knowledge	https://aclanthology.org/2021.sigtyp-1.5	Done	practical	across language	finetune-train/test	covariate	naturally occurring shifts	multitask	Verna	Dieuwke																
choudhary-2021-improving	2021	Improving the Performance of {UD}ify with Linguistic Typology Knowledge	https://aclanthology.org/2021.sigtyp-1.5	Done	practical	across task	finetune-train/test	label	naturally occurring shifts	multitask	Verna	Dieuwke																
scherbakov-etal-2021-anlirika	2021	Anlirika: An {LSTM}{--}{CNN} Flow Twister for Spoken Language Identification	https://aclanthology.org/2021.sigtyp-1.14																									
ek-etal-2021-training	2021	Training Strategies for Neural Multilingual Morphological Inflection	https://aclanthology.org/2021.sigmorphon-1.26	Done	practical	across language	train-test	covariate	generated shifts	morphological inflection	Huygaa	Dieuwke	A single model was trained as a multilingual system, and data hallucination was used to enrich the training data															
papangelis-etal-2021-generative	2021	Generative Conversational Networks	https://aclanthology.org/2021.sigdial-1.12																									
lin-etal-2021-domain	2021	Domain-independent User Simulation with Transformers for Task-oriented Dialogue Systems	https://aclanthology.org/2021.sigdial-1.47																									
taya-etal-2021-ochadai	2021	{OCHADAI}-{KYOTO} at {S}em{E}val-2021 Task 1: Enhancing Model Generalization and Robustness for Lexical Complexity Prediction	https://aclanthology.org/2021.semeval-1.2																									
hettiarachchi-ranasinghe-2021-transwic	2021	{T}rans{W}i{C} at {S}em{E}val-2021 Task 2: Transformer-based Multilingual and Cross-lingual Word-in-Context Disambiguation	https://aclanthology.org/2021.semeval-1.102	Done	practical	across language	finetune-train/test	covariate	generated shifts	NLU	Mikel	Huygaa	A multilingual model was finetuned in English and evaluated in cross-lingual settings. The task was automatically generated from WordNet															
liu-etal-2021-x2parser	2021	{X}2{P}arser: Cross-Lingual and Cross-Domain Framework for Task-Oriented Compositional Semantic Parsing	https://aclanthology.org/2021.repl4nlp-1.13	Done	practical	across language	finetune-train/test	covariate	natural data splits	semantic parsing / role labelling	Mikel	Yanai	Cross-lingual, cross-domain and cross-lingual+cross-domain generalization experiments on semantic parsing															
liu-etal-2021-x2parser	2021	{X}2{P}arser: Cross-Lingual and Cross-Domain Framework for Task-Oriented Compositional Semantic Parsing	https://aclanthology.org/2021.repl4nlp-1.13	Done	practical	across domain	finetune-train/test	covariate	natural data splits	semantic parsing / role labelling	Mikel	Yanai	Cross-lingual, cross-domain and cross-lingual+cross-domain generalization experiments on semantic parsing															
kimura-etal-2021-towards-language	2021	Towards a Language Model for Temporal Commonsense Reasoning	https://aclanthology.org/2021.ranlp-srw.12	Done	intrinsic	across domain	finetune-train/test	covariate	natural data splits	reasoning	Leila	Huygaa	Finetuned BERT-base on SWAG and tested on MC-TACO															
kimura-etal-2021-towards-language	2021	Towards a Language Model for Temporal Commonsense Reasoning	https://aclanthology.org/2021.ranlp-srw.12	Done	intrinsic	across domain	finetune-train/test	covariate	natural data splits	reasoning	Leila	Huygaa	Finetuned BERT-base on CosmosQA and tested on MC-TACO															
auersperger-pecina-2021-solving	2021	Solving {SCAN} Tasks with Data Augmentation and Input Embeddings	https://aclanthology.org/2021.ranlp-1.11	Done	practical	compositional	train-test	covariate	fully generated/selected	seq2seq synthetic	Koustuv	Verna																
robertson-2021-word	2021	Word Discriminations for Vocabulary Inventory Prediction	https://aclanthology.org/2021.ranlp-1.134	Done	practical	robustness	train-test	covariate	naturally occurring shifts	other	Verna	Dieuwke	Chose motivation of "practical" due to the intro mentioning quicker placements for testing. Type is unclear, they test generalisation across participants, so opted for robustness.															
panda-levitan-2021-detecting	2021	Detecting Multilingual {COVID}-19 Misinformation on Social Media via Contextualized Embeddings	https://aclanthology.org/2021.nlp4if-1.19	Done	practical	across language	finetune-train/test	covariate	natural data splits	other	Mikel	Yanai	Typical cross-lingual generalization setup with zero, few or full examples in the target language. Contains experiments with and without multilingual pretraining															
panda-levitan-2021-detecting	2021	Detecting Multilingual {COVID}-19 Misinformation on Social Media via Contextualized Embeddings	https://aclanthology.org/2021.nlp4if-1.19	Done	practical	across language	train-test	covariate	natural data splits	other	Mikel	Yanai																
douka-etal-2021-juribert	2021	{J}uri{BERT}: A Masked-Language Model Adaptation for {F}rench Legal Text	https://aclanthology.org/2021.nllp-1.9																									
wang-etal-2021-meta	2021	Meta-Learning for Domain Generalization in Semantic Parsing	https://aclanthology.org/2021.naacl-main.33	Done	practical	across domain	all	double	naturally occurring shifts	semantic parsing / role labelling	Florian	Verna	Test different pretrained embeddings (BERT and GloVe) in their base parser (i.e. label shift from pretrain to train) and then train the model with their method which generalizes better to new domains (i.e. covariate shift from train to test)															
nan-etal-2021-dart	2021	{DART}: Open-Domain Structured Data Record to Text Generation	https://aclanthology.org/2021.naacl-main.37	Done	practical	compositional	finetune-train/test	covariate	fully generated/selected	NLG	Huygaa	Zhijing	DART experiments on table 3															
zhao-etal-2021-sparta	2021	{SPARTA}: Efficient Open-Domain Question Answering via Sparse Transformer Matching Retrieval	https://aclanthology.org/2021.naacl-main.47																									
du-etal-2021-towards	2021	Towards Interpreting and Mitigating Shortcut Learning Behavior of {NLU} models	https://aclanthology.org/2021.naacl-main.71	Done	practical	robustness	all	double	generated shifts	NLU	Dieuwke	Zhijing	Investigation of shortcut behaviour in NLI models + a proposed new framework to mitigate it. They compare two different pretraining architectures, so I am marking this double shift with two loci															
cheng-etal-2021-posterior	2021	Posterior Differential Regularization with f-divergence for Improving Model Robustness	https://aclanthology.org/2021.naacl-main.85																									
chen-durrett-2021-robust	2021	Robust Question Answering Through Sub-part Alignment	https://aclanthology.org/2021.naacl-main.98	Done	practical	across domain	finetune-train/test	covariate	generated shifts	question answering	Huygaa	Zhijing	Model was trained on SQuAD 1.1, and was tested on several adversarial and out-of-domain datasets. 															
longpre-etal-2021-transferability	2021	On the Transferability of Minimal Prediction Preserving Inputs in Question Answering	https://aclanthology.org/2021.naacl-main.101																									
zhou-etal-2021-temporal	2021	Temporal Reasoning on Implicit Events from Distant Supervision	https://aclanthology.org/2021.naacl-main.107	Done	intrinsic	across task	pretrain-test	full	naturally occurring shifts	reasoning	Leila	Yanai																
zhou-etal-2021-temporal	2021	Temporal Reasoning on Implicit Events from Distant Supervision	https://aclanthology.org/2021.naacl-main.107	Done	intrinsic	across task	pretrain-test	full	generated shifts	reasoning	Leila	Yanai																
chen-yang-2021-structure	2021	Structure-Aware Abstractive Conversation Summarization via Discourse and Action Graphs	https://aclanthology.org/2021.naacl-main.109																									
shen-etal-2021-explicitly	2021	Explicitly Modeling Syntax in Language Models with Incremental Parsing and a Dynamic Oracle	https://aclanthology.org/2021.naacl-main.132																									
huang-etal-2021-continual	2021	Continual Learning for Text Classification with Information Disentanglement Based Regularization	https://aclanthology.org/2021.naacl-main.218																									
wang-etal-2021-learning-synthesize	2021	Learning to Synthesize Data for Semantic Parsing	https://aclanthology.org/2021.naacl-main.220	Done	practical	compositional	finetune-train/test	covariate	natural data splits	semantic parsing / role labelling	Huygaa	Dennis	In-Domain Setting - Table 1 - GEOQUERY experiments. BART finetuned															
wang-etal-2021-learning-synthesize	2021	Learning to Synthesize Data for Semantic Parsing	https://aclanthology.org/2021.naacl-main.220	Done	practical	across domain	finetune-train/test	covariate	naturally occurring shifts	semantic parsing / role labelling	Huygaa	Dennis	Out-of-Domain Setting - Table 2 - Spider experiments. BART finetuned															
mccoy-etal-2019-right	2019	Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference	https://aclanthology.org/P19-1334	Done	intrinsic	robustness	finetune-train/test	covariate	generated shifts	NLI	Janie	Mario																
yin-etal-2021-compositional	2021	Compositional Generalization for Neural Semantic Parsing via Span-level Supervised Attention	https://aclanthology.org/2021.naacl-main.225	Done	practical	compositional	train-test	covariate	natural data splits	semantic parsing / role labelling	Verna	Dieuwke	SMCALFLOW & ATIS experiments															
yin-etal-2021-compositional	2021	Compositional Generalization for Neural Semantic Parsing via Span-level Supervised Attention	https://aclanthology.org/2021.naacl-main.225	Done	practical	compositional	train-test	covariate	fully generated/selected	semantic parsing / role labelling	Verna	Dieuwke	CFQ experiments															
el-mekki-etal-2021-domain	2021	Domain Adaptation for {A}rabic Cross-Domain and Cross-Dialect Sentiment Analysis from Contextualized Word Embedding	https://aclanthology.org/2021.naacl-main.226	First check done	practical	across domain	finetune-train/test	label	naturally occurring shifts	sentiment analysis	Rita	Dennis	It is dialect and domain shift, but I marked is as domain and not language becasue of domain shift importance in the paper															
mhamdi-etal-2021-x	2021	{X}-{METRA}-{ADA}: Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question Answering	https://aclanthology.org/2021.naacl-main.283	Done	practical	across language	finetune-train/test	covariate	natural data splits	NLU	Mikel	Rita	Typical zero-shot and few-shot cross-lingual transfer setup with no or few training examples in the target language. The paper focuses on the latter scenario with a metalearning approach, and the zero-shot transfer results are reported for reference															
saha-etal-2021-multiprover	2021	multi{PR}over: Generating Multiple Proofs for Improved Interpretability in Rule Reasoning	https://aclanthology.org/2021.naacl-main.287	Done	cognitive	across domain	train-test	covariate	fully generated/selected	reasoning	Rita	Dennis	Fully synthetic data for DU0-DU5 															
saha-etal-2021-multiprover	2021	multi{PR}over: Generating Multiple Proofs for Improved Interpretability in Rule Reasoning	https://aclanthology.org/2021.naacl-main.287	Done	cognitive	across domain	train-test	covariate	naturally occurring shifts	reasoning	Rita	Dennis	Birds electricity and para rules dataset															
kavumba-etal-2021-learning	2021	Learning to Learn to be Right for the Right Reasons	https://aclanthology.org/2021.naacl-main.304	Done	practical	robustness	finetune-train/test	covariate	natural data splits	reasoning	Dieuwke	Janie	Metalearning during finetuning, comparing easy and hard examples															
kachuee-etal-2021-self	2021	Self-Supervised Contrastive Learning for Efficient User Satisfaction Prediction in Conversational Agents	https://aclanthology.org/2021.naacl-main.319	Done	cognitive	compositional	train-test	covariate	generated shifts	sentiment analysis	Dieuwke	Zhijing																
allaway-etal-2021-adversarial	2021	Adversarial Learning for Zero-Shot Stance Detection on Social Media	https://aclanthology.org/2021.naacl-main.379	Done	cognitive	across domain	train-test	covariate	natural data splits	text classification	Rita	Huygaa	Stance detection task (political classification) with adversarial learning and zero-shot testing															
silfverberg-etal-2021-rnn	2021	Do RNN States Encode Abstract Phonological Alternations?	https://aclanthology.org/2021.naacl-main.435	Done	cognitive	structural	train-test	covariate	fully generated/selected	morphological inflection	Rita	Huygaa	Task to evaluate capability of RNNs to discovetr phonological alterations in Finnish by exploring whether hidden states encode them															
chen-etal-2021-shadowgnn	2021	{S}hadow{GNN}: Graph Projection Neural Network for Text-to-{SQL} Parser	https://aclanthology.org/2021.naacl-main.441	Flag for further review	cognitive	compositional	train-test	assumed	generated shifts	semantic parsing / role labelling	Rita	Koustuv																
yu-etal-2021-adaptsum	2021	{A}dapt{S}um: Towards Low-Resource Domain Adaptation for Abstractive Summarization	https://aclanthology.org/2021.naacl-main.471	First check done	practical	across domain	pretrain-train	label	naturally occurring shifts	summarisation	Rita	Yanai	Summarisation accross different domains on finetuned model															
liu-etal-2021-lexical	2021	Lexical Semantic Recognition	https://aclanthology.org/2021.mwe-1.6	Done	intrinsic	across domain	train-test	covariate	natural data splits	semantic parsing / role labelling	Rita	Dieuwke	Lexical semantic supertagging across different corpora, that are considered out of domain according to the authors (see sec 3.1)															
adelani-etal-2021-effect	2021	The Effect of Domain and Diacritics in {Y}oruba{--}{E}nglish Neural Machine Translation	https://aclanthology.org/2021.mtsummit-research.6	Done	practical	across domain	pretrain-test	covariate	natural data splits	machine translation	Rita	Huygaa	OPUS-MT, Google GMNMT, Facebook M2M-100 models were tested on new domain data															
adelani-etal-2021-effect	2021	The Effect of Domain and Diacritics in {Y}oruba{--}{E}nglish Neural Machine Translation	https://aclanthology.org/2021.mtsummit-research.6	Done	practical	across domain	all	covariate	natural data splits	machine translation	Rita	Huygaa	Cross-domain machine translation testing															
phung-etal-2021-learning	2021	Learning Cross-lingual Representations for Event Coreference Resolution with Multi-view Alignment and Optimal Transport	https://aclanthology.org/2021.mrl-1.6	Flag for further review	cognitive	across language	finetune-train/test	full	naturally occurring shifts	multiple tasks	Rita	Yanai																
han-lundin-2021-multi	2021	Multi-Pair Text Style Transfer for Unbalanced Data via Task-Adaptive Meta-Learning	https://aclanthology.org/2021.metanlp-1.4	Flag for further review	cognitive	across task	finetune-train/test	covariate	natural data splits	paraphrasing	Rita	Yanai	Style-transfer using pretrained models and meta-learning															
cattan-etal-2021-cross	2021	On the cross-lingual transferability of multilingual prototypical models across {NLU} tasks	https://aclanthology.org/2021.metanlp-1.5	Done	practical	across language	finetune-train/test	covariate	natural data splits	NLU	Mikel	Rita	Typical cross-lingual transfer setup (train on all languages or all languages but target)															
li-zhang-2021-semi	2021	Semi-supervised Meta-learning for Cross-domain Few-shot Intent Classification	https://aclanthology.org/2021.metanlp-1.8	First check done	cognitive	across domain	finetune-train/test	covariate	generated shifts	intent classification	Rita		Improvement of generalization capability of intent classification task through meta learning applications. Application of masking on datasets.															
pradeep-etal-2021-scientific	2021	Scientific Claim Verification with {V}er{T}5erini	https://aclanthology.org/2021.louhi-1.11	First check done	cognitive	across domain	train-test	covariate	generated shifts	reasoning	Rita		Task: reasoning, fact verification. zero-shot setup															
vilar-federico-2021-statistical	2021	A Statistical Extension of Byte-Pair Encoding	https://aclanthology.org/2021.iwslt-1.31	Flag for further review		compositional	train-test		generated shifts	other	Rita		The article describes improvement in byte pair enccoding and then it application in MT models, so I am not sure if it should be included. Please confirm.															
zhou-etal-2021-encoding	2021	Encoding Explanatory Knowledge for Zero-shot Science Question Answering	https://aclanthology.org/2021.iwcs-1.5	First check done	practical	across domain	all	covariate	natural data splits	question answering	Rita																	
gung-palmer-2021-predicate	2021	Predicate Representations and Polysemy in {V}erb{N}et Semantic Parsing	https://aclanthology.org/2021.iwcs-1.6	First check done	practical	across domain	finetune-train/test	covariate	natural data splits	semantic parsing / role labelling	Huygaa		Table 4															
betz-etal-2021-critical	2021	Critical Thinking for Language Models	https://aclanthology.org/2021.iwcs-1.7	First check done	cognitive	across domain	finetune-train/test	label	fully generated/selected	reasoning	Rita		NLI, reasoning task with fully generated training, eval datasets finetuned on GPT2															
falk-etal-2021-automatic	2021	Automatic Classification of Attributes in {G}erman Adjective-Noun Phrases	https://aclanthology.org/2021.iwcs-1.23	First check done	intrinsic	structural	finetune-train/test	label	fully generated/selected	text classification	Rita		Adjective attributes classification with augmented datasets with Bert and FastText															
zhou-tan-2021-investigating	2021	Investigating the Effect of Natural Language Explanations on Out-of-Distribution Generalization in Few-shot {NLI}	https://aclanthology.org/2021.insights-1.17	Done	cognitive	across domain	pretrain-train	covariate	generated shifts	NLI	Rita	Kaiser	NLI taskn with multiple generated OOD templates , since there are different combinations or templates and vocabs I checked data shift as double															                           
bhargava-etal-2021-generalization	2021	Generalization in NLI: Ways (Not) To Go Beyond Simple Heuristics	https://aclanthology.org/2021.insights-1.18	Flag for further review	cognitive	across domain	pretrain-train	covariate	generated shifts	NLI	Rita	Kaiser	Improving generalization in NLI by data shifts in pretrained models tested on two generated datasets HANS and MNLI Added by Kaiser: They fine-tune the models on MNLI and then evaluate it on HANS, so I think the locus should be more of finetune-test and the generalization is in the context of robustness,															
conforti-etal-2021-adversarial	2021	Adversarial Training for News Stance Detection: Leveraging Signals from a Multi-Genre Corpus.	https://aclanthology.org/2021.hackashop-1.1	First check done	practical	across domain	pretrain-train	label	naturally occurring shifts	intent classification	Rita		Task: news stance detection (I mark intent classification) within the same domain but different genres (i.e. twitter or news), because of that I decided to use domain shift (feel free to double check)															
pelicon-etal-2021-zero	2021	Zero-shot Cross-lingual Content Filtering: Offensive Language and Hate Speech Detection	https://aclanthology.org/2021.hackashop-1.5	Flag for further review	practical	across language	finetune-train/test	covariate	natural data splits	hate speech detection	Mikel		Typical zero-shot cross-lingual transfer setup (train on English, evaluate in target language). Unsure about the motivation: the tasks are offensive language and hate speech detection (which seem related to fairness) but the exploration in the paper around cross-lingual generalization seems more practically motivated															
wang-etal-2021-evaluating	2021	Evaluating Text Generation from Discourse Representation Structures	https://aclanthology.org/2021.gem-1.8	Flag for further review	intrinsic	structural	train-test	assumed	generated shifts	NLG	Mario	Rita	tense, polarity, and grammatical number															
wang-etal-2021-evaluating	2021	Evaluating Text Generation from Discourse Representation Structures	https://aclanthology.org/2021.gem-1.8	Flag for further review	intrinsic	robustness	train-test	covariate	generated shifts	NLG	Mario	Rita	unseen names and quantities															
jin-etal-2021-learn-continually	2021	Learn Continually, Generalize Rapidly: Lifelong Knowledge Accumulation for Few-shot Learning	https://aclanthology.org/2021.findings-emnlp.62	Done	cognitive	across task	train-test	full	naturally occurring shifts	multiple tasks	Leila	Yanai	instant performance and final performance															
jin-etal-2021-learn-continually	2021	Learn Continually, Generalize Rapidly: Lifelong Knowledge Accumulation for Few-shot Learning	https://aclanthology.org/2021.findings-emnlp.62	Done	cognitive	across task	pretrain-test	full	naturally occurring shifts	multiple tasks	Leila	Yanai	few shot performnace															
jin-etal-2021-learn-continually	2021	Learn Continually, Generalize Rapidly: Lifelong Knowledge Accumulation for Few-shot Learning	https://aclanthology.org/2021.findings-emnlp.62	Done	cognitive	across task	pretrain-train	full	naturally occurring shifts	multiple tasks	Leila	Yanai	few shot performnace															
li-etal-2021-select-one	2021	How to Select One Among All ? An Empirical Study Towards the Robustness of Knowledge Distillation in Natural Language Understanding	https://aclanthology.org/2021.findings-emnlp.65	First check done	intrinsic	across domain	train-test	covariate	naturally occurring shifts	multiple tasks	Leila	Yanai	OOD testing on various Knowledge distillation methods (paraphrasing/NLI)															
li-etal-2021-select-one	2021	How to Select One Among All ? An Empirical Study Towards the Robustness of Knowledge Distillation in Natural Language Understanding	https://aclanthology.org/2021.findings-emnlp.65	First check done	intrinsic	robustness	train-test	covariate	generated shifts	multiple tasks	Leila	Yanai	Adverserail testing on KD methods (parphrasing/NLI/sentiment analysis/linguistic acceptability/regression/QA)															
yoo-qi-2021-towards-improving	2021	Towards Improving Adversarial Training of {NLP} Models	https://aclanthology.org/2021.findings-emnlp.81	Done	practical	robustness	train-test	covariate	generated shifts	text classification	Leila	Yanai	aims to increase robustness by adverserail attack and shows it has generalisation opver unseen attacks and ood data in the target task															
yoo-qi-2021-towards-improving	2021	Towards Improving Adversarial Training of {NLP} Models	https://aclanthology.org/2021.findings-emnlp.81	Done	practical	robustness	train-test	covariate	naturally occurring shifts	text classification	Leila	Yanai	aims to increase robustness by adverserail attack and shows it has generalisation opver unseen attacks and ood data in the target task															
yoo-qi-2021-towards-improving	2021	Towards Improving Adversarial Training of {NLP} Models	https://aclanthology.org/2021.findings-emnlp.81	Done	practical	across domain	train-test	covariate	naturally occurring shifts	text classification	Leila	Yanai	aims to increase robustness by adverserail attack and shows it has generalisation opver unseen attacks and ood data in the target task															
sung-etal-2021-cnnbif-cnn	2021	{CNNB}i{F}: {CNN}-based Bigram Features for Named Entity Recognition	https://aclanthology.org/2021.findings-emnlp.87																									
zheng-lapata-2021-compositional-generalization	2021	Compositional Generalization via Semantic Tagging	https://aclanthology.org/2021.findings-emnlp.88																									
zhang-etal-2021-effectiveness-pre	2021	Effectiveness of Pre-training for Few-shot Intent Classification	https://aclanthology.org/2021.findings-emnlp.96	Flag for further review	practical	across domain	pretrain-test	full	naturally occurring shifts	intent classification	Leila	Koustuv	Finetunes BERT on labled intent data and then few shots over different intent domains															
lee-2021-improving-end	2021	Improving End-to-End Task-Oriented Dialog System with A Simple Auxiliary Task	https://aclanthology.org/2021.findings-emnlp.112	Flag for further review	practical	across domain	train-test	full	natural data splits	dialogue	Leila	Koustuv	For task oriented dialogue they use and auxiliary task to better do TOD. In this part I cant say where the generalization is. But they have OOD testing with fewshot finetuning. 															
zhong-etal-2021-wikibias-detecting	2021	{WIKIBIAS}: Detecting Multi-Span Subjective Biases in Language	https://aclanthology.org/2021.findings-emnlp.155																									
gai-etal-2021-grounded-graph	2021	Grounded Graph Decoding improves Compositional Generalization in Question Answering	https://aclanthology.org/2021.findings-emnlp.157																									
shrivastava-etal-2021-span-pointer	2021	Span Pointer Networks for Non-Autoregressive Task-Oriented Semantic Parsing	https://aclanthology.org/2021.findings-emnlp.161																									
hu-etal-2021-relation-guided	2021	Relation-Guided Pre-Training for Open-Domain Question Answering	https://aclanthology.org/2021.findings-emnlp.292	Done	practical	across domain	pretrain-train	covariate	naturally occurring shifts	question answering	Leila	Koustuv	Creates a QA dataset from wikipedia articles to address low relation coverage, pretrain QA system on this dataset and then finestunes and test over three QA datasets.															
shuster-etal-2021-retrieval-augmentation	2021	Retrieval Augmentation Reduces Hallucination in Conversation	https://aclanthology.org/2021.findings-emnlp.320																									
picco-etal-2021-neural-unification	2021	Neural Unification for Logic Reasoning over Natural Language	https://aclanthology.org/2021.findings-emnlp.331																									
wullach-etal-2021-fight-fire	2021	Fight Fire with Fire: Fine-tuning Hate Detectors using Large Samples of Generated Hate Speech	https://aclanthology.org/2021.findings-emnlp.402	Done	practical	across domain	finetune-train/test	covariate	naturally occurring shifts	hate speech detection	Verna	Zhijing	Technically it's across dataset generalisation, but in the process it's also across domain, because the different datasets have slightly different domains. Pretrained models are rather similar (BERT, Roberta, ALBERT) so I'm not marking the pretrain shift, which is not at all the focus.															
lent-etal-2021-testing	2021	Testing Cross-Database Semantic Parsers With Canonical Utterances	https://aclanthology.org/2021.eval4nlp-1.8																									
zou-etal-2021-low	2021	Low-Resource Dialogue Summarization with Domain-Agnostic Multi-Source Pretraining	https://aclanthology.org/2021.emnlp-main.7		practical	robustness	all	full	naturally occurring shifts	summarisation	Janie		maybe there should be a second row since they also test not full															
sen-etal-2021-counterfactually	2021	How Does Counterfactually Augmented Data Impact Models for Social Computing Constructs?	https://aclanthology.org/2021.emnlp-main.28	Done	practical	across domain	finetune-train/test	full	naturally occurring shifts	text classification	Leila	Koustuv																
sen-etal-2021-counterfactually	2021	How Does Counterfactually Augmented Data Impact Models for Social Computing Constructs?	https://aclanthology.org/2021.emnlp-main.28	Done	practical	robustness	finetune-train/test	label	generated shifts	text classification	Leila	Koustuv	They perform the above but with adverserial samples made from the original data (sentiment, sexisem, and hate speech)															
csordas-etal-2021-devil	2021	The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers	https://aclanthology.org/2021.emnlp-main.49	Done	practical	robustness	train-test	covariate	generated shifts	seq2seq synthetic	Yanai	Verna																
rivera-soto-etal-2021-learning	2021	Learning Universal Authorship Representations	https://aclanthology.org/2021.emnlp-main.70								Janie																	
wei-etal-2021-frequency	2021	Frequency Effects on Syntactic Rule Learning in Transformers	https://aclanthology.org/2021.emnlp-main.72	Done	practical	structural	pretrain-test	covariate	natural data splits	diagnostic task	Tiago	Dieuwke	This paper evaluates the impact of verb frequencies in BERT's pretraining data on its ability to do number agreement. I wasnt sure if this means we should have train-test or pretrain-test in this case. I annotated it as train--test, since BERT's "pretraining data" is treated as train data here.															
weber-etal-2021-extend	2021	Extend, donâ€™t rebuild: Phrasing conditional graph modification as autoregressive sequence labelling	https://aclanthology.org/2021.emnlp-main.93																									
perez-mayos-etal-2021-much	2021	How much pretraining data do language models need to learn syntax?	https://aclanthology.org/2021.emnlp-main.118	Done	practical	across task	pretrain-train	full	naturally occurring shifts	multiple tasks	Leila	Koustuv	They investigate how much learning with bigger data helps a LM in downstream tasks (POS tagging/semantic parsing/paraphrase detection)															
perez-mayos-etal-2021-much	2021	How much pretraining data do language models need to learn syntax?	https://aclanthology.org/2021.emnlp-main.118	Flag for further review	intrinsic	across task	pretrain-test	full	naturally occurring shifts	syntactic tests	Koustuv	Tiago	Probing task															
perez-mayos-etal-2021-much	2021	How much pretraining data do language models need to learn syntax?	https://aclanthology.org/2021.emnlp-main.118	Flag for further review	intrinsic	across task	pretrain-test	full	naturally occurring shifts	syntactic tests	Koustuv	Tiago	Syntactic Generalization															
huang-etal-2021-improving-zero	2021	Improving Zero-Shot Cross-Lingual Transfer Learning via Robust Training	https://aclanthology.org/2021.emnlp-main.126	Done	practical	across language	finetune-train/test	covariate	natural data splits	multitask	Mikel	Leila	This paper proposes an approach to improve multilingual BERT and evaluates on the typical zero-shot cross-lingualt transfer setup															
bari-etal-2021-nearest	2021	Nearest Neighbour Few-Shot Learning for Cross-lingual Classification	https://aclanthology.org/2021.emnlp-main.131	Done	practical	across language	finetune-train/test	full	natural data splits	multiple tasks	Leila	Koustuv	They finetune a multilingual LM on English data of a certain task and then few shot (wo change of parameters) with other languages in the same task															
bari-etal-2021-nearest	2021	Nearest Neighbour Few-Shot Learning for Cross-lingual Classification	https://aclanthology.org/2021.emnlp-main.131	Done	practical	across task	finetune-train/test	full	natural data splits	paraphrase detection	Leila	Koustuv	Here they perform the above procedure but also perform on a different task															
wu-etal-2021-dialki	2021	{DIALKI}: Knowledge Identification in Conversational Systems through Dialogue-Document Contextualization	https://aclanthology.org/2021.emnlp-main.140																									
tian-etal-2021-diagnosing	2021	Diagnosing the First-Order Logical Reasoning Ability Through {L}ogic{NLI}	https://aclanthology.org/2021.emnlp-main.303	Done	intrinsic	robustness	all	double	generated shifts	reasoning	Yanai	Dieuwke	Investigate how well different pretrained models generalise to a newly proposed first-order-logic reasoning dataset. Because there are different step setup in the FOL stage, this paper is marked to investigate a multi-locus double shift. the training/val/test data of that dataset itself, so the experiments are marked "label shift" with as locus pretrain to test															
ma-etal-2021-exploring	2021	Exploring Strategies for Generalizable Commonsense Reasoning with Pre-trained Models	https://aclanthology.org/2021.emnlp-main.445																									
bau-andreas-2021-neural	2021	How Do Neural Sequence Models Generalize? Local and Global Cues for Out-of-Distribution Prediction	https://aclanthology.org/2021.emnlp-main.448																									
gupta-etal-2021-paired	2021	Paired Examples as Indirect Supervision in Latent Decision Models	https://aclanthology.org/2021.emnlp-main.466																									
aghajanyan-etal-2021-muppet	2021	Muppet: Massive Multi-task Representations with Pre-Finetuning	https://aclanthology.org/2021.emnlp-main.468	Flag for further review	practical	across task	pretrain-train	full	naturally occurring shifts	multiple tasks	Leila	Koustuv	They add a new training stage called pre-finetuning with huge amount of data on a number of enormous tasks. This stage is after pretraining, before finetuning (for task-specific knowledge).															
friedman-etal-2021-single	2021	Single-dataset Experts for Multi-dataset Question Answering	https://aclanthology.org/2021.emnlp-main.495	Done	practical	across domain	finetune-train/test	covariate	naturally occurring shifts	question answering	Leila	Koustuv	This model consists of adaptor networks, where multiple adaptors are trained over multiple tasks. So, in one adaptor case the shift locus is pre-train-train, but for all of them combined the locus in finetune train-test.															
jiang-bansal-2021-inducing	2021	Inducing Transformer{'}s Compositional Generalization Ability via Auxiliary Sequence Prediction Tasks	https://aclanthology.org/2021.emnlp-main.505																									
zuo-etal-2021-adversarial	2021	Adversarial Regularization as Stackelberg Game: An Unrolled Optimization Approach	https://aclanthology.org/2021.emnlp-main.527																									
longpre-etal-2021-entity	2021	Entity-Based Knowledge Conflicts in Question Answering	https://aclanthology.org/2021.emnlp-main.565																									
ye-etal-2021-crossfit	2021	{C}ross{F}it: A Few-shot Learning Challenge for Cross-task Generalization in {NLP}	https://aclanthology.org/2021.emnlp-main.572	Done	practical	across task	pretrain-train	full	natural data splits	multitask	Huygaa	Koustuv	Held-out-NLI experiments															
ye-etal-2021-crossfit	2021	{C}ross{F}it: A Few-shot Learning Challenge for Cross-task Generalization in {NLP}	https://aclanthology.org/2021.emnlp-main.572	Done	practical	across task	all	double	natural data splits	multitask	Huygaa	Koustuv	Random Split experiments															
dalvi-etal-2021-explaining	2021	Explaining Answers with Entailment Trees	https://aclanthology.org/2021.emnlp-main.585																									
zhang-choi-2021-situatedqa	2021	{S}ituated{QA}: Incorporating Extra-Linguistic Contexts into {QA}	https://aclanthology.org/2021.emnlp-main.586	Done	intrinsic	compositional	pretrain-test	covariate	fully generated/selected	question answering	Huygaa	Leila	Table 5, row 1,2,4,5 (unmodified, query modified baselines)															
zhang-choi-2021-situatedqa	2021	{S}ituated{QA}: Incorporating Extra-Linguistic Contexts into {QA}	https://aclanthology.org/2021.emnlp-main.586	Done	intrinsic	compositional	pretrain-train	full	fully generated/selected	question answering	Huygaa	Leila	Table 5, row 3,6 finetuned BERT															
zhou-etal-2021-rica	2021	{RICA}: Evaluating Robust Inference Capabilities Based on Commonsense Axioms	https://aclanthology.org/2021.emnlp-main.598																									
lin-etal-2021-zero	2021	Zero-Shot Dialogue State Tracking via Cross-Task Transfer	https://aclanthology.org/2021.emnlp-main.622																									
chen-etal-2021-honey	2021	Honey or Poison? Solving the Trigger Curse in Few-shot Event Detection via Causal Intervention	https://aclanthology.org/2021.emnlp-main.637																									
yu-joty-2021-effective	2021	Effective Fine-Tuning Methods for Cross-lingual Adaptation	https://aclanthology.org/2021.emnlp-main.668																									
bartolo-etal-2021-improving	2021	Improving Question Answering Model Robustness with Synthetic Adversarial Data Generation	https://aclanthology.org/2021.emnlp-main.696	Flag for further review	practical	compositional	finetune-train/test	covariate	fully generated/selected	question answering	Huygaa		Answer selection experiments - Table 2															
bartolo-etal-2021-improving	2021	Improving Question Answering Model Robustness with Synthetic Adversarial Data Generation	https://aclanthology.org/2021.emnlp-main.696	Done	practical	robustness	finetune-train/test	covariate	generated shifts	question answering	Huygaa	Dieuwke	Table 7. (experiments on adversarial test sets)															
bartolo-etal-2021-improving	2021	Improving Question Answering Model Robustness with Synthetic Adversarial Data Generation	https://aclanthology.org/2021.emnlp-main.696	Done	practical	across domain	finetune-train/test	covariate	generated shifts	question answering	Huygaa	Dieuwke	MRQA experiments - Table 8															
gan-etal-2021-exploring	2021	Exploring Underexplored Limitations of Cross-Domain Text-to-{SQL} Generalization	https://aclanthology.org/2021.emnlp-main.702	Done	practical	across domain	train-test	covariate	natural data splits	semantic parsing / role labelling	Koustuv	Huygaa	Text to SQL task															
ren-etal-2021-text	2021	Text {A}uto{A}ugment: Learning Compositional Augmentation Policy for Text Classification	https://aclanthology.org/2021.emnlp-main.711																									
zhao-etal-2021-unified	2021	A Unified Speaker Adaptation Approach for {ASR}	https://aclanthology.org/2021.emnlp-main.737																									
xu-etal-2021-raise	2021	Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning	https://aclanthology.org/2021.emnlp-main.749	Done	intrinsic	across domain	finetune-train/test	covariate	natural data splits	NLI	Huygaa	Yanai	Table 2 Finetune on MNLI and test on SciTail 															
xu-etal-2021-raise	2021	Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning	https://aclanthology.org/2021.emnlp-main.749	Done	intrinsic	across task	finetune-train/test	full	natural data splits	text classification	Huygaa	Yanai	Figure 2 - Text classification as probing task. Finetune on MRPC and test on other tasks, e.g., QNLI, QQP, CoLA															
korner-etal-2021-classifying	2021	On Classifying whether Two Texts are on the Same Side of an Argument	https://aclanthology.org/2021.emnlp-main.795																									
huang-etal-2021-shot	2021	Few-Shot Named Entity Recognition: An Empirical Baseline Study	https://aclanthology.org/2021.emnlp-main.813																									
basu-roy-chowdhury-etal-2021-everything	2021	Is Everything in Order? A Simple Way to Order Sentences	https://aclanthology.org/2021.emnlp-main.841	Done	intrinsic	robustness	finetune-train/test	covariate	naturally occurring shifts	other	Tiago	Dieuwke	Section 5 experiments across datasets in a single domain (either of): scientific paper abstracts and narratives.															
basu-roy-chowdhury-etal-2021-everything	2021	Is Everything in Order? A Simple Way to Order Sentences	https://aclanthology.org/2021.emnlp-main.841	Done	intrinsic	across domain	finetune-train/test	covariate	naturally occurring shifts	other	Tiago	Dieuwke	Section 5 experiments across datasets in two different domains: scientific paper abstracts and narratives.															
basu-roy-chowdhury-etal-2021-everything	2021	Is Everything in Order? A Simple Way to Order Sentences	https://aclanthology.org/2021.emnlp-main.841	Done	intrinsic	robustness	finetune-train/test	covariate	natural data splits	other	Tiago	Dieuwke	Section 6.2 experiments on length.															
oren-etal-2021-finding	2021	Finding needles in a haystack: Sampling Structurally-diverse Training Sets from Synthetic Data for Compositional Generalization	https://aclanthology.org/2021.emnlp-main.843																									
roy-etal-2021-attribute	2021	Attribute Value Generation from Product Title using Language Models	https://aclanthology.org/2021.ecnlp-1.2																									
huang-etal-2021-counterfactual	2021	Counterfactual Matters: Intrinsic Probing For Dialogue State Tracking	https://aclanthology.org/2021.eancs-1.1																									
kedia-chinthakindi-2021-keep	2021	Keep Learning: Self-supervised Meta-learning for Learning from Inference	https://aclanthology.org/2021.eacl-main.6																									
malkiel-wolf-2021-maximal	2021	Maximal Multiverse Learning for Promoting Cross-Task Generalization of Fine-Tuned Language Models	https://aclanthology.org/2021.eacl-main.14																									
yanaka-etal-2021-exploring	2021	Exploring Transitivity in Neural {NLI} Models through Veridicality	https://aclanthology.org/2021.eacl-main.78	Done	cognitive	compositional	all	double	fully generated/selected	NLI	Yanai	Dieuwke	Compare how pretrained LSTMs (Glove 'pretraining') and BERT models generalise to a generated NLI dataset. Because the dataset is constructed to measure compositionality, and the authors evaluate two different pretraining procedures, they evaluate two stages of  the modelling process. The entry is therefore marked to have a "all" locus, and a double shift.															
lewis-etal-2021-question	2021	Question and Answer Test-Train Overlap in Open-Domain Question Answering Datasets	https://aclanthology.org/2021.eacl-main.86	Done	intrinsic	robustness	finetune-train/test	covariate	natural data splits	question answering	Dieuwke	Florian																
li-etal-2021-zero	2021	Zero-shot Generalization in Dialog State Tracking through Generative Question Answering	https://aclanthology.org/2021.eacl-main.91																									
wu-etal-2021-alternating	2021	Alternating Recurrent Dialog Model with Large-scale Pre-trained Language Models	https://aclanthology.org/2021.eacl-main.110																									
ren-etal-2021-cross	2021	Cross-Topic Rumor Detection using Topic-Mixtures	https://aclanthology.org/2021.eacl-main.131																									
sogaard-etal-2021-need	2021	We Need To Talk About Random Splits	https://aclanthology.org/2021.eacl-main.156	Done	intrinsic	robustness	train-test	covariate	naturally occurring shifts	multitask	Tiago	Huygaa																
weber-etal-2021-language	2021	Language Modelling as a Multi-Task Problem	https://aclanthology.org/2021.eacl-main.176	Done	cognitive	structural	train-test	covariate	natural data splits	syntactic tests	Dieuwke	Dennis																
zhou-etal-2021-hidden	2021	Hidden Biases in Unreliable News Detection Datasets	https://aclanthology.org/2021.eacl-main.211	Done	practical	robustness	finetune-train/test	covariate	naturally occurring shifts	text classification	Huygaa	Zhijing																
yaghoobzadeh-etal-2021-increasing	2021	Increasing Robustness to Spurious Correlations using Forgettable Examples	https://aclanthology.org/2021.eacl-main.291																									
toshniwal-etal-2021-generalization	2021	On Generalization in Coreference Resolution	https://aclanthology.org/2021.crac-1.12																									
dankers-etal-2021-generalising	2021	Generalising to {G}erman Plural Noun Classe	https://aclanthology.org/2021.conll-1.8	Done	cognitive	structural	train-test	covariate	generated shifts	morphological inflection	Dieuwke	Verna																
scholman-etal-2021-comparison	2021	Comparison of methods for explicit discourse connective identification across various domains	https://aclanthology.org/2021.codi-main.9																									
liu-chen-2021-improving	2021	Improving Multi-Party Dialogue Discourse Parsing via Domain Integration	https://aclanthology.org/2021.codi-main.11																									
liu-etal-2021-dmrst	2021	{DMRST}: A Joint Framework for Document-Level Multilingual {RST} Discourse Segmentation and Parsing	https://aclanthology.org/2021.codi-main.15																									
kouris-etal-2021-abstractive	2021	Abstractive Text Summarization: Enhancing Sequence-to-Sequence Models Using Word Sense Disambiguation and Semantic Content Generalization	https://aclanthology.org/2021.cl-4.27																									
xiachong-etal-2021-incorporating	2021	Incorporating Commonsense Knowledge into Abstractive Dialogue Summarization via Heterogeneous Graph Networks	https://aclanthology.org/2021.ccl-1.86																									
haneczok-etal-2021-fine	2021	Fine-grained Event Classification in News-like Text Snippets - Shared Task	https://aclanthology.org/2021.case-1.23																									
cabrera-diego-etal-2021-using	2021	Using a Frustratingly Easy Domain and Tagset Adaptation for Creating {S}lavic Named Entity Recognition Systems	https://aclanthology.org/2021.bsnlp-1.12																									
hansen-sogaard-2021-guideline	2021	Guideline Bias in {W}izard-of-{O}z Dialogues	https://aclanthology.org/2021.bppf-1.2																									
chaabouni-etal-2021-transformers	2021	Can Transformers Jump Around Right in Natural Language? Assessing Performance Transfer from {SCAN}	https://aclanthology.org/2021.blackboxnlp-1.9	Done	intrinsic	compositional	train-test	covariate	fully generated/selected	seq2seq synthetic	Koustuv	Florian	Test whether performance improvements on SCAN transfer to natural language improvements															
chaabouni-etal-2021-transformers	2021	Can Transformers Jump Around Right in Natural Language? Assessing Performance Transfer from {SCAN}	https://aclanthology.org/2021.blackboxnlp-1.9	Flag for further review	intrinsic	compositional	train-test	covariate	generated shifts	seq2seq synthetic	Koustuv	Florian	Experiments with MT corpora															
kanashiro-pereira-etal-2021-multi	2021	Multi-Layer Random Perturbation Training for improving Model Generalization Efficiently	https://aclanthology.org/2021.blackboxnlp-1.23																									
yanaka-mineshima-2021-assessing	2021	Assessing the Generalization Capacity of Pre-trained Language Models through {J}apanese Adversarial Natural Language Inference	https://aclanthology.org/2021.blackboxnlp-1.26	First check done	intrinsic	robustness	finetune-train/test	full	generated shifts	NLI	Huygaa		Table 7. Japanese BERT was finetuned on JSICK and tested on JaNLI. JSICK's contradiction and neutral labels are equal to non-entailment label of JaNLI. 															
nguyen-etal-2021-combining	2021	Combining Shallow and Deep Representations for Text-Pair Classification	https://aclanthology.org/2021.alta-1.7																									
maronikolakis-schutze-2021-multidomain	2021	Multidomain Pretrained Language Models for Green {NLP}	https://aclanthology.org/2021.adaptnlp-1.1																									
wu-etal-2021-conditional	2021	Conditional Adversarial Networks for Multi-Domain Text Classification	https://aclanthology.org/2021.adaptnlp-1.3	First check done	practical	across domain	train-test	covariate	natural data splits	text classification	Huygaa		Table 4															
zhao-etal-2021-effective	2021	Effective Distant Supervision for Temporal Relation Extraction	https://aclanthology.org/2021.adaptnlp-1.20																									
ghosh-etal-2021-helpful	2021	How Helpful is Inverse Reinforcement Learning for Table-to-Text Generation?	https://aclanthology.org/2021.acl-short.11																									
han-wang-2021-good	2021	Doing Good or Doing Right? Exploring the Weakness of Commonsense Causal Reasoning Models	https://aclanthology.org/2021.acl-short.20																									
ranasinghe-etal-2021-exploratory	2021	An Exploratory Analysis of Multilingual Word-Level Quality Estimation with Cross-Lingual Transformers	https://aclanthology.org/2021.acl-short.55	Done	practical	across language	finetune-train/test	covariate	natural data splits	other	Mikel		Typical cross-lingual transfer setup for MT quality estimation (train on all languages, all but target, or few examples in target)															
li-etal-2021-exploration	2021	Exploration and Exploitation: Two Ways to Improve {C}hinese Spelling Correction Models	https://aclanthology.org/2021.acl-short.56	Done	practical	robustness	all	double	generated shifts	other	Dieuwke	Dennis	Generalisation to unseen misspellings in the finetuning stage. Because multiple pretrained models are compared, I marked this a double shift with a full locus. Because the test adds adversarial data in the finetuning stage, I mark the shift "generated".															
kim-etal-2021-improving	2021	Improving Compositional Generalization in Classification Tasks via Structure Annotations	https://aclanthology.org/2021.acl-short.81	Done	practical	compositional	train-test	covariate	generated shifts	text classification	Koustuv	Florian																
ye-ren-2021-learning	2021	Learning to Generate Task-Specific Adapters from Task Description	https://aclanthology.org/2021.acl-short.82																									
du-etal-2021-qa	2021	{QA}-Driven Zero-shot Slot Filling with Weak Supervision Pretraining	https://aclanthology.org/2021.acl-short.83																									
gupta-srikumar-2021-x	2021	{X}-Fact: A New Benchmark Dataset for Multilingual Fact Checking	https://aclanthology.org/2021.acl-short.86	Done	practical	across domain	finetune-train/test	covariate	naturally occurring shifts	fact checking	Dieuwke	Yanai	Introduction of X-Fact, a multilingual fact-checking dataset, with challenge sets to evaluate generalisation across domain and across languages. Authors finetune an M-BERT model to the task and compare within-domain and across domain generalisation, as well as generalisation to languages that were not in the training data. This entry refers to the across-domain experiments (alpha 2).															
gupta-srikumar-2021-x	2021	{X}-Fact: A New Benchmark Dataset for Multilingual Fact Checking	https://aclanthology.org/2021.acl-short.86	Done	practical	across language	finetune-train/test	covariate	naturally occurring shifts	fact checking	Dieuwke	Yanai	Introduction of X-Fact, a multilingual fact-checking dataset, with challenge sets to evaluate generalisation across domain and across languages. Authors finetune an M-BERT model to the task and compare within-domain and across domain generalisation, as well as generalisation to languages that were not in the training data. This entry refers to the across-language experiments (alpha_3).															
srivastava-goodman-2021-question	2021	Question Generation for Adaptive Education	https://aclanthology.org/2021.acl-short.88																									
moghimifar-etal-2021-neural	2021	Neural-Symbolic Commonsense Reasoner with Relation Predictors	https://aclanthology.org/2021.acl-short.100																									
tang-etal-2021-dureader	2021	{D}u{R}eader{\_}robust: A {C}hinese Dataset Towards Evaluating Robustness and Generalization of Machine Reading Comprehension in Real-World Applications	https://aclanthology.org/2021.acl-short.120																									
liang-leung-2021-improving	2021	Improving Model Generalization: A {C}hinese Named Entity Recognition Case Study	https://aclanthology.org/2021.acl-short.125																									
zhou-lampouras-2021-generalising	2021	Generalising Multilingual Concept-to-Text {NLG} with Language Agnostic Delexicalisation	https://aclanthology.org/2021.acl-long.10																									
lin-etal-2021-learning	2021	Learning Language Specific Sub-network for Multilingual Machine Translation	https://aclanthology.org/2021.acl-long.25	Done	practical	across language	pretrain-train	covariate	naturally occurring shifts	machine translation	Florian	Huygaa	Finetuning experiments															
lin-etal-2021-learning	2021	Learning Language Specific Sub-network for Multilingual Machine Translation	https://aclanthology.org/2021.acl-long.25	Done	practical	across language	pretrain-test	covariate	naturally occurring shifts	machine translation	Florian	Huygaa	Zero shot experiments															
karimi-mahabadi-etal-2021-parameter	2021	Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks	https://aclanthology.org/2021.acl-long.47	Flag for further review	practical	across task	pretrain-train	label	naturally occurring shifts	multitask	Dieuwke		If I understand this well, they propose a new method/model for (low-resource) finetuning															
herzig-berant-2021-span	2021	Span-based Semantic Parsing for Compositional Generalization	https://aclanthology.org/2021.acl-long.74																									
shaw-etal-2021-compositional	2021	Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?	https://aclanthology.org/2021.acl-long.75	Flag for further review	cognitive	compositional	train-test	covariate	fully generated/selected	seq2seq synthetic	Verna	Zhijing	SCAN (intro mentions both practical & cognitive motivation, not sure which one to choose)															
shaw-etal-2021-compositional	2021	Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?	https://aclanthology.org/2021.acl-long.75	Done	cognitive	compositional	train-test	covariate	natural data splits	semantic parsing / role labelling	Verna	Zhijing	Spider & GeoQuery  (intro mentions both practical & cognitive motivation, not sure which one to choose)															
davis-van-schijndel-2021-uncovering	2021	Uncovering Constraint-Based Behavior in Neural Models via Targeted Fine-Tuning	https://aclanthology.org/2021.acl-long.93																									
zhang-etal-2021-dataset	2021	A Dataset and Baselines for Multilingual Reply Suggestion	https://aclanthology.org/2021.acl-long.97	Done	practical	across language	finetune-train/test	covariate	natural data splits	other	Mikel	Huygaa	Typical cross-lingual transfer setup (train on English only vs multiple languages vs target language)															
nie-etal-2021-like	2021	I like fish, especially dolphins: Addressing Contradictions in Dialogue Modeling	https://aclanthology.org/2021.acl-long.134																									
xu-etal-2021-optimizing	2021	Optimizing Deeper Transformers on Small Datasets	https://aclanthology.org/2021.acl-long.163	Done	practical	across domain	all	double	naturally occurring shifts	semantic parsing / role labelling	Dieuwke	Dennis	My understanding is that this is a study where they consider all kinds of different settings with both pretraining and finetuning (hence double locus + split), but I could be wrong.															
lee-etal-2021-kaggledbqa	2021	{K}aggle{DBQA}: Realistic Evaluation of Text-to-{SQL} Parsers	https://aclanthology.org/2021.acl-long.176																									
shen-etal-2021-enhancing	2021	Enhancing the generalization for Intent Classification and Out-of-Domain Detection in {SLU}	https://aclanthology.org/2021.acl-long.190																									
ding-etal-2021-nerd	2021	Few-{NERD}: A Few-shot Named Entity Recognition Dataset	https://aclanthology.org/2021.acl-long.248																									
conklin-etal-2021-meta	2021	Meta-Learning to Compositionally Generalize	https://aclanthology.org/2021.acl-long.258	Done	cognitive	compositional	train-test	covariate	fully generated/selected	seq2seq synthetic	Verna	Dieuwke	SCAN experiments															
conklin-etal-2021-meta	2021	Meta-Learning to Compositionally Generalize	https://aclanthology.org/2021.acl-long.258	Done	cognitive	compositional	train-test	covariate	fully generated/selected	semantic parsing / role labelling	Verna	Dieuwke	COGS experiments															
hofmann-etal-2021-superbizarre	2021	Superbizarre Is Not Superb: Derivational Morphology Improves {BERT}{'}s Interpretation of Complex Words	https://aclanthology.org/2021.acl-long.279	Done	practical	structural	finetune-train/test	covariate	naturally occurring shifts	text classification	Huygaa	Tiago	Text classifications as probing tasks of BERT, and its input was linguistically better segmented.															
qian-etal-2021-structural	2021	Structural Guidance for Transformer Language Models	https://aclanthology.org/2021.acl-long.289																									
chen-etal-2021-hiddencut	2021	{H}idden{C}ut: Simple Data Augmentation for Natural Language Understanding with Better Generalizability	https://aclanthology.org/2021.acl-long.338																									
peng-etal-2021-raddle	2021	{RADDLE}: An Evaluation Benchmark and Analysis Platform for Robust Task-oriented Dialog Systems	https://aclanthology.org/2021.acl-long.341																									
ahmad-etal-2021-syntax	2021	Syntax-augmented Multilingual {BERT} for Cross-lingual Transfer	https://aclanthology.org/2021.acl-long.350	Done	practical	across language	finetune-train/test	covariate	natural data splits	multiple tasks	Mikel	Leila	The paper proposes augmenting multilingual BERT pretraining with syntactic information and evaluates it in typical zero-shot cross-lingual transfer settings															
li-etal-2021-compositional	2021	On Compositional Generalization of Neural Machine Translation	https://aclanthology.org/2021.acl-long.368	Done	intrinsic	compositional	train-test	covariate	generated shifts	machine translation	Verna	Dieuwke																
akyurek-andreas-2021-lexicon	2021	Lexicon Learning for Few Shot Sequence Modeling	https://aclanthology.org/2021.acl-long.382																									
kuribayashi-etal-2021-lower	2021	Lower Perplexity is Not Always Human-Like	https://aclanthology.org/2021.acl-long.405	Flag for further review							Tiago		I am not sure this paper investigates generalisation. It basically trains several language models in English and Japanese. The authors then evaluate whether the surprisal estimates of better models lead to better predicitons of reading times in Japanese (as has been reported in English). So they investigate whether the psycholinguistic theory generalises across languages, but not the models themselves.															
du-etal-2021-meta	2021	Meta-Learning with Variational Semantic Memory for Word Sense Disambiguation	https://aclanthology.org/2021.acl-long.409																									
mao-etal-2021-banditmtl	2021	{B}andit{MTL}: Bandit-based Multi-task Learning for Text Classification	https://aclanthology.org/2021.acl-long.428	Done	practical	across task	finetune-train/test	label	naturally occurring shifts	text classification	Florian	Zhijing	Label shift was chosen because the method uses both tasks during training but is then only evaluated on both individually.															
lee-etal-2021-learning-perturb	2021	Learning to Perturb Word Embeddings for Out-of-distribution {QA}	https://aclanthology.org/2021.acl-long.434																									
liu-etal-2021-learning-ask	2021	Learning to Ask Conversational Questions by Optimizing {L}evenshtein Distance	https://aclanthology.org/2021.acl-long.438																									
liu-etal-2021-mulda	2021	{M}ul{DA}: A Multilingual Data Augmentation Framework for Low-Resource Cross-Lingual {NER}	https://aclanthology.org/2021.acl-long.453																									
guan-etal-2021-openmeva	2021	{O}pen{MEVA}: A Benchmark for Evaluating Open-ended Story Generation Metrics	https://aclanthology.org/2021.acl-long.500																									
liang-etal-2021-super	2021	Super Tickets in Pre-Trained Language Models: From Model Compression to Improving Generalization	https://aclanthology.org/2021.acl-long.510	Done	practical	across domain	pretrain-train	covariate	naturally occurring shifts	text classification	Tiago	Zhijing	In section 7, this paper evaluates how model performs when transfered across domains/datasets.															
sengupta-2020-datamafia	2020	{DATAMAFIA} at {WNUT}-2020 {T}ask 2: {A} {S}tudy of {P}re-trained {L}anguage {M}odels along with {R}egularization {T}echniques for {D}ownstream {T}asks	https://aclanthology.org/2020.wnut-1.51																									
chauhan-2020-neu	2020	{NEU} at {WNUT}-2020 Task 2: Data Augmentation To Tell {BERT} That Death Is Not Necessarily Informative	https://aclanthology.org/2020.wnut-1.64																									
cerda-mardini-etal-2020-translating	2020	Translating Natural Language Instructions for Behavioral Robot Navigation with a Multi-Head Attention Mechanism	https://aclanthology.org/2020.winlp-1.24																									
mccoy-etal-2020-syntax	2020	Does Syntax Need to Grow on Trees? Sources of Hierarchical Inductive Bias in Sequence-to-Sequence Networks	https://aclanthology.org/2020.tacl-1.9	Done	cognitive	structural	train-test	covariate	generated shifts	seq2seq synthetic	Tiago	Huygaa																
tu-etal-2020-empirical	2020	An Empirical Study on Robustness to Spurious Correlations using Pre-trained Language Models	https://aclanthology.org/2020.tacl-1.40	Done	practical	robustness	all	double	generated shifts	NLI	Yanai	Dieuwke	Experiment to evaluate how well different pretrained models generise to HANS. Because there is both a label shift from pretraining to finetuning that the authors investigate (there are different models compared), and there is also a covariate shift in the finetuning stage, this entry is marked "double shift", with locus "all"															
tu-etal-2020-empirical	2020	An Empirical Study on Robustness to Spurious Correlations using Pre-trained Language Models	https://aclanthology.org/2020.tacl-1.40	Done	practical	robustness	all	double	generated shifts	paraphrase detection	Yanai	Dieuwke	Experiment to evaluate how well different pretrained models generise to PAWS. Because there is both a label shift from pretraining to finetuning that the authors investigate (there are different models compared), and there is also a covariate shift in the finetuning stage, this entry is marked "double shift", with locus "all"															
bartolo-etal-2020-beat	2020	Beat the {AI}: Investigating Adversarial Human Annotation for Reading Comprehension	https://aclanthology.org/2020.tacl-1.43	First check done	practical	robustness	finetune-train/test	covariate	natural data splits	reading comprehension	Huygaa		Table 7. 															
kobayashi-etal-2020-efficient	2020	Efficient Estimation of Influence of a Training Instance	https://aclanthology.org/2020.sustainlp-1.6																									
mishra-sachdeva-2020-need	2020	Do We Need to Create Big Datasets to Learn a Task?	https://aclanthology.org/2020.sustainlp-1.23																									
ravichander-etal-2020-systematicity	2020	On the Systematicity of Probing Contextualized Word Representations: The Case of Hypernymy in {BERT}	https://aclanthology.org/2020.starsem-1.10	Done	intrinsic	structural	pretrain-test	covariate	generated shifts	language modelling	Yanai	Dieuwke																
chen-etal-2020-reading	2020	Reading the Manual: Event Extraction as Definition Comprehension	https://aclanthology.org/2020.spnlp-1.9																									
vylomova-etal-2020-sigmorphon	2020	{SIGMORPHON} 2020 Shared Task 0: Typologically Diverse Morphological Inflection	https://aclanthology.org/2020.sigmorphon-1.1	Done	cognitive	structural	train-test	covariate	naturally occurring shifts	morphological inflection	Huygaa	Tiago																
jhunjhunwala-etal-2020-multi	2020	Multi-Action Dialog Policy Learning with Interactive Human Teaching	https://aclanthology.org/2020.sigdial-1.36																									
mahurkar-patil-2020-lrg	2020	{LRG} at {S}em{E}val-2020 Task 7: Assessing the Ability of {BERT} and Derivative Models to Perform Short-Edits Based Humor Grading	https://aclanthology.org/2020.semeval-1.108																									
pedersen-2020-duluth	2020	{D}uluth at {S}em{E}val-2020 Task 12: Offensive Tweet Identification in {E}nglish with Logistic Regression	https://aclanthology.org/2020.semeval-1.255	Flag for further review							Leila		I do not think this paper is relevant. It states that the rekeased gold data for the competition has noise in the labels and that is why powerfull methods learnin g anomalies might not generalize well. 															
cengiz-yuret-2020-joint	2020	Joint Training with Semantic Role Labeling for Better Generalization in Natural Language Inference	https://aclanthology.org/2020.repl4nlp-1.11																									
ek-etal-2020-punctuation	2020	How does Punctuation Affect Neural Models in Natural Language Inference	https://aclanthology.org/2020.pam-1.15	Done	practical	robustness	finetune-train/test	covariate	generated shifts	NLI	Tiago	Zhijing																
gao-etal-2020-machine	2020	From Machine Reading Comprehension to Dialogue State Tracking: Bridging the Gap	https://aclanthology.org/2020.nlp4convai-1.10																									
cruz-etal-2020-localization	2020	Localization of Fake News Detection via Multitask Transfer Learning	https://aclanthology.org/2020.lrec-1.316																									
ahamad-etal-2020-accentdb	2020	{A}ccent{DB}: A Database of Non-Native {E}nglish Accents to Assist Neural Speech Recognition	https://aclanthology.org/2020.lrec-1.659																									
chowdhury-etal-2020-multi	2020	A Multi-Platform {A}rabic News Comment Dataset for Offensive Language Detection	https://aclanthology.org/2020.lrec-1.761																									
grivas-etal-2020-cute	2020	Not a cute stroke: Analysis of Rule- and Neural Network-based Information Extraction Systems for Brain Radiology Reports	https://aclanthology.org/2020.louhi-1.4																									
huber-etal-2020-supervised	2020	Supervised Adaptation of Sequence-to-Sequence Speech Recognition Systems using Batch-Weighting	https://aclanthology.org/2020.lifelongnlp-1.2																									
bandyopadhyay-zhao-2020-natural	2020	Natural Language Response Generation from {SQL} with Generalization and Back-translation	https://aclanthology.org/2020.intexsempar-1.6	Done	practical	compositional	pretrain-test	covariate	fully generated/selected	NLG	Rita	Huygaa																
huang-etal-2020-counterfactually	2020	Counterfactually-Augmented {SNLI} Training Data Does Not Yield Better Generalization Than Unaugmented Data	https://aclanthology.org/2020.insights-1.13																									
kale-rastogi-2020-text	2020	Text-to-Text Pre-Training for Data-to-Text Tasks	https://aclanthology.org/2020.inlg-1.14																									
du-etal-2020-schema	2020	Schema-Guided Natural Language Generation	https://aclanthology.org/2020.inlg-1.35																									
peng-etal-2020-shot	2020	Few-shot Natural Language Generation for Task-Oriented Dialog	https://aclanthology.org/2020.findings-emnlp.17	Done	practical	across domain	pretrain-train	full	naturally occurring shifts	dialogue	Leila	Koustuv	They use a pre-traine LM (GPT2) and further pre-train it on huge labelled dialogue state data, then finetune with little samples and test on a dataset they collect															
li-etal-2020-event	2020	Event Extraction as Multi-turn Question Answering	https://aclanthology.org/2020.findings-emnlp.73																									
wu-etal-2020-improving	2020	Improving {QA} Generalization by Concurrent Modeling of Multiple Biases	https://aclanthology.org/2020.findings-emnlp.74																									
yang-etal-2020-generative	2020	Generative Data Augmentation for Commonsense Reasoning	https://aclanthology.org/2020.findings-emnlp.90	Flag for further review	practical	robustness	finetune-train/test	covariate	generated shifts	question answering	Leila	Koustuv																
welbl-etal-2020-undersensitivity	2020	Undersensitivity in Neural Reading Comprehension	https://aclanthology.org/2020.findings-emnlp.103																									
gardner-etal-2020-evaluating	2020	Evaluating Models{'} Local Decision Boundaries via Contrast Sets	https://aclanthology.org/2020.findings-emnlp.117	Done	intrinsic	robustness	finetune-train/test	covariate	generated shifts	NLI	Yanai	Verna																
gardner-etal-2020-evaluating	2020	Evaluating Models{'} Local Decision Boundaries via Contrast Sets	https://aclanthology.org/2020.findings-emnlp.117	Done	intrinsic	robustness	finetune-train/test	covariate	generated shifts	sentiment analysis	Yanai	Verna																
gardner-etal-2020-evaluating	2020	Evaluating Models{'} Local Decision Boundaries via Contrast Sets	https://aclanthology.org/2020.findings-emnlp.117	Done	intrinsic	robustness	finetune-train/test	covariate	generated shifts	question answering	Yanai	Verna																
ye-etal-2020-teaching	2020	Teaching Machine Comprehension with Compositional Explanations	https://aclanthology.org/2020.findings-emnlp.145																									
roman-roman-etal-2020-rmm	2020	{RMM}: A Recursive Mental Model for Dialogue Navigation	https://aclanthology.org/2020.findings-emnlp.157																									
lin-etal-2020-commongen	2020	{C}ommon{G}en: A Constrained Text Generation Challenge for Generative Commonsense Reasoning	https://aclanthology.org/2020.findings-emnlp.165	Flag for further review	practical	across task	finetune-train/test	full	naturally occurring shifts	question answering	Leila	Koustuv	They investigate how finetuned/trained models on comonGen can improve QA															
khashabi-etal-2020-unifiedqa	2020	{UNIFIEDQA}: Crossing Format Boundaries with a Single {QA} System	https://aclanthology.org/2020.findings-emnlp.171																									
oren-etal-2020-improving	2020	Improving Compositional Generalization in Semantic Parsing	https://aclanthology.org/2020.findings-emnlp.225																									
limisiewicz-etal-2020-universal	2020	{U}niversal {D}ependencies {A}ccording to {BERT}: {B}oth {M}ore {S}pecific and {M}ore {G}eneral	https://aclanthology.org/2020.findings-emnlp.245																									
du-etal-2020-general	2020	General Purpose Text Embeddings from Pre-trained Language Models for Scalable Inference	https://aclanthology.org/2020.findings-emnlp.271																									
yin-etal-2020-learning	2020	Learning to Generalize for Sequential Decision Making	https://aclanthology.org/2020.findings-emnlp.273																									
xia-etal-2020-composed	2020	Composed Variational Natural Language Generation for Few-shot Intents	https://aclanthology.org/2020.findings-emnlp.303																									
zhang-etal-2020-margin	2020	Margin-aware Unsupervised Domain Adaptation for Cross-lingual Text Labeling	https://aclanthology.org/2020.findings-emnlp.315																									
chen-etal-2020-cdevalsumm	2020	{CDE}val{S}umm: An Empirical Study of Cross-Dataset Evaluation for Neural Summarization Systems	https://aclanthology.org/2020.findings-emnlp.329																									
maharana-bansal-2020-adversarial	2020	Adversarial Augmentation Policy Search for Domain and Cross-Lingual Generalization in Reading Comprehension	https://aclanthology.org/2020.findings-emnlp.333	First check done	practical	robustness	finetune-train/test	covariate	generated shifts	reading comprehension	Huygaa		Table 2 - Adversarial evaluation of Roberta finetuned on SQUAD 2.0															
maharana-bansal-2020-adversarial	2020	Adversarial Augmentation Policy Search for Domain and Cross-Lingual Generalization in Reading Comprehension	https://aclanthology.org/2020.findings-emnlp.333	First check done	practical	across domain	finetune-train/test	covariate	generated shifts	reading comprehension	Huygaa		Table 4															
maharana-bansal-2020-adversarial	2020	Adversarial Augmentation Policy Search for Domain and Cross-Lingual Generalization in Reading Comprehension	https://aclanthology.org/2020.findings-emnlp.333	First check done	practical	across language	finetune-train/test	covariate	generated shifts	reading comprehension	Huygaa		Table 5															
harrigian-etal-2020-models	2020	Do Models of Mental Health Based on Social Media Data Generalize?	https://aclanthology.org/2020.findings-emnlp.337	Done	intrinsic	across domain	train-test	covariate	naturally occurring shifts	other	Tiago	Zhijing	This paper evaluates how classifiers trained to detect depression fare both across time (train in year x vs tested in year y) and across social media (trianed in twitter vs tested in reddit)															
he-etal-2020-enhancing	2020	Enhancing Generalization in Natural Language Inference by Syntax	https://aclanthology.org/2020.findings-emnlp.447																									
banerjee-baral-2020-self	2020	Self-Supervised Knowledge Triplet Learning for Zero-Shot Question Answering	https://aclanthology.org/2020.emnlp-main.11																									
warstadt-etal-2020-learning	2020	Learning Which Features Matter: {R}o{BERT}a Acquires a Preference for Linguistic Generalizations (Eventually)	https://aclanthology.org/2020.emnlp-main.16	Done	intrinsic	structural	finetune-train/test	covariate	fully generated/selected	seq2seq synthetic	Yanai	Tiago	Are you sure this should be pretrain-train? I would mark it as finetune-test, as the distribution changes from finetuning to test sets. I would also mark it as a covariate shift, as the same p(y|x) is used for finetuning and testing, but different instances x are used.															
bansal-etal-2020-self	2020	Self-Supervised Meta-Learning for Few-Shot Natural Language Classification Tasks	https://aclanthology.org/2020.emnlp-main.38																									
forbes-etal-2020-social	2020	Social Chemistry 101: Learning to Reason about Social and Moral Norms	https://aclanthology.org/2020.emnlp-main.48																									
ng-etal-2020-ssmba	2020	{SSMBA}: Self-Supervised Manifold Based Data Augmentation for Improving Out-of-Domain Robustness	https://aclanthology.org/2020.emnlp-main.97																									
weller-etal-2020-learning	2020	Learning from Task Descriptions	https://aclanthology.org/2020.emnlp-main.105																									
ponti-etal-2020-xcopa	2020	{XCOPA}: A Multilingual Dataset for Causal Commonsense Reasoning	https://aclanthology.org/2020.emnlp-main.185	Done	cognitive	across domain	pretrain-train	covariate	naturally occurring shifts	reasoning	Huygaa	Leila	Table 5 CO-ZS: multilingual LMs fine-tuned on SIQA and tested on XCOPA. Domain difference between SIQA and XCOPA. Here is only language involved, English.															
ponti-etal-2020-xcopa	2020	{XCOPA}: A Multilingual Dataset for Causal Commonsense Reasoning	https://aclanthology.org/2020.emnlp-main.185	Done	cognitive	across language	pretrain-train	full	naturally occurring shifts	reasoning	Leila	Huygaa	Table 5 CO-TLV: multilingual LMs fine-tuned and tested on XCOPA															
ponti-etal-2020-xcopa	2020	{XCOPA}: A Multilingual Dataset for Causal Commonsense Reasoning	https://aclanthology.org/2020.emnlp-main.185	Done	cognitive	robustness	pretrain-train	full	generated shifts	reasoning	Huygaa	Leila	Table 6 															
ponti-etal-2020-xcopa	2020	{XCOPA}: A Multilingual Dataset for Causal Commonsense Reasoning	https://aclanthology.org/2020.emnlp-main.185	Done	cognitive	across language	pretrain-train	full	naturally occurring shifts	reasoning	Leila	Huygaa	Adaptation to unseen languages															
xiao-etal-2020-modeling	2020	Modeling Content Importance for Summarization with Pre-trained Language Models	https://aclanthology.org/2020.emnlp-main.293																									
rosenman-etal-2020-exposing	2020	{E}xposing {S}hallow {H}euristics of {R}elation {E}xtraction {M}odels with {C}hallenge {D}ata	https://aclanthology.org/2020.emnlp-main.302	Done	intrinsic	robustness	train-test	covariate	generated shifts	relation extraction	Yanai	Tiago																
huang-etal-2020-joint-multiple	2020	A Joint Multiple Criteria Model in Transfer Learning for Cross-domain {C}hinese Word Segmentation	https://aclanthology.org/2020.emnlp-main.318																									
daza-frank-2020-x	2020	{X}-{SRL}: A Parallel Cross-Lingual Semantic Role Labeling Dataset	https://aclanthology.org/2020.emnlp-main.321	First check done	practical	across language	finetune-train/test	covariate	fully generated/selected	semantic parsing / role labelling	Huygaa		Table 6. Finetune mBERT on English and test it on DE, FR, ES. All the data was fully generated using machine translation. 															
wang-etal-2020-negative	2020	On Negative Interference in Multilingual Models: Findings and A Meta-Learning Treatment	https://aclanthology.org/2020.emnlp-main.359	Flag for further review	practical	across language	finetune-train/test	covariate	natural data splits	multiple tasks	Mikel		This paper studies negative interference during multilingual pre-training. It compares different approaches to address interference, and compare how they affect performance when fine-tuning in the target language and transfering cross-lingually (fine-tune on English, test on target language). My annotation corresponds to the latter, not sure how the former would fit in our framework (if relevant)															
philip-etal-2020-monolingual	2020	Monolingual Adapters for Zero-Shot Neural Machine Translation	https://aclanthology.org/2020.emnlp-main.361	Done	practical	across language	train-test	label	naturally occurring shifts	machine translation	Florian	Zhijing	adaptation experiments (not included in this annotation is what they call zero-shot because it is not really zero-shot in my eyes, since the source and the target language were seen already, just the translation direction is unseen)															
wilcox-etal-2020-structural	2020	Structural Supervision Improves Few-Shot Learning and Syntactic Generalization in Neural Language Models	https://aclanthology.org/2020.emnlp-main.375	Done	cognitive	structural	train-test	covariate	naturally occurring shifts	language modelling	Florian	Zhijing																
li-etal-2020-optimus	2020	Optimus: Organizing Sentences via Pre-trained Modeling of a Latent Space	https://aclanthology.org/2020.emnlp-main.378																									
chen-etal-2020-low	2020	Low-Resource Domain Adaptation for Compositional Task-Oriented Semantic Parsing	https://aclanthology.org/2020.emnlp-main.413																									
guo-etal-2020-sequence	2020	Sequence-Level Mixed Sample Data Augmentation	https://aclanthology.org/2020.emnlp-main.447																									
keskar-etal-2020-thieves	2020	The Thieves on Sesame Street are Polyglots - Extracting Multilingual Models from Monolingual {API}s	https://aclanthology.org/2020.emnlp-main.501	Flag for further review	practical	across language	finetune-train/test	covariate	natural data splits	NLI	Mikel		Unusual setup: prior work has shown that it is possible to "steal" a classifier without access to the model weights by training on gibberish input data paired with the label predicted by the victim model; this paper explores if such an approach also generalizes cross-lingually for multilingual models, using only gibberish data in English															
desai-etal-2020-compressive	2020	Compressive Summarization with Plausibility and Salience Modeling	https://aclanthology.org/2020.emnlp-main.507																									
yin-etal-2020-universal	2020	Universal Natural Language Processing with Limited Annotations: Try Few-shot Textual Entailment as a Start	https://aclanthology.org/2020.emnlp-main.660																									
stacey-etal-2020-avoiding	2020	{A}voiding the {H}ypothesis-{O}nly {B}ias in {N}atural {L}anguage {I}nference via {E}nsemble {A}dversarial {T}raining	https://aclanthology.org/2020.emnlp-main.665																									
flachs-etal-2020-grammatical	2020	Grammatical Error Correction in Low Error Density Domains: A New Benchmark and Analyses	https://aclanthology.org/2020.emnlp-main.680																									
chen-etal-2020-kgpt	2020	{KGPT}: Knowledge-Grounded Pre-Training for Data-to-Text Generation	https://aclanthology.org/2020.emnlp-main.697																									
allaway-mckeown-2020-zero	2020	{Z}ero-{S}hot {S}tance {D}etection: {A} {D}ataset and {M}odel using {G}eneralized {T}opic {R}epresentations	https://aclanthology.org/2020.emnlp-main.717																									
kim-linzen-2020-cogs	2020	{COGS}: A Compositional Generalization Challenge Based on Semantic Interpretation	https://aclanthology.org/2020.emnlp-main.731	Done	cognitive	compositional	train-test	covariate	fully generated/selected	semantic parsing / role labelling	Verna	Florian																
swayamdipta-etal-2020-dataset	2020	Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics	https://aclanthology.org/2020.emnlp-main.746	Done	intrinsic	robustness	finetune-train/test	covariate	natural data splits	NLI	Verna	Florian																
swayamdipta-etal-2020-dataset	2020	Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics	https://aclanthology.org/2020.emnlp-main.746	Done	intrinsic	robustness	finetune-train/test	covariate	natural data splits	reasoning	Verna	Florian																
frank-petty-2020-sequence	2020	Sequence-to-Sequence Networks Learn the Meaning of Reflexive Anaphora	https://aclanthology.org/2020.crac-1.16																									
joshi-etal-2020-taxinli	2020	{T}axi{NLI}: Taking a Ride up the {NLU} Hill	https://aclanthology.org/2020.conll-1.4																									
bhattacharya-van-schijndel-2020-filler	2020	Filler-gaps that neural networks fail to generalize	https://aclanthology.org/2020.conll-1.39																									
eisape-etal-2020-cloze	2020	Cloze Distillation: Improving Neural Language Models with Human Next-Word Prediction	https://aclanthology.org/2020.conll-1.49																									
nicolai-silfverberg-2020-noise	2020	Noise Isn{'}t Always Negative: Countering Exposure Bias in Sequence-to-Sequence Inflection Models	https://aclanthology.org/2020.coling-main.255																									
zeng-etal-2020-event	2020	Event Coreference Resolution with their Paraphrases and Argument-aware Embeddings	https://aclanthology.org/2020.coling-main.275																									
li-etal-2020-target	2020	Target Word Masking for Location Metonymy Resolution	https://aclanthology.org/2020.coling-main.330																									
bansal-etal-2020-learning	2020	Learning to Few-Shot Learn Across Diverse Natural Language Classification Tasks	https://aclanthology.org/2020.coling-main.448																									
kalouli-etal-2020-hy	2020	Hy-{NLI}: a Hybrid system for Natural Language Inference	https://aclanthology.org/2020.coling-main.459																									
geiger-etal-2020-neural	2020	Neural Natural Language Inference Models Partially Embed Theories of Lexical Entailment and Negation	https://aclanthology.org/2020.blackboxnlp-1.16																									
mccoy-etal-2020-berts	2020	{BERT}s of a feather do not generalize together: Large variability in generalization across models with similar test set performance	https://aclanthology.org/2020.blackboxnlp-1.21	Done	intrinsic	robustness	finetune-train/test	covariate	fully generated/selected	NLI	Yanai	Tiago																
thrush-etal-2020-investigating	2020	Investigating Novel Verb Learning in {BERT}: Selectional Preference Classes and Alternation-Based Syntactic Generalization	https://aclanthology.org/2020.blackboxnlp-1.25																									
newman-etal-2020-eos	2020	The {EOS} Decision and Length Extrapolation	https://aclanthology.org/2020.blackboxnlp-1.26	Done	intrinsic	structural	train-test	covariate	fully generated/selected	seq2seq synthetic	Florian	Verna	Dyck and SCAN experiments															
newman-etal-2020-eos	2020	The {EOS} Decision and Length Extrapolation	https://aclanthology.org/2020.blackboxnlp-1.26	Done	intrinsic	structural	train-test	covariate	natural data splits	machine translation	Florian	Verna	Machine translation															
haley-2020-bert	2020	This is a {BERT}. Now there are several of them. Can they generalize to novel words?	https://aclanthology.org/2020.blackboxnlp-1.31	Done	intrinsic	structural	pretrain-test	covariate	generated shifts	language modelling	Tiago	Zhijing	This task could probably be number agreement, instead of language modelling.															
rawat-etal-2020-entity	2020	Entity-Enriched Neural Models for Clinical Question Answering	https://aclanthology.org/2020.bionlp-1.12																									
nejadgholi-kiritchenko-2020-cross	2020	On Cross-Dataset Generalization in Automatic Detection of Online Abuse	https://aclanthology.org/2020.alw-1.20																									
buyukoz-etal-2020-analyzing	2020	Analyzing {ELM}o and {D}istil{BERT} on Socio-political News Classification	https://aclanthology.org/2020.aespen-1.4																									
ray-chowdhury-etal-2020-cross	2020	Cross-Lingual Disaster-related Multi-label Tweet Classification with Manifold Mixup	https://aclanthology.org/2020.acl-srw.39																									
russin-etal-2020-compositional	2020	Compositional Generalization by Factorizing Alignment and Translation	https://aclanthology.org/2020.acl-srw.42	Done	cognitive	compositional	train-test	covariate	fully generated/selected	seq2seq synthetic	Rita	Dieuwke	Even though the task mentions MT, it seems that the goal is to research compositional generalization. Datasets used; SCAN and MT dataset from teh same paper															
zhao-etal-2020-designing	2020	Designing Precise and Robust Dialogue Response Evaluators	https://aclanthology.org/2020.acl-main.4																									
chen-etal-2020-shot	2020	Few-Shot {NLG} with Pre-Trained Language Model	https://aclanthology.org/2020.acl-main.18																									
dubois-etal-2020-location	2020	{L}ocation {A}ttention for {E}xtrapolation to {L}onger {S}equences	https://aclanthology.org/2020.acl-main.39																									
rybak-etal-2020-klej	2020	{KLEJ}: Comprehensive Benchmark for {P}olish Language Understanding	https://aclanthology.org/2020.acl-main.111																									
hu-etal-2020-systematic	2020	A Systematic Assessment of Syntactic Generalization in Neural Language Models	https://aclanthology.org/2020.acl-main.158	Done	intrinsic	structural	train-test	assumed	fully generated/selected	language modelling	Florian	Yanai																
mccurdy-etal-2020-inflecting	2020	Inflecting When There{'}s No Majority: Limitations of Encoder-Decoder Neural Networks as Cognitive Models for {G}erman Plurals	https://aclanthology.org/2020.acl-main.159	Done	cognitive	structural	train-test	covariate	generated shifts	morphological inflection	Tiago	Zhijing																
goodwin-etal-2020-probing	2020	Probing Linguistic Systematicity	https://aclanthology.org/2020.acl-main.177	Done	intrinsic	compositional	train-test	covariate	fully generated/selected	NLI	Tiago	Zhijing																
talmor-etal-2020-olmpics	2020	o{LM}pics-On What Language Model Pre-training Captures	https://aclanthology.org/2020.tacl-1.48	Done	intrinsic	across task	pretrain-train	covariate	fully generated/selected	multiple tasks	Tiago	Yanai	In this paper, the authors evaluate whether roberta (and two other non-pretrained transformers) can perform 8 different tasks zero shot (through masked language modelling) and when finetuned on these tasks with different amounts of data.															
talmor-etal-2020-olmpics	2020	o{LM}pics-On What Language Model Pre-training Captures	https://aclanthology.org/2020.tacl-1.48	Done	intrinsic	across task	pretrain-test	covariate	fully generated/selected	multiple tasks	Tiago	Yanai	In this paper, the authors evaluate whether roberta (and two other non-pretrained transformers) can perform 8 different tasks zero shot (through masked language modelling) and when finetuned on these tasks with different amounts of data.															
jiang-etal-2020-smart	2020	{SMART}: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization	https://aclanthology.org/2020.acl-main.197	Flag for further review	practical						Huygaa		Really hard to say if its experiments are related to generalization. 															
zhang-etal-2020-overfitting	2020	Why Overfitting Isn{'}t Always Bad: Retrofitting Cross-Lingual Word Embeddings to Dictionaries	https://aclanthology.org/2020.acl-main.201																									
min-etal-2020-syntactic	2020	Syntactic Data Augmentation Increases Robustness to Inference Heuristics	https://aclanthology.org/2020.acl-main.212																									
hendrycks-etal-2020-pretrained	2020	Pretrained Transformers Improve Out-of-Distribution Robustness	https://aclanthology.org/2020.acl-main.244																									
lepori-etal-2020-representations	2020	Representations of Syntax {[MASK]} Useful: {E}ffects of Constituency and Dependency Structure in Recursive {LSTM}s	https://aclanthology.org/2020.acl-main.303																									
yue-etal-2020-clinical	2020	Clinical Reading Comprehension: A Thorough Analysis of the emr{QA} Dataset	https://aclanthology.org/2020.acl-main.410																									
artetxe-etal-2020-cross	2020	On the Cross-lingual Transferability of Monolingual Representations	https://aclanthology.org/2020.acl-main.421	Flag for further review	practical	across language		covariate	natural data splits	multitask	Mikel		The paper explores if an English transformer can be adapted to a new languages by only learning a new set of embeddings. Unsure what the locus is as there is an English pretraining phase followed by the target language pretraining phase followed by English finetuning followed by multilingual evaluation															
ribeiro-etal-2020-beyond	2020	Beyond Accuracy: Behavioral Testing of {NLP} Models with {C}heck{L}ist	https://aclanthology.org/2020.acl-main.442	Done	practical	robustness	train-test	covariate	generated shifts	sentiment analysis	Yanai	Tiago																
ribeiro-etal-2020-beyond	2020	Beyond Accuracy: Behavioral Testing of {NLP} Models with {C}heck{L}ist	https://aclanthology.org/2020.acl-main.442	Done	practical	robustness	train-test	covariate	generated shifts	paraphrase detection	Yanai	Tiago																
ribeiro-etal-2020-beyond	2020	Beyond Accuracy: Behavioral Testing of {NLP} Models with {C}heck{L}ist	https://aclanthology.org/2020.acl-main.442	Done	practical	robustness	train-test	covariate	generated shifts	question answering	Yanai	Tiago																
yanaka-etal-2020-neural	2020	Do Neural Models Learn Systematicity of Monotonicity Inference in Natural Language?	https://aclanthology.org/2020.acl-main.543	Done	practical	compositional	train-test	covariate	generated shifts	NLI	Yanai	Tiago																
wang-etal-2020-rat	2020	{RAT-SQL}: Relation-Aware Schema Encoding and Linking for Text-to-{SQL} Parsers	https://aclanthology.org/2020.acl-main.677																									
naik-rose-2020-towards	2020	Towards Open Domain Event Trigger Identification using Adversarial Domain Adaptation	https://aclanthology.org/2020.acl-main.681	First check done	practical	across domain	finetune-train/test	covariate	natural data splits	other	Huygaa																	
naik-rose-2020-towards	2020	Towards Open Domain Event Trigger Identification using Adversarial Domain Adaptation	https://aclanthology.org/2020.acl-main.681	First check done	practical	across domain	train-test	covariate	natural data splits	other	Huygaa																	
pouran-ben-veyseh-etal-2020-exploiting	2020	Exploiting the Syntax-Model Consistency for Neural Relation Extraction	https://aclanthology.org/2020.acl-main.715																									
suhr-etal-2020-exploring	2020	Exploring Unexplored Generalization Challenges for Cross-Database Semantic Parsing	https://aclanthology.org/2020.acl-main.742																									
lin-etal-2020-triggerner	2020	{T}rigger{NER}: Learning with Entity Triggers as Explanations for Named Entity Recognition	https://aclanthology.org/2020.acl-main.752																									
karimi-mahabadi-etal-2020-end	2020	End-to-End Bias Mitigation by Modelling Biases in Corpora	https://aclanthology.org/2020.acl-main.769	Done	practical	compositional	finetune-train/test	covariate	generated shifts	NLU	Huygaa	Zhijing	Section 4															
karimi-mahabadi-etal-2020-end	2020	End-to-End Bias Mitigation by Modelling Biases in Corpora	https://aclanthology.org/2020.acl-main.769	Done	practical	across domain	finetune-train/test	full	naturally occurring shifts	NLU	Huygaa	Zhijing	Section 5															
han-etal-2020-data	2020	More Data, More Relations, More Context and More Openness: A Review and Outlook for Relation Extraction	https://aclanthology.org/2020.aacl-main.75																									
dhar-van-der-plas-2019-learning	2019	Learning to Predict Novel Noun-Noun Compounds	https://aclanthology.org/W19-5105																									
korrel-etal-2019-transcoding	2019	Transcoding Compositionally: Using Attention to Find More Generalizable Solutions	https://aclanthology.org/W19-4801	Done	practical	compositional	train-test	covariate	fully generated/selected	seq2seq synthetic	Dieuwke	Leila	New architecture to get improvements on compositional generalisation tasks															
yanaka-etal-2019-neural	2019	Can Neural Networks Understand Monotonicity Reasoning?	https://aclanthology.org/W19-4804	Flag for further review	intrinsic	structural	train-test	covariate	naturally occurring shifts	NLI	Leila	Koustuv																
talman-chatzikyriakidis-2019-testing	2019	Testing the Generalization Power of Neural Network Models across {NLI} Benchmarks	https://aclanthology.org/W19-4810	First check done	intrinsic	robustness	train-test	assumed	naturally occurring shifts	NLI	Tiago	Yanai	The authors analyse the performance of an NLI model trained on one dataset and evaluated on another.															
eljundi-etal-2019-hulmona	2019	h{ULM}on{A}: The Universal Language Model in {A}rabic	https://aclanthology.org/W19-4608																									
khaddaj-etal-2019-improved	2019	Improved Generalization of {A}rabic Text Classifiers	https://aclanthology.org/W19-4618	First check done	practical	across domain	train-test	covariate	natural data splits	text classification	Huygaa		Table 1															
garcia-silva-etal-2019-empirical	2019	An Empirical Study on Pre-trained Embeddings and Language Models for Bot Detection	https://aclanthology.org/W19-4317	Flag for further review	practical	across task	pretrain-train	full	naturally occurring shifts	text classification	Dieuwke		Bot detection, comparison of different pretrained Glove embeddings (I am assuming here that Glove etc are not trained on twitter data, but I am not completely sure), it might be a label shift only otherwise															
winata-etal-2019-learning	2019	Learning Multilingual Meta-Embeddings for Code-Switching Named Entity Recognition	https://aclanthology.org/W19-4320																									
oseki-etal-2019-inverting	2019	Inverting and Modeling Morphological Inflection	https://aclanthology.org/W19-4220	First check done	cognitive	structural	train-test	covariate	generated shifts	morphological inflection	Tiago		I'm not sure this paper fits the generalisation definition. In practice, they do analyse model generalisation, but that's not the focus imo. They train a model and compare it's confidence on wug wordforms (in Japanese) to human production probabilities. The focus of the paper, however, is on how humans compare to the model, and not how the model compares to humans. I think I would actually keep this paper here, but I'm not sure.															
yu-etal-2019-gumdrop	2019	{G}um{D}rop at the {DISRPT}2019 Shared Task: A Model Stacking Approach to Discourse Unit Segmentation and Connective Detection	https://aclanthology.org/W19-2717																									
subramanian-roth-2019-improving	2019	Improving Generalization in Coreference Resolution via Adversarial Training	https://aclanthology.org/S19-1021	First check done	practical	robustness	train-test	covariate	generated shifts	named entity recognition (NER)	Huygaa																	
baumann-2019-multilingual	2019	Multilingual Language Models for Named Entity Recognition in {G}erman and {E}nglish	https://aclanthology.org/R19-2004	Flag for further review								Mikel	Rather irrelevant paper published in a student workshop, no interesting experiments in cross-lingual generalization, I would remove it															
pappas-henderson-2019-gile	2019	{GILE}: A Generalized Input-Label Embedding for Text Classification	https://aclanthology.org/Q19-1009	First check done	practical	robustness	train-test	covariate	natural data splits	text classification	Tiago		They evaluate their model on labels present in the training set, and labels which are only present in test time. This however, if I understood correctly, still does not represent a label shift (only a covariate shift) because the distribution p(y|x) changes. 															
warstadt-etal-2019-neural	2019	Neural Network Acceptability Judgments	https://aclanthology.org/Q19-1040	Done	intrinsic	structural	train-test	covariate	natural data splits	other	Tiago	Zhijing	The task in this paper was to classify a sentence's grammatical acceptability.															
sen-etal-2019-heidl	2019	{HEIDL}: Learning Linguistic Expressions with Deep Learning and Human-in-the-Loop	https://aclanthology.org/P19-3023																									
fried-etal-2019-cross	2019	Cross-Domain Generalization of Neural Constituency Parsers	https://aclanthology.org/P19-1031	Done	practical	across domain	train-test	covariate	natural data splits	constituency parsing	Dieuwke	Mikel	Test how well neural and non-neural constituency parsers generalise across domains. They also present a test with different pretrained representations, which I put in the next line															
fried-etal-2019-cross	2019	Cross-Domain Generalization of Neural Constituency Parsers	https://aclanthology.org/P19-1031	Done	practical	across domain	all	double	natural data splits	constituency parsing	Dieuwke	Mikel	Test of how different pretrained representations aid in cross-domain generalisation															
farag-yannakoudakis-2019-multi	2019	Multi-Task Learning for Coherence Modeling	https://aclanthology.org/P19-1060																									
xu-etal-2019-cross	2019	A Cross-Domain Transferable Neural Coherence Model	https://aclanthology.org/P19-1067																									
zhang-etal-2019-knowledge	2019	Knowledge-aware Pronoun Coreference Resolution	https://aclanthology.org/P19-1083	Done	practical	across domain	train-test	covariate	naturally occurring shifts	other	Dieuwke	Mikel	Pronoun coreference resolution, across different domains. The models use ELMO and glove embeddings, but they are not finetuned, so I marked it "train/test"															
zheng-etal-2019-diag	2019	{DIAG}-{NRE}: A Neural Pattern Diagnosis Framework for Distantly Supervised Neural Relation Extraction	https://aclanthology.org/P19-1137																									
liu-etal-2019-reinforced	2019	Reinforced Training Data Selection for Domain Adaptation	https://aclanthology.org/P19-1189																									
logeswaran-etal-2019-zero	2019	Zero-Shot Entity Linking by Reading Entity Descriptions	https://aclanthology.org/P19-1335																									
zhou-etal-2019-dual	2019	Dual Adversarial Neural Transfer for Low-Resource Named Entity Recognition	https://aclanthology.org/P19-1336	First check done	practical	across language	pretrain-train	covariate	natural data splits	named entity recognition (NER)	Huygaa		Figure 2a. The model first pretrained on English CoNLL dataset then it is finetuned on very low ratio of target language's dataset.															
zhou-etal-2019-dual	2019	Dual Adversarial Neural Transfer for Low-Resource Named Entity Recognition	https://aclanthology.org/P19-1336	First check done	practical	across domain	pretrain-train	covariate	natural data splits	named entity recognition (NER)	Huygaa		Figure 2b.															
dessi-baroni-2019-cnns	2019	{CNN}s found to jump around more skillfully than {RNN}s: Compositional Generalization in Seq2seq Convolutional Networks	https://aclanthology.org/P19-1381	Done	intrinsic	compositional	train-test	covariate	fully generated/selected	seq2seq synthetic	Dieuwke	Florian	Comparison of CNNs and RNNs on SCaN															
jiang-de-marneffe-2019-know	2019	Do You Know That Florence Is Packed with Visitors? Evaluating State-of-the-art Models of Speaker Commitment	https://aclanthology.org/P19-1412																									
yu-etal-2019-sparc	2019	{SP}ar{C}: Cross-Domain Semantic Parsing in Context	https://aclanthology.org/P19-1443	Done	practical	across domain	train-test	covariate	natural data splits	semantic parsing / role labelling	Dieuwke	Huygaa	Cross-domain semantic parsing dataset, split along different dimensions.															
yu-etal-2019-sparc	2019	{SP}ar{C}: Cross-Domain Semantic Parsing in Context	https://aclanthology.org/P19-1443	Done	intrinsic	across domain	train-test	covariate	natural data splits	semantic parsing / role labelling	Dieuwke	Huygaa	If the Spider is intrinsic (which I think is), this is closer to intrinsic than practical) 															
talmor-berant-2019-multiqa	2019	{M}ulti{QA}: An Empirical Investigation of Generalization and Transfer in Reading Comprehension	https://aclanthology.org/P19-1485																									
vulic-etal-2019-multilingual	2019	Multilingual and Cross-Lingual Graded Lexical Entailment	https://aclanthology.org/P19-1490																									
kumar-etal-2019-zero	2019	Zero-shot Word Sense Disambiguation using Sense Definition Embeddings	https://aclanthology.org/P19-1568	Done	practical	compositional	finetune-train/test	full	natural data splits	word sense disambiguation (WSD)	Huygaa	Zhijing																
marzinotto-etal-2019-robust	2019	Robust Semantic Parsing with Adversarial Learning for Domain Generalization	https://aclanthology.org/N19-2021	Done	practical	across domain	train-test	covariate	natural data splits	question answering	Huygaa	Zhijing																
mchardy-etal-2019-adversarial	2019	Adversarial Training for Satire Detection: Controlling for Confounding Variables	https://aclanthology.org/N19-1069																									
havrylov-etal-2019-cooperative	2019	Cooperative Learning of Disjoint Syntax and Semantics	https://aclanthology.org/N19-1115																									
al-shedivat-parikh-2019-consistency	2019	Consistency by Agreement in Zero-Shot Neural Machine Translation	https://aclanthology.org/N19-1121	Done	practical	across language	train-test	covariate	natural data splits	machine translation	Rita	Mikel																
bjerva-etal-2019-probabilistic	2019	A Probabilistic Generative Model of Linguistic Typology	https://aclanthology.org/N19-1156	First check done	practical	across language	train-test	covariate	natural data splits	other	Tiago		The task analysed in this paper is predicting typological features in a language. The authors analyse the impact of having less or more training data from languages in a specific genus on the model's performance (fig 5). They also analyse the impact of using a pretrained lstm to get language-specific features (section 7.4), but I dont think that experiment counts as investigating generalisation.															
zhou-etal-2019-density	2019	Density Matching for Bilingual Word Embedding	https://aclanthology.org/N19-1161																									
hessel-lee-2019-somethings	2019	Something{'}s Brewing! Early Prediction of Controversy-causing Posts from Discussion Features	https://aclanthology.org/N19-1166																									
qian-etal-2019-learning	2019	Learning to Decipher Hate Symbols	https://aclanthology.org/N19-1305																									
rossiello-etal-2019-learning	2019	Learning Relational Representations by Analogy using Hierarchical {S}iamese Networks	https://aclanthology.org/N19-1327																									
wilcox-etal-2019-structural	2019	Structural Supervision Improves Learning of Non-Local Grammatical Dependencies	https://aclanthology.org/N19-1334																									
joshi-etal-2019-pair2vec	2019	pair2vec: Compositional Word-Pair Embeddings for Cross-Sentence Inference	https://aclanthology.org/N19-1362																									
rozen-etal-2019-diversify	2019	Diversify Your Datasets: Analyzing Generalization via Controlled Variance in Adversarial Datasets	https://aclanthology.org/K19-1019	Done	intrinsic	robustness	train-test	covariate	generated shifts	NLI	Yanai	Zhijing																
gillick-etal-2019-learning	2019	Learning Dense Representations for Entity Retrieval	https://aclanthology.org/K19-1049																									
liu-etal-2019-self	2019	Self-Adaptive Scaling for Learnable Residual Structure	https://aclanthology.org/K19-1080																									
salvatore-etal-2019-logical	2019	A logical-based corpus for cross-lingual evaluation	https://aclanthology.org/D19-6103																									
basu-roy-chowdhury-etal-2019-instance	2019	Instance-based Inductive Deep Transfer Learning by Cross-Dataset Querying with Locality Sensitive Hashing	https://aclanthology.org/D19-6120																									
fisch-etal-2019-mrqa	2019	{MRQA} 2019 Shared Task: Evaluating Generalization in Reading Comprehension	https://aclanthology.org/D19-5801																									
wu-etal-2019-improving	2019	Improving the Robustness of Deep Reading Comprehension Models by Leveraging Syntax Prior	https://aclanthology.org/D19-5807																									
angelidis-etal-2019-book	2019	Book {QA}: Stories of Challenges and Opportunities	https://aclanthology.org/D19-5811																									
takahashi-etal-2019-cler	2019	{CLER}: Cross-task Learning with Expert Representation to Generalize Reading and Understanding	https://aclanthology.org/D19-5824																									
lee-etal-2019-domain	2019	Domain-agnostic Question-Answering with Adversarial Training	https://aclanthology.org/D19-5826	First check done	practical	across domain	finetune-train/test	covariate	natural data splits	question answering	Huygaa		Table 2															
su-etal-2019-generalizing	2019	Generalizing Question Answering System with Pre-trained Language Model Fine-tuning	https://aclanthology.org/D19-5827																									
dou-etal-2019-domain	2019	Domain Differential Adaptation for Neural Machine Translation	https://aclanthology.org/D19-5606	Done	practical	across domain	train-test	covariate	naturally occurring shifts	machine translation	Florian	Verna	Training with WMT and testing on multilingual TED															
dou-etal-2019-domain	2019	Domain Differential Adaptation for Neural Machine Translation	https://aclanthology.org/D19-5606	Done	practical	across domain	train-test	covariate	naturally occurring shifts	language modelling	Florian	Verna	Language modelling experiments															
schmidt-2019-generalization	2019	Generalization in Generation: A closer look at Exposure Bias	https://aclanthology.org/D19-5616																									
bodapati-etal-2019-robustness	2019	Robustness to Capitalization Errors in Named Entity Recognition	https://aclanthology.org/D19-5531																									
huo-etal-2019-graph	2019	Graph Enhanced Cross-Domain Text-to-{SQL} Generation	https://aclanthology.org/D19-5319																									
tayyar-madabushi-etal-2019-cost	2019	Cost-Sensitive {BERT} for Generalisable Sentence Classification on Imbalanced Data	https://aclanthology.org/D19-5018																									
castro-ferreira-etal-2019-neural	2019	Neural data-to-text generation: A comparison between pipeline and end-to-end architectures	https://aclanthology.org/D19-1052																									
zhao-etal-2019-moverscore	2019	{M}over{S}core: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance	https://aclanthology.org/D19-1053																									
wu-dredze-2019-beto	2019	Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of {BERT}	https://aclanthology.org/D19-1077																									
czarnowska-etal-2019-dont	2019	Don{'}t Forget the Long Tail! A Comprehensive Analysis of Morphological Generalization in Bilingual Lexicon Induction	https://aclanthology.org/D19-1090	Done	cognitive	structural	train-test	covariate	naturally occurring shifts	morphological inflection	Verna	Florian																
lu-etal-2019-look	2019	Look-up and Adapt: A One-shot Semantic Parser	https://aclanthology.org/D19-1104																									
andrews-bishop-2019-learning	2019	Learning Invariant Representations of Social Media Users	https://aclanthology.org/D19-1178																									
perez-etal-2019-finding	2019	Finding Generalizable Evidence by Learning to Convince {Q}{\&}{A} Models	https://aclanthology.org/D19-1244																									
mihaylov-frank-2019-discourse	2019	Discourse-Aware Semantic Self-Attention for Narrative Reading Comprehension	https://aclanthology.org/D19-1257																									
wang-etal-2019-learning	2019	Learning Semantic Parsers from Denotations with Latent Structured Alignments and Abstract Programs	https://aclanthology.org/D19-1391																									
geng-etal-2019-induction	2019	Induction Networks for Few-Shot Text Classification	https://aclanthology.org/D19-1403																									
kumar-etal-2019-topics	2019	Topics to Avoid: Demoting Latent Confounds in Text Classification	https://aclanthology.org/D19-1425																									
li-etal-2019-compositional	2019	Compositional Generalization for Primitive Substitutions	https://aclanthology.org/D19-1438	Done	cognitive	compositional	train-test	covariate	fully generated/selected	seq2seq synthetic	Verna	Zhijing																
geiger-etal-2019-posing	2019	Posing Fair Generalization Tasks for Natural Language Inference	https://aclanthology.org/D19-1456																									
sinha-etal-2019-clutrr	2019	{CLUTRR}: A Diagnostic Benchmark for Inductive Reasoning from Text	https://aclanthology.org/D19-1458	Done	practical	compositional	finetune-train/test	covariate	fully generated/selected	reasoning	Koustuv	Dieuwke																
pham-etal-2019-generic	2019	Generic and Specialized Word Embeddings for Multi-Domain Machine Translation	https://aclanthology.org/2019.iwslt-1.26	Done	practical	across domain	train-test	covariate	naturally occurring shifts	machine translation	Florian	Zhijing	General experiments (e.g. additional domain scenario)															
pham-etal-2019-generic	2019	Generic and Specialized Word Embeddings for Multi-Domain Machine Translation	https://aclanthology.org/2019.iwslt-1.26	Done	practical	robustness	train-test	covariate	naturally occurring shifts	machine translation	Florian	Zhijing	Noisy domain labels															
jagfeld-etal-2018-sequence	2018	Sequence-to-Sequence Models for Data-to-Text Natural Language Generation: Word- vs. Character-based Processing and Output Diversity	https://aclanthology.org/W18-6529																									
reed-etal-2018-neural	2018	Can Neural Generators for Dialogue Learn Sentence Planning and Discourse Structuring?	https://aclanthology.org/W18-6535																									
prickett-etal-2018-seq2seq	2018	{S}eq2{S}eq Models with Dropout can Learn Generalizable Reduplication	https://aclanthology.org/W18-5810	Done	cognitive	structural	train-test	covariate	generated shifts	morphological inflection	Huygaa	Zhijing	Morphological reduplication in this paper is a special case of morphological inflection															
bastings-etal-2018-jump	2018	Jump to better conclusions: {SCAN} both left and right	https://aclanthology.org/W18-5407	Done	practical	compositional	train-test	covariate	fully generated/selected	seq2seq synthetic	Verna	Dieuwke	I opted for motivation "practical". While SCAN's motivation is cognitive, NACS motivation is to solve issues with SCAN, namely the fact that it prefers very basic models. Second annotator might disagree though.															
loula-etal-2018-rearranging	2018	Rearranging the Familiar: Testing Compositional Generalization in Recurrent Networks	https://aclanthology.org/W18-5413	Done	cognitive	compositional	train-test	covariate	fully generated/selected	seq2seq synthetic	Dieuwke	Mikel	Follow up paper of SCAN, with slighly different splits															
naderi-hirst-2018-using	2018	Using context to identify the language of face-saving	https://aclanthology.org/W18-5214																									
karan-snajder-2018-cross	2018	Cross-Domain Detection of Abusive Language Online	https://aclanthology.org/W18-5117																									
zhao-eskenazi-2018-zero	2018	Zero-Shot Dialog Generation with Cross-Domain Latent Actions	https://aclanthology.org/W18-5001	Done	practical	across domain	train-test	full	natural data splits	dialogue	Dieuwke	Mario	New model for zero-shot generation in new dialogue domains															
zhao-kawahara-2018-unified	2018	A Unified Neural Architecture for Joint Dialog Act Segmentation and Recognition in Spoken Dialog System	https://aclanthology.org/W18-5021																									
moeller-etal-2018-neural	2018	A Neural Morphological Analyzer for {A}rapaho Verbs Learned from a Finite State Transducer	https://aclanthology.org/W18-4802	Done	practical	structural	finetune-train/test	covariate	naturally occurring shifts	morphological inflection	Huygaa	Zhijing																
risch-krestel-2018-aggression	2018	Aggression Identification Using Deep Learning and Data Augmentation	https://aclanthology.org/W18-4418	Done	practical	robustness	train-test	covariate	naturally occurring shifts	toxicity classification	Dieuwke	Mikel	Authors propose a new toxitiy classification model, and say it generalises better to other datasets. I went for "robustness" rather than across domain, because this seem to be the main reason they are investigating generalisation to a different dataset.															
kann-etal-2018-character	2018	Character-level Supervision for Low-resource {POS} Tagging	https://aclanthology.org/W18-3401	Done	practical	across task	train-test	full	generated shifts	POStagging	Dieuwke	Leila	An investigation of POStagging (for low-resource languages) can benefit from co-training on other, sometimes synthetic/generated, tasks															
shvartzshanider-etal-2018-recipe	2018	{RECIPE}: Applying Open Domain Question Answering to Privacy Policies	https://aclanthology.org/W18-2608	Flag for further review	practical	across domain	train-test	covariate	naturally occurring shifts	other	Dieuwke		As far as I understand, they are using a pretrained open-domain QA model on a different corpus that they consider out-of-domain (but I found it a bit difficult to understand)															
wadhwa-etal-2018-towards	2018	Towards Inference-Oriented Reading Comprehension: {P}arallel{QA}	https://aclanthology.org/W18-1001	Flag for further review	intrinsic	robustness	train-test	covariate	natural data splits	reading comprehension	Dieuwke		Test what type of errors reading comprehension systems make, considering in particular if they rely on shallow patterns.															
wadhwa-etal-2018-towards	2018	Towards Inference-Oriented Reading Comprehension: {P}arallel{QA}	https://aclanthology.org/W18-1001	Flag for further review	intrinsic	robustness	pretrain-test	covariate	naturally occurring shifts	reading comprehension	Dieuwke	Leila	Test what type of errors reading comprehension systems make, considering in particular if they rely on shallow patterns.															
weber-etal-2018-fine	2018	The Fine Line between Linguistic Generalization and Failure in {S}eq2{S}eq-Attention Models	https://aclanthology.org/W18-1004	Done	intrinsic	structural	train-test	covariate	fully generated/selected	seq2seq synthetic	Verna	Florian																
nadeem-ostendorf-2018-estimating	2018	Estimating Linguistic Complexity for Science Texts	https://aclanthology.org/W18-0505	Done	practical	across domain	train-test	covariate	natural data splits	text classification	Koustuv	Florian																
bingel-bjerva-2018-cross	2018	Cross-lingual complex word identification with multitask learning	https://aclanthology.org/W18-0518	Done	practical	across language	train-test	covariate	natural data splits	other	Huygaa	Zhijing	Trained on German and Spanish, and tested on French															
roy-roth-2018-mapping	2018	Mapping to Declarative Knowledge for Word Problem Solving	https://aclanthology.org/Q18-1012	Done	practical	robustness	train-test	covariate	fully generated/selected	seq2seq synthetic	Dieuwke	Tiago	Novel approach to solving word math problems,  test generalisation from biased datasets															
roy-roth-2018-mapping	2018	Mapping to Declarative Knowledge for Word Problem Solving	https://aclanthology.org/Q18-1012	Flag for further review	practical	robustness	train-test	covariate	generated shifts	reasoning	Dieuwke	Tiago																
kunchukuttan-etal-2018-leveraging	2018	Leveraging Orthographic Similarity for Multilingual Neural Transliteration	https://aclanthology.org/Q18-1022	Done	practical	across language	train-test	covariate	naturally occurring shifts	machine translation	Dieuwke	Rita	Multilingual transliteration models, they compare both with bilingual models, and test the zero-shot setup for language pairs not seen together during training															
yin-etal-2018-end	2018	End-Task Oriented Textual Entailment via Deep Explorations of Inter-Sentence Interactions	https://aclanthology.org/P18-2086	Flag for further review	intrinsic	across domain	train-test	covariate	natural data splits	NLI	Dieuwke	Leila	This paper proposes a new model for entailment, there is one generalisation experiment, where they consider a different dataset. Because it seems to be more of a diagnostic test,  I labelled it "intrinsic" as motivation, but I am not completely sure.															
glockner-etal-2018-breaking	2018	Breaking {NLI} Systems with Sentences that Require Simple Lexical Inferences	https://aclanthology.org/P18-2103	Done	practical	robustness	train-test	covariate	generated shifts	NLI	Yanai	Verna																
xu-etal-2018-cross	2018	Cross-Target Stance Classification with Self-Attention Networks	https://aclanthology.org/P18-2123	Flag for further review	practical	robustness	train-test	covariate	natural data splits	stance classification	Dieuwke	Leila	Cross-target stance classification. I am not super sure if this is actually a generalisation experiment, but give it the benefit of the doubt.															
finegan-dollak-etal-2018-improving	2018	Improving Text-to-{SQL} Evaluation Methodology	https://aclanthology.org/P18-1033	Done	practical	robustness	train-test	covariate	natural data splits	semantic parsing / role labelling	Dieuwke	Huygaa	Query-based splitting to evaluate robustness of text-to-SQL models															
hershcovich-etal-2018-multitask	2018	Multitask Parsing Across Semantic Representations	https://aclanthology.org/P18-1035	Done	practical	across domain	train-test	covariate	naturally occurring shifts	dependency parsing	Dieuwke	Huygaa	Dependency parsing across datasets															
chen-bansal-2018-fast	2018	Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting	https://aclanthology.org/P18-1063	Done	practical	across domain	train-test	covariate	naturally occurring shifts	summarisation	Dieuwke	Huygaa	Investigation of summarisation models, their "test-only" setup is an out of domain experiment															
shwartz-dagan-2018-paraphrase	2018	Paraphrase to Explicate: Revealing Implicit Noun-Compound Relations	https://aclanthology.org/P18-1111	Done	practical	robustness	train-test	covariate	natural data splits	paraphrasing	Dieuwke	Mario	Focus on unseen noun-noun constituents / compounds															
dong-de-melo-2018-helping	2018	A Helping Hand: Transfer Learning for Deep Sentiment Analysis	https://aclanthology.org/P18-1235	Flag for further review							Dieuwke		This has the word "domain" in it a lot, but I don't think there is actually any OUT of domain evaluation															
koshorek-etal-2018-text	2018	Text Segmentation as a Supervised Learning Task	https://aclanthology.org/N18-2075	Flag for further review							Dieuwke		The abstract says something about generalisation to unseen text, but upon further reading I could not actually find any generalisation experiments															
li-etal-2018-whats	2018	What{'}s in a Domain? Learning Domain-Robust Text Representations using Adversarial Training	https://aclanthology.org/N18-2076	Done	intrinsic	across domain	train-test	covariate	naturally occurring shifts	other	Dieuwke	Huygaa	Language identification, across domains															
robertson-goldwater-2018-evaluating	2018	Evaluating Historical Text Normalization Systems: How Well Do They Generalize?	https://aclanthology.org/N18-2113	First check done	practical	across domain	train-test	covariate	naturally occurring shifts	other	Dieuwke		Generalisation to unseen words in historical text normalisation systems															
sanchez-etal-2018-behavior	2018	Behavior Analysis of {NLI} Models: Uncovering the Influence of Three Factors on Robustness	https://aclanthology.org/N18-1179	Done	intrinsic	robustness	train-test	covariate	generated shifts	NLI	Dieuwke	Leila	Investigation of robustness of NLI models, with test sets that are systematically transformed from the original test set															
moosavi-strube-2018-using	2018	Using Linguistic Features to Improve the Generalization Capability of Neural Coreference Resolvers	https://aclanthology.org/D18-1018	Done	intrinsic	across domain	train-test	covariate	naturally occurring shifts	coreference resolution	Dieuwke	Rita	Investigation of if and how incorporating linguistic features helps with across domain generalisation															
herzig-berant-2018-decoupling	2018	Decoupling Structure and Lexicon for Zero-Shot Semantic Parsing	https://aclanthology.org/D18-1190	Done	practical	across domain	train-test	covariate	naturally occurring shifts	semantic parsing / role labelling	Dieuwke	Huygaa	Semantic parsing across domains															
zhou-etal-2018-zero	2018	Zero-Shot Open Entity Typing as Type-Compatible Grounding	https://aclanthology.org/D18-1231	Done	practical	across domain	train-test	covariate	naturally occurring shifts	other	Dieuwke	Leila	New approach to entity typing that generalises well to different datasets															
gupta-lewis-2018-neural	2018	Neural Compositional Denotational Semantics for Question Answering	https://aclanthology.org/D18-1239	Done	practical	compositional	train-test	covariate	fully generated/selected	question answering	Dieuwke	Huygaa	New approach to QA, with some compositional tests, in particular to longer and more complex sequences															
gupta-lewis-2018-neural	2018	Neural Compositional Denotational Semantics for Question Answering	https://aclanthology.org/D18-1239	Done	practical	across domain	train-test	covariate	natural data splits	question answering	Dieuwke	Huygaa	Generalisation to human generated language															
chaudhary-etal-2018-adapting	2018	Adapting Word Embeddings to New Languages with Morphological and Phonological Subword Representations	https://aclanthology.org/D18-1366	Done	practical	across language	finetune-train/test	covariate	naturally occurring shifts	machine translation	Huygaa	Zhijing	MT experiments on cross-lingual transfer for TR to UG and HI to BN															
he-etal-2018-adaptive	2018	Adaptive Semi-supervised Learning for Cross-domain Sentiment Classification	https://aclanthology.org/D18-1383	Flag for further review	practical	across domain	train-test	covariate	naturally occurring shifts	sentiment analysis	Dieuwke		I am not sure if this should perhaps be removed, if they add the (unlabeled) data of the different domain to the training data, is there stil a label shift?															
yu-etal-2018-spider	2018	{S}pider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-{SQL} Task	https://aclanthology.org/D18-1425	Done	intrinsic	across domain	train-test	covariate	natural data splits	multiple tasks	Dieuwke	Leila	A new dataset with cross-domain semantic parsing splits, with tests for a number of different architectures (train-test, no pretraining as far as I can see)															
hanselowski-etal-2018-retrospective	2018	A Retrospective Analysis of the Fake News Challenge Stance-Detection Task	https://aclanthology.org/C18-1158	Done	intrinsic	across domain	train-test	covariate	naturally occurring shifts	stance classification	Dieuwke	Leila	Investigation of robustness and cross-domain stance clasification															
hanselowski-etal-2018-retrospective	2018	A Retrospective Analysis of the Fake News Challenge Stance-Detection Task	https://aclanthology.org/C18-1158	Done	intrinsic	compositional	train-test	covariate	natural data splits	stance classification	Dieuwke	Leila	Investigation of robustness and cross-domain stance clasification															
sachan-etal-2018-investigating	2018	Investigating the Working of Text Classifiers	https://aclanthology.org/C18-1180	Done	intrinsic	across domain	train-test	covariate	naturally occurring shifts	text classification	Dieuwke	Huygaa	Invesigate text-classification, both a compositional split and a cross-domain split based on a split of lexical items															
chen-etal-2017-recurrent-neural	2017	Recurrent Neural Network-Based Sentence Encoder with Gated Attention for Natural Language Inference	https://aclanthology.org/W17-5307	Done	practical	across domain	train-test	covariate	naturally occurring shifts	NLI	Dieuwke	Huygaa	Submission to the RepEval 2017 shared task for NLI, including results on a cross-domain test set															
ajjour-etal-2017-unit	2017	Unit Segmentation of Argumentative Texts	https://aclanthology.org/W17-5115	Done	practical	across domain	train-test	covariate	naturally occurring shifts	other	Dieuwke	Leila	Cross-domain generalisation argument unit segmentation															
hitschler-etal-2017-authorship	2017	Authorship Attribution with Convolutional Neural Networks and {POS}-Eliding	https://aclanthology.org/W17-4907	Done	practical	across domain	train-test	covariate	naturally occurring shifts	text classification	Dieuwke	Mario	Generalisation across time in generalisation across time: papers in 2006 as dev, 2007 as training, rest for training															
peng-dredze-2017-multi	2017	Multi-task Domain Adaptation for Sequence Tagging	https://aclanthology.org/W17-2612	Flag for further review	practical	across domain	pretrain-train	covariate	natural data splits	multiple tasks	Dieuwke	Rita	Investigation of MTL helps domain adaptation															
peng-dredze-2017-multi	2017	Multi-task Domain Adaptation for Sequence Tagging	https://aclanthology.org/W17-2612	Flag for further review	practical	across task	pretrain-train	covariate	natural data splits	multitask	Dieuwke	Rita	Investigation of MTL models for task adaptation															
king-cook-2017-supervised	2017	Supervised and unsupervised approaches to measuring usage similarity	https://aclanthology.org/W17-1906	Flag for further review	practical	robustness	train-test	covariate	natural data splits	other	Dieuwke	Rita	Usage similarity, generalisation to unseen lemma's															
moosavi-strube-2017-lexical	2017	Lexical Features in Coreference Resolution: To be Used With Caution	https://aclanthology.org/P17-2003	Done	intrinsic	across domain	train-test	covariate	naturally occurring shifts	other	Dieuwke	Rita	Coreference resolution across domains															
kim-etal-2017-domain	2017	Domain Attention with an Ensemble of Experts	https://aclanthology.org/P17-1060	Done	practical	across domain	train-test	covariate	naturally occurring shifts	intent classification	Dieuwke	Rita	Domain adaptation for intent classification (for automatic speech recognition  task)															
kreutzer-etal-2017-bandit	2017	Bandit Structured Prediction for Neural Sequence-to-Sequence Learning	https://aclanthology.org/P17-1138	Done	practical	across domain	finetune-train/test	covariate	naturally occurring shifts	machine translation	Dieuwke	Rita	Domain adaptation for machine translation. I am not completely clear on the phases here, but I'd say there is a shift from pretraining to finetuning (where finetuning is the adaptation phase)															
levy-etal-2017-zero	2017	Zero-Shot Relation Extraction via Reading Comprehension	https://aclanthology.org/K17-1034	Done	practical	robustness	train-test	covariate	natural data splits	relation extraction	Dieuwke	Rita	Relation-extraction by mapping relations to natural language questions, generalisation to unseen question templates and unseen relations. I am not super sure about the model that is used there, should be carefully double checked															
ziser-reichart-2017-neural	2017	Neural Structural Correspondence Learning for Domain Adaptation	https://aclanthology.org/K17-1040	Done	practical	across domain	train-test	covariate	naturally occurring shifts	sentiment analysis	Dieuwke	Rita	Cross-domain sentiment classification															
tran-nguyen-2017-natural	2017	Natural Language Generation for Spoken Dialogue System using RNN Encoder-Decoder Networks	https://aclanthology.org/K17-1044	Done	practical	across domain	train-test	full	naturally occurring shifts	dialogue	Dieuwke	Mario	Spoken (?) dialogue generation, generalisation across different domains															
eisenberg-finlayson-2017-simpler	2017	A Simpler and More Generalizable Story Detector using Verb and Character Features	https://aclanthology.org/D17-1287	Done	practical	across domain	train-test	covariate	naturally occurring shifts	other	Dieuwke	Rita	Story detection, within and across different corpora															
ammar-etal-2016-many	2016	Many Languages, One Parser	https://aclanthology.org/Q16-1031	Done	practical	across language	train-test	full	naturally occurring shifts	dependency parsing	Dieuwke	Florian	Generalisation across languages in a syntactic parsing task, comparison between mono- and multilingual models															
yang-etal-2016-leveraging	2016	Leveraging Multiple Domains for Sentiment Classification	https://aclanthology.org/C16-1280	Done	practical	across domain	train-test	covariate	naturally occurring shifts	sentiment analysis	Dieuwke	Huygaa	Sentiment classifacation models that generalise across domains ((NB: I am not actually sure if this might be a full shift)															
marasovic-etal-2016-modal	2016	Modal Sense Classification At Large: Paraphrase-Driven Sense Projectio	https://aclanthology.org/2016.lilt-14.3	Done	practical	across language	train-test	covariate	natural data splits	text classification	Dieuwke	Rita	Modal sense classifation, across dataset (NB: I am not actually sure if this might be a full shift)															
hallmann-etal-2016-sarcastic	2016	Sarcastic Soulmates: Intimacy and irony markers in social media messaging	https://aclanthology.org/2016.lilt-14.7	Done	practical	across domain	train-test	covariate	natural data splits	text classification	Dieuwke	Rita	Sarcasm classification of twitter message, with both within and across category experiments ((NB: I am not actually sure if this might be a full shift)															
pimentel-ryskina-etal-2021-sigmorphon	2021	{SIGMORPHON} 2021 Shared Task on Morphological Reinflection: Generalization Across Languages	https://aclanthology.org/2021.sigmorphon-1.25/	Done	cognitive	structural	train-test	covariate	naturally occurring shifts	morphological inflection	Tiago	Huygaa																